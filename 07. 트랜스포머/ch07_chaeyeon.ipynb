{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "## 위치 인코딩\n",
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
        "        )\n",
        "\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[: x.size(0)]\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "encoding = PositionalEncoding(d_model=128, max_len=50)\n",
        "\n",
        "plt.pcolormesh(encoding.pe.numpy().squeeze(), cmap=\"RdBu\")\n",
        "plt.xlabel(\"Embedding Dimension\")\n",
        "plt.xlim((0, 128))\n",
        "plt.ylabel(\"Position\")\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Zf3W85ulQVhl"
      },
      "id": "Zf3W85ulQVhl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 트랜스포머 모델 실습"
      ],
      "metadata": {
        "id": "kwBiKNbFQQ4a"
      },
      "id": "kwBiKNbFQQ4a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "torch 와 torchtext버전 호환성 문제 발생\n",
        "- torchtext==0.16.0 로 맞추니까 코드 실행됨\n",
        "-tochtext cache데이터도 삭제"
      ],
      "metadata": {
        "id": "PY0fUMzrOxCO"
      },
      "id": "PY0fUMzrOxCO"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "b8ce3e81",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8ce3e81",
        "outputId": "c94a19d4-9406-473f-9a3a-3f9b1820106b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'de' are deprecated. Please use the\n",
            "full pipeline package name 'de_core_news_sm' instead.\u001b[0m\n",
            "Collecting de-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.7.0/de_core_news_sm-3.7.0-py3-none-any.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from de-core-news-sm==3.7.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (13.9.3)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
            "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m96.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.3)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download de\n",
        "!python -m spacy download en"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchdata torchtext spacy portalocker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0FUl6IPKKug",
        "outputId": "1d085bfd-a903-41c0-a675-ad5dc0fd047d"
      },
      "id": "n0FUl6IPKKug",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchdata in /usr/local/lib/python3.10/dist-packages (0.9.0)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.18.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (2.10.1)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.32.3)\n",
            "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.5.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.26.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2->torchdata) (1.3.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.3)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uigM2-qNPEo",
        "outputId": "3bf04857-f34f-4acd-dbe5-91077b60f631"
      },
      "id": "-uigM2-qNPEo",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5.1+cu124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 이코드 필연적 토치와 토치텍스트 버전 호환\n",
        "!pip uninstall torchtext -y\n",
        "!pip install torchtext==0.16.0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqKCExN4NkQn",
        "outputId": "f63175dc-2cdb-4746-a285-6d57b6c37b01"
      },
      "id": "EqKCExN4NkQn",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torchtext 0.16.0\n",
            "Uninstalling torchtext-0.16.0:\n",
            "  Successfully uninstalled torchtext-0.16.0\n",
            "Collecting torchtext==0.16.0\n",
            "  Using cached torchtext-0.16.0-cp310-cp310-manylinux1_x86_64.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.16.0) (4.66.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.16.0) (2.32.3)\n",
            "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torchtext==0.16.0) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.16.0) (1.26.4)\n",
            "Requirement already satisfied: torchdata==0.7.0 in /usr/local/lib/python3.10/dist-packages (from torchtext==0.16.0) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext==0.16.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext==0.16.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext==0.16.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext==0.16.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext==0.16.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext==0.16.0) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext==0.16.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext==0.16.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext==0.16.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext==0.16.0) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext==0.16.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext==0.16.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext==0.16.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext==0.16.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext==0.16.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext==0.16.0) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext==0.16.0) (12.1.105)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext==0.16.0) (2.1.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.7.0->torchtext==0.16.0) (2.2.3)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.0->torchtext==0.16.0) (12.4.127)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.16.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.16.0) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.16.0) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0->torchtext==0.16.0) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->torchtext==0.16.0) (1.3.0)\n",
            "Using cached torchtext-0.16.0-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n",
            "Installing collected packages: torchtext\n",
            "Successfully installed torchtext-0.16.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#torch 캐시 삭제\n",
        "!rm -rf ~/.cache/torchtext\n"
      ],
      "metadata": {
        "id": "xU-crsHNNnDR"
      },
      "id": "xU-crsHNNnDR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5173254c-db77-4791-8f00-23a89b6cf6d2",
      "metadata": {
        "id": "5173254c-db77-4791-8f00-23a89b6cf6d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9397357b-569c-4f57-f09d-2902aab889fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token Transform:\n",
            "{'de': functools.partial(<function _spacy_tokenize at 0x7b18091c16c0>, spacy=<spacy.lang.de.German object at 0x7b17ff05ff10>), 'en': functools.partial(<function _spacy_tokenize at 0x7b18091c16c0>, spacy=<spacy.lang.en.English object at 0x7b17ff0faf80>)}\n",
            "Vocab Transform:\n",
            "{'de': Vocab(), 'en': Vocab()}\n"
          ]
        }
      ],
      "source": [
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "# 토큰 생성 함수 정의: 텍스트 iterator와 언어(독일어/영어)를 받아, 해당 언어에 맞게 토큰화된 단어들을 yield함\n",
        "def generate_tokens(text_iter, language):\n",
        "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}  # 언어 인덱스 설정\n",
        "\n",
        "    for text in text_iter:\n",
        "        yield token_transform[language](text[language_index[language]])\n",
        "\n",
        "# 언어 설정: 독일어를 소스 언어, 영어를 타겟 언어로 지정\n",
        "SRC_LANGUAGE = \"de\"\n",
        "TGT_LANGUAGE = \"en\"\n",
        "\n",
        "# 특별 토큰의 인덱스 설정\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "special_symbols = [\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]\n",
        "\n",
        "# 토큰화 도구 설정: 각 언어에 맞는 spaCy 모델 사용\n",
        "token_transform = {\n",
        "    SRC_LANGUAGE: get_tokenizer(\"spacy\", language=\"de_core_news_sm\"),\n",
        "    TGT_LANGUAGE: get_tokenizer(\"spacy\", language=\"en_core_web_sm\"),\n",
        "}\n",
        "print(\"Token Transform:\")\n",
        "print(token_transform)  # 언어별 토큰화 함수 출력\n",
        "\n",
        "# 각 언어별 사전을 저장할 딕셔너리 생성\n",
        "vocab_transform = {}\n",
        "\n",
        "# 각 언어에 대해 토큰을 생성하고 사전을 구축\n",
        "for language in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    train_iter = Multi30k(split=\"train\", language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))  # 학습용 데이터셋 로드\n",
        "    vocab_transform[language] = build_vocab_from_iterator(\n",
        "        generate_tokens(train_iter, language),  # 해당 언어로 토큰화된 데이터\n",
        "        min_freq=1,                            # 최소 빈도 조건 설정\n",
        "        specials=special_symbols,              # 특별 토큰 추가\n",
        "        special_first=True,                    # 특별 토큰을 앞에 위치\n",
        "    )\n",
        "\n",
        "# UNK 토큰의 기본 인덱스를 설정하여 사전에 없는 토큰을 처리\n",
        "for language in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    vocab_transform[language].set_default_index(UNK_IDX)\n",
        "\n",
        "print(\"Vocab Transform:\")\n",
        "print(vocab_transform)  # 언어별 사전 출력\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "fd11e22d-1136-4d5a-96a8-281aaabe83bc",
      "metadata": {
        "id": "fd11e22d-1136-4d5a-96a8-281aaabe83bc"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# PositionalEncoding: Transformer에 위치 정보를 추가하기 위한 클래스\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)  # 드롭아웃 설정\n",
        "\n",
        "        # 위치 정보를 계산\n",
        "        position = torch.arange(max_len).unsqueeze(1)  # [max_len, 1] 형태\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
        "        )  # 각 위치의 주기적 변동을 계산\n",
        "\n",
        "        pe = torch.zeros(max_len, 1, d_model)  # 위치 인코딩 텐서 초기화\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)  # 짝수 인덱스에 대해 sin 적용\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)  # 홀수 인덱스에 대해 cos 적용\n",
        "        self.register_buffer(\"pe\", pe)  # 학습에서 제외되는 버퍼로 등록\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 입력에 위치 인코딩을 더하고 드롭아웃 적용\n",
        "        x = x + self.pe[: x.size(0)]\n",
        "        return self.dropout(x)\n",
        "\n",
        "# TokenEmbedding: 단어 토큰을 벡터로 임베딩하는 클래스\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)  # 임베딩 레이어 생성\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        # 임베딩을 루트 크기로 정규화하여 반환\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "# Seq2SeqTransformer: 전체 Transformer 모델 정의\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_encoder_layers,  # 인코더 레이어 수\n",
        "        num_decoder_layers,  # 디코더 레이어 수\n",
        "        emb_size,            # 임베딩 크기\n",
        "        max_len,             # 최대 시퀀스 길이\n",
        "        nhead,               # 멀티헤드 어텐션의 헤드 수\n",
        "        src_vocab_size,      # 소스 언어의 단어 수\n",
        "        tgt_vocab_size,      # 타겟 언어의 단어 수\n",
        "        dim_feedforward,     # 피드포워드 레이어 크기\n",
        "        dropout=0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)  # 소스 임베딩 레이어\n",
        "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)  # 타겟 임베딩 레이어\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            d_model=emb_size, max_len=max_len, dropout=dropout\n",
        "        )  # 위치 인코딩 레이어\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=emb_size,\n",
        "            nhead=nhead,\n",
        "            num_encoder_layers=num_encoder_layers,\n",
        "            num_decoder_layers=num_decoder_layers,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "        )  # Transformer 인코더-디코더 구조\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)  # 출력층 생성\n",
        "\n",
        "    # 순전파 함수: 소스와 타겟 텐서, 마스크 및 패딩 마스크를 인자로 받아 모델 출력 반환\n",
        "    def forward(\n",
        "        self,\n",
        "        src,                # 소스 시퀀스\n",
        "        trg,                # 타겟 시퀀스\n",
        "        src_mask,           # 소스 마스크\n",
        "        tgt_mask,           # 타겟 마스크\n",
        "        src_padding_mask,   # 소스 패딩 마스크\n",
        "        tgt_padding_mask,   # 타겟 패딩 마스크\n",
        "        memory_key_padding_mask,  # 메모리 키 패딩 마스크\n",
        "    ):\n",
        "        # 소스와 타겟에 임베딩 및 위치 인코딩 적용\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "        # Transformer를 통해 인코딩 및 디코딩 수행\n",
        "        outs = self.transformer(\n",
        "            src=src_emb,\n",
        "            tgt=tgt_emb,\n",
        "            src_mask=src_mask,\n",
        "            tgt_mask=tgt_mask,\n",
        "            memory_mask=None,\n",
        "            src_key_padding_mask=src_padding_mask,\n",
        "            tgt_key_padding_mask=tgt_padding_mask,\n",
        "            memory_key_padding_mask=memory_key_padding_mask\n",
        "        )\n",
        "        return self.generator(outs)  # 최종 출력 생성\n",
        "\n",
        "    # 인코더에서 소스 시퀀스 인코딩\n",
        "    def encode(self, src, src_mask):\n",
        "        return self.transformer.encoder(\n",
        "            self.positional_encoding(self.src_tok_emb(src)), src_mask\n",
        "        )\n",
        "\n",
        "    # 디코더에서 타겟 시퀀스 디코딩\n",
        "    def decode(self, tgt, memory, tgt_mask):\n",
        "        return self.transformer.decoder(\n",
        "            self.positional_encoding(self.tgt_tok_emb(tgt)), memory, tgt_mask\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d88994b6-897a-462e-b4b9-c8d585dc6dfd",
      "metadata": {
        "id": "d88994b6-897a-462e-b4b9-c8d585dc6dfd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8750501c-13d2-4254-8f8a-184f7cadc5f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src_tok_emb\n",
            "└ embedding\n",
            "tgt_tok_emb\n",
            "└ embedding\n",
            "positional_encoding\n",
            "└ dropout\n",
            "transformer\n",
            "└ encoder\n",
            "│  └ layers\n",
            "│  │  └ 0\n",
            "│  │  └ 1\n",
            "│  │  └ 2\n",
            "│  └ norm\n",
            "└ decoder\n",
            "│  └ layers\n",
            "│  │  └ 0\n",
            "│  │  └ 1\n",
            "│  │  └ 2\n",
            "│  └ norm\n",
            "generator\n"
          ]
        }
      ],
      "source": [
        "from torch import optim\n",
        "\n",
        "# 배치 크기 및 디바이스 설정\n",
        "BATCH_SIZE = 128\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # GPU가 사용 가능하면 \"cuda\", 아니면 \"cpu\" 사용\n",
        "\n",
        "# Seq2SeqTransformer 모델 초기화\n",
        "model = Seq2SeqTransformer(\n",
        "    num_encoder_layers=3,           # 인코더 레이어 수\n",
        "    num_decoder_layers=3,           # 디코더 레이어 수\n",
        "    emb_size=512,                   # 임베딩 크기\n",
        "    max_len=512,                    # 최대 시퀀스 길이\n",
        "    nhead=8,                        # 멀티헤드 어텐션 헤드 수\n",
        "    src_vocab_size=len(vocab_transform[SRC_LANGUAGE]),  # 소스 언어의 어휘 크기\n",
        "    tgt_vocab_size=len(vocab_transform[TGT_LANGUAGE]),  # 타겟 언어의 어휘 크기\n",
        "    dim_feedforward=512,            # 피드포워드 레이어의 차원 크기\n",
        ").to(DEVICE)  # 모델을 지정된 디바이스로 이동\n",
        "\n",
        "# 손실 함수 설정: CrossEntropyLoss를 사용하되, 패딩 인덱스는 무시\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX).to(DEVICE)\n",
        "\n",
        "# 옵티마이저 설정: Adam을 사용하여 모델의 파라미터를 최적화\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# 모델 계층 구조 출력\n",
        "for main_name, main_module in model.named_children():\n",
        "    print(main_name)  # 최상위 모듈 이름 출력\n",
        "    for sub_name, sub_module in main_module.named_children():\n",
        "        print(\"└\", sub_name)  # 첫 번째 하위 모듈 이름 출력\n",
        "        for ssub_name, ssub_module in sub_module.named_children():\n",
        "            print(\"│  └\", ssub_name)  # 두 번째 하위 모듈 이름 출력\n",
        "            for sssub_name, sssub_module in ssub_module.named_children():\n",
        "                print(\"│  │  └\", sssub_name)  # 세 번째 하위 모듈 이름 출력\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d92b11a3-89a0-4887-b233-4ef934ef1252",
      "metadata": {
        "id": "d92b11a3-89a0-4887-b233-4ef934ef1252",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1d6638d-a152-466a-8aee-de620c83cd5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(source, target):\n",
            "('Eine Gruppe von Männern lädt Baumwolle auf einen Lastwagen', 'A group of men are loading cotton onto a truck')\n",
            "source_batch: torch.Size([35, 128])\n",
            "tensor([[   2,    2,    2,  ...,    2,    2,    2],\n",
            "        [  14,    5,    5,  ...,    5,   21,    5],\n",
            "        [  38,   12,   35,  ...,   12, 1750,   69],\n",
            "        ...,\n",
            "        [   1,    1,    1,  ...,    1,    1,    1],\n",
            "        [   1,    1,    1,  ...,    1,    1,    1],\n",
            "        [   1,    1,    1,  ...,    1,    1,    1]])\n",
            "target_batch: torch.Size([30, 128])\n",
            "tensor([[   2,    2,    2,  ...,    2,    2,    2],\n",
            "        [   6,    6,    6,  ...,  250,   19,    6],\n",
            "        [  39,   12,   35,  ...,   12, 3254,   61],\n",
            "        ...,\n",
            "        [   1,    1,    1,  ...,    1,    1,    1],\n",
            "        [   1,    1,    1,  ...,    1,    1,    1],\n",
            "        [   1,    1,    1,  ...,    1,    1,    1]])\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# 여러 변환을 순차적으로 적용하는 함수 생성\n",
        "def sequential_transforms(*transforms):\n",
        "    def func(txt_input):\n",
        "        for transform in transforms:  # 주어진 변환을 차례대로 적용\n",
        "            txt_input = transform(txt_input)\n",
        "        return txt_input\n",
        "    return func\n",
        "\n",
        "# 텍스트를 토큰 ID로 변환하고, 시작(BOS) 및 종료(EOS) 토큰을 추가\n",
        "def input_transform(token_ids):\n",
        "    return torch.cat(\n",
        "        (torch.tensor([BOS_IDX]), torch.tensor(token_ids), torch.tensor([EOS_IDX]))\n",
        "    )\n",
        "\n",
        "# 배치 데이터를 처리하여 패딩을 추가하는 함수\n",
        "def collator(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in batch:  # 각 샘플에 대해 변환 적용\n",
        "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))  # 소스 텍스트 변환\n",
        "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))  # 타겟 텍스트 변환\n",
        "\n",
        "    # 배치에 대해 패딩 추가\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
        "    return src_batch, tgt_batch  # 패딩 처리된 텍스트 배치 반환\n",
        "\n",
        "\n",
        "# 텍스트 변환 함수 설정: 토큰화, 어휘 변환, 입력 변환\n",
        "text_transform = {}\n",
        "for language in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    text_transform[language] = sequential_transforms(\n",
        "        token_transform[language], vocab_transform[language], input_transform\n",
        "    )\n",
        "\n",
        "# Multi30k 데이터셋에서 \"valid\" 데이터 로드\n",
        "data_iter = Multi30k(split=\"valid\", language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "\n",
        "# DataLoader 설정: 배치 크기 및 collate_fn 지정\n",
        "dataloader = DataLoader(data_iter, batch_size=BATCH_SIZE, collate_fn=collator)\n",
        "\n",
        "# 첫 번째 배치 데이터를 가져옴\n",
        "source_tensor, target_tensor = next(iter(dataloader))\n",
        "\n",
        "# 데이터를 출력하여 확인\n",
        "print(\"(source, target):\")\n",
        "print(next(iter(data_iter)))  # 실제 데이터 샘플 출력\n",
        "\n",
        "print(\"source_batch:\", source_tensor.shape)  # 소스 배치의 크기 출력\n",
        "print(source_tensor)  # 소스 배치 텐서 출력\n",
        "\n",
        "print(\"target_batch:\", target_tensor.shape)  # 타겟 배치의 크기 출력\n",
        "print(target_tensor)  # 타겟 배치 텐서 출력\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "25d4f176-b171-4862-9fc7-93eaea1207d8",
      "metadata": {
        "id": "25d4f176-b171-4862-9fc7-93eaea1207d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ddd814b-4e7f-4050-bda3-d5c13498fdce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_mask: torch.Size([35, 35])\n",
            "tensor([[False, False, False,  ..., False, False, False],\n",
            "        [False, False, False,  ..., False, False, False],\n",
            "        [False, False, False,  ..., False, False, False],\n",
            "        ...,\n",
            "        [False, False, False,  ..., False, False, False],\n",
            "        [False, False, False,  ..., False, False, False],\n",
            "        [False, False, False,  ..., False, False, False]])\n",
            "target_mask: torch.Size([29, 29])\n",
            "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
            "         -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
            "         -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
            "         -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
            "         -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
            "         -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
            "         -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
            "         -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
            "         -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
            "         -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
            "         -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
            "         -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
            "         -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
            "         -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
            "         -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
            "         -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
            "         -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
            "         -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf,\n",
            "         -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf,\n",
            "         -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf,\n",
            "         -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
            "         -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf,\n",
            "         -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf,\n",
            "         -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0.]])\n",
            "source_padding_mask: torch.Size([128, 35])\n",
            "tensor([[False, False, False,  ...,  True,  True,  True],\n",
            "        [False, False, False,  ...,  True,  True,  True],\n",
            "        [False, False, False,  ...,  True,  True,  True],\n",
            "        ...,\n",
            "        [False, False, False,  ...,  True,  True,  True],\n",
            "        [False, False, False,  ...,  True,  True,  True],\n",
            "        [False, False, False,  ...,  True,  True,  True]])\n",
            "target_padding_mask: torch.Size([128, 29])\n",
            "tensor([[False, False, False,  ...,  True,  True,  True],\n",
            "        [False, False, False,  ...,  True,  True,  True],\n",
            "        [False, False, False,  ...,  True,  True,  True],\n",
            "        ...,\n",
            "        [False, False, False,  ...,  True,  True,  True],\n",
            "        [False, False, False,  ...,  True,  True,  True],\n",
            "        [False, False, False,  ...,  True,  True,  True]])\n"
          ]
        }
      ],
      "source": [
        "## 어텐션 마스크 생성\n",
        "# 주어진 시퀀스 길이에 대해 디코더 마스크(상삼각 행렬)를 생성하는 함수\n",
        "def generate_square_subsequent_mask(s):\n",
        "    # 상삼각 행렬 생성 (0보다 작은 인덱스를 masked로 처리)\n",
        "    mask = (torch.triu(torch.ones((s, s), device=DEVICE)) == 1).transpose(0, 1)\n",
        "\n",
        "    # 상삼각 행렬에서 0은 음의 무한대로, 1은 0으로 설정\n",
        "    mask = (\n",
        "        mask.float()\n",
        "        .masked_fill(mask == 0, float(\"-inf\"))  # 미래 단어들을 볼 수 없게 설정\n",
        "        .masked_fill(mask == 1, float(0.0))    # 현재와 이전 단어는 볼 수 있게 설정\n",
        "    )\n",
        "    return mask\n",
        "\n",
        "# 소스와 타겟 시퀀스에 대한 마스크를 생성하는 함수\n",
        "def create_mask(src, tgt):\n",
        "    src_seq_len = src.shape[0]  # 소스 시퀀스 길이\n",
        "    tgt_seq_len = tgt.shape[0]  # 타겟 시퀀스 길이\n",
        "\n",
        "    # 타겟 마스크는 디코더에서 사용할 마스크, 소스 마스크는 입력 시퀀스를 위한 마스크\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)  # 타겟의 마스크 생성\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len), device=DEVICE).type(torch.bool)  # 소스는 자기 자신을 볼 수 있도록 마스크 없음\n",
        "\n",
        "    # 소스와 타겟의 패딩을 위한 마스크 생성\n",
        "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)  # 소스 시퀀스에서 PAD_IDX 위치에 True 설정\n",
        "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)  # 타겟 시퀀스에서 PAD_IDX 위치에 True 설정\n",
        "\n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n",
        "\n",
        "# 타겟 입력과 타겟 출력 시퀀스 준비 (타겟의 마지막 토큰은 예측해야 하므로 잘라내고, 첫 번째 토큰은 예측하기 위해 준비)\n",
        "target_input = target_tensor[:-1, :]\n",
        "target_out = target_tensor[1:, :]\n",
        "\n",
        "# 소스 시퀀스와 타겟 입력에 대한 마스크 생성\n",
        "source_mask, target_mask, source_padding_mask, target_padding_mask = create_mask(\n",
        "    source_tensor, target_input\n",
        ")\n",
        "\n",
        "# 마스크의 형태와 내용을 출력\n",
        "print(\"source_mask:\", source_mask.shape)\n",
        "print(source_mask)\n",
        "print(\"target_mask:\", target_mask.shape)\n",
        "print(target_mask)\n",
        "print(\"source_padding_mask:\", source_padding_mask.shape)\n",
        "print(source_padding_mask)\n",
        "print(\"target_padding_mask:\", target_padding_mask.shape)\n",
        "print(target_padding_mask)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db94b9ae-333e-444b-9309-f9137f9afb0d",
      "metadata": {
        "id": "db94b9ae-333e-444b-9309-f9137f9afb0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1dac75c-b1a1-44fb-fba4-78968f583b41"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Train loss: 4.465, Val loss: 3.724\n",
            "Epoch: 2, Train loss: 3.559, Val loss: 3.479\n",
            "Epoch: 3, Train loss: 3.308, Val loss: 3.405\n"
          ]
        }
      ],
      "source": [
        "# 모델을 학습 모드 또는 평가 모드로 설정하고, 주어진 split에 맞는 데이터를 로드한 후, 모델을 학습 또는 평가하는 함수\n",
        "def run(model, optimizer, criterion, split):\n",
        "    # 모델을 학습 또는 평가 모드로 설정\n",
        "    model.train() if split == \"train\" else model.eval()\n",
        "\n",
        "    # 데이터셋 로드 (train 또는 valid 데이터셋)\n",
        "    data_iter = Multi30k(split=split, language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    dataloader = DataLoader(data_iter, batch_size=BATCH_SIZE, collate_fn=collator)\n",
        "\n",
        "    losses = 0  # 손실 값을 누적할 변수\n",
        "    for source_batch, target_batch in dataloader:\n",
        "        # 데이터를 GPU로 전송\n",
        "        source_batch = source_batch.to(DEVICE)\n",
        "        target_batch = target_batch.to(DEVICE)\n",
        "\n",
        "        # 타겟 배치에서 입력과 출력을 분리 (입력은 마지막을 제외하고, 출력은 첫 번째를 제외한 나머지)\n",
        "        target_input = target_batch[:-1, :]\n",
        "        target_output = target_batch[1:, :]\n",
        "\n",
        "        # 소스와 타겟의 마스크 생성\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(\n",
        "            source_batch, target_input\n",
        "        )\n",
        "\n",
        "        # 모델의 출력값(logits) 계산\n",
        "        logits = model(\n",
        "            src=source_batch,\n",
        "            trg=target_input,\n",
        "            src_mask=src_mask,\n",
        "            tgt_mask=tgt_mask,\n",
        "            src_padding_mask=src_padding_mask,\n",
        "            tgt_padding_mask=tgt_padding_mask,\n",
        "            memory_key_padding_mask=src_padding_mask,\n",
        "        )\n",
        "\n",
        "        # 옵티마이저의 기울기 초기화\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 손실 계산 (logits의 shape을 맞춰서 타겟 출력과 비교)\n",
        "        loss = criterion(logits.reshape(-1, logits.shape[-1]), target_output.reshape(-1))\n",
        "\n",
        "        # 훈련 모드일 경우, 역전파 및 옵티마이저 스텝을 수행\n",
        "        if split == \"train\":\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # 손실 값 누적\n",
        "        losses += loss.item()\n",
        "\n",
        "    # 에폭당 평균 손실 값 반환\n",
        "    return losses / len(list(dataloader))\n",
        "\n",
        "# 5 에폭 동안 모델을 훈련하고 검증 손실 출력\n",
        "for epoch in range(5):\n",
        "    train_loss = run(model, optimizer, criterion, \"train\")  # 훈련 손실\n",
        "    val_loss = run(model, optimizer, criterion, \"valid\")   # 검증 손실\n",
        "    print(f\"Epoch: {epoch+1}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfb49846-9505-4327-b42a-0cc4d72a0a4e",
      "metadata": {
        "id": "cfb49846-9505-4327-b42a-0cc4d72a0a4e"
      },
      "outputs": [],
      "source": [
        "# 그리디 디코딩 방식으로 번역을 수행하는 함수\n",
        "def greedy_decode(model, source_tensor, source_mask, max_len, start_symbol):\n",
        "    # 소스 텐서와 마스크를 GPU로 이동\n",
        "    source_tensor = source_tensor.to(DEVICE)\n",
        "    source_mask = source_mask.to(DEVICE)\n",
        "\n",
        "    # 모델의 인코더를 통해 소스 텍스트에 대한 메모리 계산\n",
        "    memory = model.encode(source_tensor, source_mask)\n",
        "\n",
        "    # 시작 토큰으로 초기화된 ys 텐서 생성 (크기 1x1)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "\n",
        "    # 최대 길이까지 번역 생성\n",
        "    for i in range(max_len - 1):\n",
        "        memory = memory.to(DEVICE)\n",
        "\n",
        "        # 타겟 마스크를 생성\n",
        "        target_mask = generate_square_subsequent_mask(ys.size(0))\n",
        "        target_mask = target_mask.type(torch.bool).to(DEVICE)\n",
        "\n",
        "        # 디코더의 출력을 계산\n",
        "        out = model.decode(ys, memory, target_mask)\n",
        "        out = out.transpose(0, 1)  # 차원 순서를 바꿈\n",
        "        prob = model.generator(out[:, -1])  # 마지막 토큰의 예측 확률을 얻음\n",
        "\n",
        "        # 가장 확률이 높은 단어를 선택\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.item()  # 단어의 인덱스를 가져옴\n",
        "\n",
        "        # 선택된 단어를 ys에 추가\n",
        "        ys = torch.cat(\n",
        "            [ys, torch.ones(1, 1).type_as(source_tensor.data).fill_(next_word)], dim=0\n",
        "        )\n",
        "\n",
        "        # EOS 토큰이 나오면 종료\n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "\n",
        "    return ys\n",
        "\n",
        "\n",
        "# 주어진 문장을 번역하는 함수\n",
        "def translate(model, source_sentence):\n",
        "    model.eval()  # 모델을 평가 모드로 설정\n",
        "    # 소스 문장을 텍스트 변환기에 입력하여 텐서로 변환\n",
        "    source_tensor = text_transform[SRC_LANGUAGE](source_sentence).view(-1, 1)\n",
        "    num_tokens = source_tensor.shape[0]\n",
        "\n",
        "    # 소스 문장의 마스크를 생성 (패딩 마스크)\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "\n",
        "    # 그리디 디코딩을 통해 타겟 토큰을 생성\n",
        "    tgt_tokens = greedy_decode(\n",
        "        model, source_tensor, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX\n",
        "    ).flatten()\n",
        "\n",
        "    # 결과 토큰을 실제 단어로 변환하고, BOS와 EOS 토큰을 제외\n",
        "    output = vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))[1:-1]\n",
        "    return \" \".join(output)\n",
        "\n",
        "\n",
        "# 예시 문장 번역\n",
        "output_oov = translate(model, \"Eine Gruppe von Menschen steht vor einem Iglu .\")  # OOV 단어 포함 예\n",
        "output = translate(model, \"Eine Gruppe von Menschen steht vor einem Gebäude .\")  # 일반적인 문장\n",
        "\n",
        "# 번역 결과 출력\n",
        "print(output_oov)\n",
        "print(output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPT실습"
      ],
      "metadata": {
        "id": "hhCt25A-QguK"
      },
      "id": "hhCt25A-QguK"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel\n",
        "\n",
        "# GPT2 모델 로드\n",
        "model = GPT2LMHeadModel.from_pretrained(pretrained_model_name_or_path=\"gpt2\")\n",
        "\n",
        "# 모델의 모든 계층을 출력\n",
        "for main_name, main_module in model.named_children():\n",
        "    print(main_name)  # 주 계층의 이름 출력\n",
        "    for sub_name, sub_module in main_module.named_children():\n",
        "        print(\"└\", sub_name)  # 하위 계층의 이름 출력\n",
        "        for ssub_name, ssub_module in sub_module.named_children():\n",
        "            print(\"│  └\", ssub_name)  # 하위의 하위 계층의 이름 출력\n",
        "            for sssub_name, sssub_module in ssub_module.named_children():\n",
        "                print(\"│  │  └\", sssub_name)  # 하위 계층의 더 깊은 하위 계층 이름 출력\n"
      ],
      "metadata": {
        "id": "ydMZioKTQDIs"
      },
      "id": "ydMZioKTQDIs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# 'text-generation' 작업을 위한 파이프라인을 생성, 모델은 'gpt2'를 사용\n",
        "generator = pipeline(task=\"text-generation\", model=\"gpt2\")\n",
        "\n",
        "# 텍스트 생성 수행\n",
        "outputs = generator(\n",
        "    text_inputs=\"Machine learning is\",  # 텍스트 생성 시작 문장\n",
        "    max_length=20,  # 생성할 최대 텍스트 길이\n",
        "    num_return_sequences=3,  # 생성할 텍스트 시퀀스의 개수\n",
        "    pad_token_id=generator.tokenizer.eos_token_id  # padding에 사용할 토큰 ID 설정 (끝 토큰으로 설정)\n",
        ")\n",
        "\n",
        "# 생성된 텍스트 출력\n",
        "print(outputs)\n"
      ],
      "metadata": {
        "id": "WGXj1CCaQoKm"
      },
      "id": "WGXj1CCaQoKm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT 실습"
      ],
      "metadata": {
        "id": "mx31kqcIQuVe"
      },
      "id": "mx31kqcIQuVe"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from Korpora import Korpora\n",
        "\n",
        "\n",
        "corpus = Korpora.load(\"nsmc\")\n",
        "df = pd.DataFrame(corpus.test).sample(20000, random_state=42)"
      ],
      "metadata": {
        "id": "kKYHY-gjQoNh"
      },
      "id": "kKYHY-gjQoNh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, valid, test = np.split(\n",
        "    df.sample(frac=1, random_state=42), [int(0.6 * len(df)), int(0.8 * len(df))]\n",
        ")\n",
        "\n",
        "print(train.head(5).to_markdown())\n",
        "print(f\"Training Data Size : {len(train)}\")\n",
        "print(f\"Validation Data Size : {len(valid)}\")\n",
        "print(f\"Testing Data Size : {len(test)}\")"
      ],
      "metadata": {
        "id": "VXSd5OxhQoQp"
      },
      "id": "VXSd5OxhQoQp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.utils.data import RandomSampler, SequentialSampler\n",
        "\n",
        "\n",
        "def make_dataset(data, tokenizer, device):\n",
        "    tokenized = tokenizer(\n",
        "        text=data.text.tolist(),\n",
        "        padding=\"longest\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    input_ids = tokenized[\"input_ids\"].to(device)\n",
        "    attention_mask = tokenized[\"attention_mask\"].to(device)\n",
        "    labels = torch.tensor(data.label.values, dtype=torch.long).to(device)\n",
        "    return TensorDataset(input_ids, attention_mask, labels)\n",
        "\n",
        "\n",
        "def get_datalodader(dataset, sampler, batch_size):\n",
        "    data_sampler = sampler(dataset)\n",
        "    dataloader = DataLoader(dataset, sampler=data_sampler, batch_size=batch_size)\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "epochs = 5\n",
        "batch_size = 32\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "tokenizer = BertTokenizer.from_pretrained(\n",
        "    pretrained_model_name_or_path=\"bert-base-multilingual-cased\",\n",
        "    do_lower_case=False\n",
        ")\n",
        "\n",
        "train_dataset = make_dataset(train, tokenizer, device)\n",
        "train_dataloader = get_datalodader(train_dataset, RandomSampler, batch_size)\n",
        "\n",
        "valid_dataset = make_dataset(valid, tokenizer, device)\n",
        "valid_dataloader = get_datalodader(valid_dataset, SequentialSampler, batch_size)\n",
        "\n",
        "test_dataset = make_dataset(test, tokenizer, device)\n",
        "test_dataloader = get_datalodader(test_dataset, SequentialSampler, batch_size)\n",
        "\n",
        "print(train_dataset[0])"
      ],
      "metadata": {
        "id": "Fql-U3b1QoTW"
      },
      "id": "Fql-U3b1QoTW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    pretrained_model_name_or_path=\"bert-base-multilingual-cased\",\n",
        "    num_labels=2\n",
        ").to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-5, eps=1e-8)"
      ],
      "metadata": {
        "id": "Vze6gXxaQoYC"
      },
      "id": "Vze6gXxaQoYC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for main_name, main_module in model.named_children():\n",
        "    print(main_name)\n",
        "    for sub_name, sub_module in main_module.named_children():\n",
        "        print(\"└\", sub_name)\n",
        "        for ssub_name, ssub_module in sub_module.named_children():\n",
        "            print(\"│  └\", ssub_name)\n",
        "            for sssub_name, sssub_module in ssub_module.named_children():\n",
        "                print(\"│  │  └\", sssub_name)"
      ],
      "metadata": {
        "id": "OF93nub9Qoa1"
      },
      "id": "OF93nub9Qoa1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "def calc_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "def train(model, optimizer, dataloader):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for input_ids, attention_mask, labels in dataloader:\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "\n",
        "        loss = outputs.loss\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    train_loss = train_loss / len(dataloader)\n",
        "    return train_loss\n",
        "\n",
        "def evaluation(model, dataloader):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        val_loss, val_accuracy = 0.0, 0.0\n",
        "\n",
        "        for input_ids, attention_mask, labels in dataloader:\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            logits = outputs.logits\n",
        "\n",
        "            loss = criterion(logits, labels)\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            label_ids = labels.to(\"cpu\").numpy()\n",
        "            accuracy = calc_accuracy(logits, label_ids)\n",
        "\n",
        "            val_loss += loss\n",
        "            val_accuracy += accuracy\n",
        "\n",
        "    val_loss = val_loss/len(dataloader)\n",
        "    val_accuracy = val_accuracy/len(dataloader)\n",
        "    return val_loss, val_accuracy\n",
        "\n",
        "\n",
        "best_loss = 10000\n",
        "for epoch in range(epochs):\n",
        "    train_loss = train(model, optimizer, train_dataloader)\n",
        "    val_loss, val_accuracy = evaluation(model, valid_dataloader)\n",
        "    print(f\"Epoch {epoch + 1}: Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f} Val Accuracy {val_accuracy:.4f}\")\n",
        "\n",
        "    if val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        torch.save(model.state_dict(), \"../models/BertForSequenceClassification.pt\")\n",
        "        print(\"Saved the model weights\")"
      ],
      "metadata": {
        "id": "kGRmLeEIQodh"
      },
      "id": "kGRmLeEIQodh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    pretrained_model_name_or_path=\"bert-base-multilingual-cased\",\n",
        "    num_labels=2\n",
        ").to(device)\n",
        "model.load_state_dict(torch.load(\"../models/BertForSequenceClassification.pt\"))\n",
        "\n",
        "test_loss, test_accuracy = evaluation(model, test_dataloader)\n",
        "print(f\"Test Loss : {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy : {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "R9RlkphTQogk"
      },
      "id": "R9RlkphTQogk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BART 실습"
      ],
      "metadata": {
        "id": "LTtdmYUWRCqI"
      },
      "id": "LTtdmYUWRCqI"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate rouge_score absl-py\n"
      ],
      "metadata": {
        "id": "pkCfr6FnRFxm"
      },
      "id": "pkCfr6FnRFxm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "news = load_dataset(\"argilla/news-summary\", split=\"test\")\n",
        "df = news.to_pandas().sample(5000, random_state=42)[[\"text\", \"prediction\"]]\n",
        "df[\"prediction\"] = df[\"prediction\"].map(lambda x: x[0][\"text\"])\n",
        "train, valid, test = np.split(\n",
        "    df.sample(frac=1, random_state=42), [int(0.6 * len(df)), int(0.8 * len(df))]\n",
        ")\n",
        "\n",
        "print(f\"Source News : {train.text.iloc[0][:200]}\")\n",
        "print(f\"Summarization : {train.prediction.iloc[0][:50]}\")\n",
        "print(f\"Training Data Size : {len(train)}\")\n",
        "print(f\"Validation Data Size : {len(valid)}\")\n",
        "print(f\"Testing Data Size : {len(test)}\")"
      ],
      "metadata": {
        "id": "f9kprZD-RF0M"
      },
      "id": "f9kprZD-RF0M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BartTokenizer\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.utils.data import RandomSampler, SequentialSampler\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "\n",
        "def make_dataset(data, tokenizer, device):\n",
        "    tokenized = tokenizer(\n",
        "        text=data.text.tolist(),\n",
        "        padding=\"longest\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\",\n",
        "        max_length=1024\n",
        "    )\n",
        "    labels = []\n",
        "    input_ids = tokenized[\"input_ids\"].to(device)\n",
        "    attention_mask = tokenized[\"attention_mask\"].to(device)\n",
        "    for target in data.prediction:\n",
        "        labels.append(tokenizer.encode(target, return_tensors=\"pt\").squeeze())\n",
        "    labels = pad_sequence(labels, batch_first=True, padding_value=-100).to(device)\n",
        "    return TensorDataset(input_ids, attention_mask, labels)\n",
        "\n",
        "\n",
        "\n",
        "def get_datalodader(dataset, sampler, batch_size):\n",
        "    data_sampler = sampler(dataset)\n",
        "    dataloader = DataLoader(dataset, sampler=data_sampler, batch_size=batch_size)\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "epochs = 5\n",
        "batch_size = 8\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "tokenizer = BartTokenizer.from_pretrained(\n",
        "    pretrained_model_name_or_path=\"facebook/bart-base\"\n",
        ")\n",
        "\n",
        "train_dataset = make_dataset(train, tokenizer, device)\n",
        "train_dataloader = get_datalodader(train_dataset, RandomSampler, batch_size)\n",
        "\n",
        "valid_dataset = make_dataset(valid, tokenizer, device)\n",
        "valid_dataloader = get_datalodader(valid_dataset, SequentialSampler, batch_size)\n",
        "\n",
        "test_dataset = make_dataset(test, tokenizer, device)\n",
        "test_dataloader = get_datalodader(test_dataset, SequentialSampler, batch_size)\n",
        "\n",
        "print(train_dataset[0])"
      ],
      "metadata": {
        "id": "UpKOxqliRF2i"
      },
      "id": "UpKOxqliRF2i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "from transformers import BartForConditionalGeneration\n",
        "\n",
        "\n",
        "model = BartForConditionalGeneration.from_pretrained(\n",
        "    pretrained_model_name_or_path=\"facebook/bart-base\"\n",
        ").to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=5e-5, eps=1e-8)"
      ],
      "metadata": {
        "id": "RLL55LhLRF5U"
      },
      "id": "RLL55LhLRF5U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for main_name, main_module in model.named_children():\n",
        "    print(main_name)\n",
        "    for sub_name, sub_module in main_module.named_children():\n",
        "        print(\"└\", sub_name)\n",
        "        for ssub_name, ssub_module in sub_module.named_children():\n",
        "            print(\"│  └\", ssub_name)\n",
        "            for sssub_name, sssub_module in ssub_module.named_children():\n",
        "                print(\"│  │  └\", sssub_name)"
      ],
      "metadata": {
        "id": "6Abt9blQRF75"
      },
      "id": "6Abt9blQRF75",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "\n",
        "def calc_rouge(preds, labels):\n",
        "    preds = preds.argmax(axis=-1)\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    rouge2 = rouge_score.compute(\n",
        "        predictions=decoded_preds,\n",
        "        references=decoded_labels\n",
        "    )\n",
        "    return rouge2[\"rouge2\"]\n",
        "\n",
        "def train(model, optimizer, dataloader):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for input_ids, attention_mask, labels in dataloader:\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    train_loss = train_loss / len(dataloader)\n",
        "    return train_loss\n",
        "\n",
        "def evaluation(model, dataloader):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        val_loss, val_rouge = 0.0, 0.0\n",
        "\n",
        "        for input_ids, attention_mask, labels in dataloader:\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "            logits = outputs.logits\n",
        "            loss = outputs.loss\n",
        "\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            label_ids = labels.to(\"cpu\").numpy()\n",
        "            rouge = calc_rouge(logits, label_ids)\n",
        "\n",
        "            val_loss += loss\n",
        "            val_rouge += rouge\n",
        "\n",
        "    val_loss = val_loss / len(dataloader)\n",
        "    val_rouge = val_rouge / len(dataloader)\n",
        "    return val_loss, val_rouge\n",
        "\n",
        "rouge_score = evaluate.load(\"rouge\", tokenizer=tokenizer)\n",
        "best_loss = 10000\n",
        "for epoch in range(epochs):\n",
        "    train_loss = train(model, optimizer, train_dataloader)\n",
        "    val_loss, val_accuracy = evaluation(model, valid_dataloader)\n",
        "    print(f\"Epoch {epoch + 1}: Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f} Val Rouge {val_accuracy:.4f}\")\n",
        "\n",
        "    if val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        torch.save(model.state_dict(), \"../models/BartForConditionalGeneration.pt\")\n",
        "        print(\"Saved the model weights\")"
      ],
      "metadata": {
        "id": "Sm0a4w0MRF-q"
      },
      "id": "Sm0a4w0MRF-q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BartForConditionalGeneration.from_pretrained(\n",
        "    pretrained_model_name_or_path=\"facebook/bart-base\"\n",
        ").to(device)\n",
        "model.load_state_dict(torch.load(\"../models/BartForConditionalGeneration.pt\"))\n",
        "\n",
        "test_loss, test_rouge_score = evaluation(model, test_dataloader)\n",
        "print(f\"Test Loss : {test_loss:.4f}\")\n",
        "print(f\"Test ROUGE-2 Score : {test_rouge_score:.4f}\")"
      ],
      "metadata": {
        "id": "HcUXs7tcRGAz"
      },
      "id": "HcUXs7tcRGAz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "\n",
        "summarizer = pipeline(\n",
        "    task=\"summarization\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=54,\n",
        "    device=\"cpu\"\n",
        ")\n",
        "\n",
        "for index in range(5):\n",
        "    news_text = test.text.iloc[index]\n",
        "    summarization = test.prediction.iloc[index]\n",
        "    predicted_summarization = summarizer(news_text)[0][\"summary_text\"]\n",
        "    print(f\"정답 요약문 : {summarization}\")\n",
        "    print(f\"모델 요약문 : {predicted_summarization}\\n\")"
      ],
      "metadata": {
        "id": "GMXA7R0gRQng"
      },
      "id": "GMXA7R0gRQng",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KoELECTRA 실습"
      ],
      "metadata": {
        "id": "yZUOtCOLRVbo"
      },
      "id": "yZUOtCOLRVbo"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from Korpora import Korpora\n",
        "\n",
        "\n",
        "corpus = Korpora.load(\"nsmc\")\n",
        "df = pd.DataFrame(corpus.test).sample(20000, random_state=42)\n",
        "train, valid, test = np.split(\n",
        "    df.sample(frac=1, random_state=42), [int(0.6 * len(df)), int(0.8 * len(df))]\n",
        ")\n",
        "\n",
        "print(train.head(5).to_markdown())\n",
        "print(f\"Training Data Size : {len(train)}\")\n",
        "print(f\"Validation Data Size : {len(valid)}\")\n",
        "print(f\"Testing Data Size : {len(test)}\")"
      ],
      "metadata": {
        "id": "P_TIXAo8RZWW"
      },
      "id": "P_TIXAo8RZWW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import ElectraTokenizer\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.utils.data import RandomSampler, SequentialSampler\n",
        "\n",
        "\n",
        "def make_dataset(data, tokenizer, device):\n",
        "    tokenized = tokenizer(\n",
        "        text=data.text.tolist(),\n",
        "        padding=\"longest\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    input_ids = tokenized[\"input_ids\"].to(device)\n",
        "    attention_mask = tokenized[\"attention_mask\"].to(device)\n",
        "    labels = torch.tensor(data.label.values, dtype=torch.long).to(device)\n",
        "    return TensorDataset(input_ids, attention_mask, labels)\n",
        "\n",
        "\n",
        "def get_datalodader(dataset, sampler, batch_size):\n",
        "    data_sampler = sampler(dataset)\n",
        "    dataloader = DataLoader(dataset, sampler=data_sampler, batch_size=batch_size)\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "epochs = 5\n",
        "batch_size = 32\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "tokenizer = ElectraTokenizer.from_pretrained(\n",
        "    pretrained_model_name_or_path=\"monologg/koelectra-base-v3-discriminator\",\n",
        "    do_lower_case=False,\n",
        ")\n",
        "\n",
        "train_dataset = make_dataset(train, tokenizer, device)\n",
        "train_dataloader = get_datalodader(train_dataset, RandomSampler, batch_size)\n",
        "\n",
        "valid_dataset = make_dataset(valid, tokenizer, device)\n",
        "valid_dataloader = get_datalodader(valid_dataset, SequentialSampler, batch_size)\n",
        "\n",
        "test_dataset = make_dataset(test, tokenizer, device)\n",
        "test_dataloader = get_datalodader(test_dataset, SequentialSampler, batch_size)\n",
        "\n",
        "print(train_dataset[0])"
      ],
      "metadata": {
        "id": "Zs75bzE9RZZL"
      },
      "id": "Zs75bzE9RZZL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "from transformers import ElectraForSequenceClassification\n",
        "\n",
        "\n",
        "model = ElectraForSequenceClassification.from_pretrained(\n",
        "    pretrained_model_name_or_path=\"monologg/koelectra-base-v3-discriminator\",\n",
        "    num_labels=2\n",
        ").to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-5, eps=1e-8)"
      ],
      "metadata": {
        "id": "rS7QNoMJRZb3"
      },
      "id": "rS7QNoMJRZb3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for main_name, main_module in model.named_children():\n",
        "    print(main_name)\n",
        "    for sub_name, sub_module in main_module.named_children():\n",
        "        print(\"└\", sub_name)\n",
        "        for ssub_name, ssub_module in sub_module.named_children():\n",
        "            print(\"│  └\", ssub_name)\n",
        "            for sssub_name, sssub_module in ssub_module.named_children():\n",
        "                print(\"│  │  └\", sssub_name)"
      ],
      "metadata": {
        "id": "hQERY1p2RZeM"
      },
      "id": "hQERY1p2RZeM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "def calc_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "def train(model, optimizer, dataloader):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for input_ids, attention_mask, labels in dataloader:\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "\n",
        "        loss = outputs.loss\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    train_loss = train_loss / len(dataloader)\n",
        "    return train_loss\n",
        "\n",
        "def evaluation(model, dataloader):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        val_loss, val_accuracy = 0.0, 0.0\n",
        "\n",
        "        for input_ids, attention_mask, labels in dataloader:\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            logits = outputs.logits\n",
        "\n",
        "            loss = criterion(logits, labels)\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            label_ids = labels.to(\"cpu\").numpy()\n",
        "            accuracy = calc_accuracy(logits, label_ids)\n",
        "\n",
        "            val_loss += loss\n",
        "            val_accuracy += accuracy\n",
        "\n",
        "    val_loss = val_loss/len(dataloader)\n",
        "    val_accuracy = val_accuracy/len(dataloader)\n",
        "    return val_loss, val_accuracy\n",
        "\n",
        "\n",
        "best_loss = 10000\n",
        "for epoch in range(epochs):\n",
        "    train_loss = train(model, optimizer, train_dataloader)\n",
        "    val_loss, val_accuracy = evaluation(model, valid_dataloader)\n",
        "    print(f\"Epoch {epoch + 1}: Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f} Val Accuracy {val_accuracy:.4f}\")\n",
        "\n",
        "    if val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        torch.save(model.state_dict(), \"../models/ElectraForSequenceClassification.pt\")\n",
        "        print(\"Saved the model weights\")"
      ],
      "metadata": {
        "id": "Re5L65ziReW-"
      },
      "id": "Re5L65ziReW-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ElectraForSequenceClassification.from_pretrained(\n",
        "    pretrained_model_name_or_path=\"monologg/koelectra-base-v3-discriminator\",\n",
        "    num_labels=2\n",
        ").to(device)\n",
        "model.load_state_dict(torch.load(\"../models/ElectraForSequenceClassification.pt\"))\n",
        "\n",
        "test_loss, test_accuracy = evaluation(model, test_dataloader)\n",
        "print(f\"Test Loss : {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy : {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "6ONaEcJlRZgt"
      },
      "id": "6ONaEcJlRZgt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# T5 실습"
      ],
      "metadata": {
        "id": "qw_HLg_cRpsC"
      },
      "id": "qw_HLg_cRpsC"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "news = load_dataset(\"argilla/news-summary\", split=\"test\")\n",
        "df = news.to_pandas().sample(5000, random_state=42)[[\"text\", \"prediction\"]]\n",
        "df[\"text\"] = \"summarize: \" + df[\"text\"]\n",
        "df[\"prediction\"] = df[\"prediction\"].map(lambda x: x[0][\"text\"])\n",
        "train, valid, test = np.split(\n",
        "    df.sample(frac=1, random_state=42), [int(0.6 * len(df)), int(0.8 * len(df))]\n",
        ")\n",
        "\n",
        "print(f\"Source News : {train.text.iloc[0][:200]}\")\n",
        "print(f\"Summarization : {train.prediction.iloc[0][:50]}\")\n",
        "print(f\"Training Data Size : {len(train)}\")\n",
        "print(f\"Validation Data Size : {len(valid)}\")\n",
        "print(f\"Testing Data Size : {len(test)}\")"
      ],
      "metadata": {
        "id": "1utldB2tRZi4"
      },
      "id": "1utldB2tRZi4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5Tokenizer\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.utils.data import RandomSampler, SequentialSampler\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "\n",
        "def make_dataset(data, tokenizer, device):\n",
        "    source = tokenizer(\n",
        "        text=data.text.tolist(),\n",
        "        padding=\"max_length\",\n",
        "        max_length=128,\n",
        "        pad_to_max_length=True,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    target = tokenizer(\n",
        "        text=data.prediction.tolist(),\n",
        "        padding=\"max_length\",\n",
        "        max_length=128,\n",
        "        pad_to_max_length=True,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    source_ids = source[\"input_ids\"].squeeze().to(device)\n",
        "    source_mask = source[\"attention_mask\"].squeeze().to(device)\n",
        "    target_ids = target[\"input_ids\"].squeeze().to(device)\n",
        "    target_mask = target[\"attention_mask\"].squeeze().to(device)\n",
        "    return TensorDataset(source_ids, source_mask, target_ids, target_mask)\n",
        "\n",
        "def get_datalodader(dataset, sampler, batch_size):\n",
        "    data_sampler = sampler(dataset)\n",
        "    dataloader = DataLoader(dataset, sampler=data_sampler, batch_size=batch_size)\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "epochs = 5\n",
        "batch_size = 8\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(\n",
        "    pretrained_model_name_or_path=\"t5-small\"\n",
        ")\n",
        "\n",
        "train_dataset = make_dataset(train, tokenizer, device)\n",
        "train_dataloader = get_datalodader(train_dataset, RandomSampler, batch_size)\n",
        "\n",
        "valid_dataset = make_dataset(valid, tokenizer, device)\n",
        "valid_dataloader = get_datalodader(valid_dataset, SequentialSampler, batch_size)\n",
        "\n",
        "test_dataset = make_dataset(test, tokenizer, device)\n",
        "test_dataloader = get_datalodader(test_dataset, SequentialSampler, batch_size)\n",
        "\n",
        "print(next(iter(train_dataloader)))\n",
        "print(tokenizer.convert_ids_to_tokens(21603))\n",
        "print(tokenizer.convert_ids_to_tokens(10))"
      ],
      "metadata": {
        "id": "2f1bK0xTRpK2"
      },
      "id": "2f1bK0xTRpK2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "from transformers import T5ForConditionalGeneration\n",
        "\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained(\n",
        "    pretrained_model_name_or_path=\"t5-small\",\n",
        ").to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-5, eps=1e-8)"
      ],
      "metadata": {
        "id": "sy4rtvzmRtIO"
      },
      "id": "sy4rtvzmRtIO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "def calc_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "\n",
        "def train(model, optimizer, dataloader):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for source_ids, source_mask, target_ids, target_mask in dataloader:\n",
        "        decoder_input_ids = target_ids[:, :-1].contiguous()\n",
        "        labels = target_ids[:, 1:].clone().detach()\n",
        "        labels[target_ids[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=source_ids,\n",
        "            attention_mask=source_mask,\n",
        "            decoder_input_ids=decoder_input_ids,\n",
        "            labels=labels,\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    train_loss = train_loss / len(dataloader)\n",
        "    return train_loss\n",
        "\n",
        "\n",
        "def evaluation(model, dataloader):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "\n",
        "        for source_ids, source_mask, target_ids, target_mask in dataloader:\n",
        "            decoder_input_ids = target_ids[:, :-1].contiguous()\n",
        "            labels = target_ids[:, 1:].clone().detach()\n",
        "            labels[target_ids[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=source_ids,\n",
        "                attention_mask=source_mask,\n",
        "                decoder_input_ids=decoder_input_ids,\n",
        "                labels=labels,\n",
        "            )\n",
        "\n",
        "            loss = outputs.loss\n",
        "            val_loss += loss\n",
        "\n",
        "    val_loss = val_loss / len(dataloader)\n",
        "    return val_loss\n",
        "\n",
        "\n",
        "best_loss = 10000\n",
        "for epoch in range(epochs):\n",
        "    train_loss = train(model, optimizer, train_dataloader)\n",
        "    val_loss = evaluation(model, valid_dataloader)\n",
        "    print(f\"Epoch {epoch + 1}: Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    if val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        torch.save(model.state_dict(), \"../models/T5ForConditionalGeneration.pt\")\n",
        "        print(\"Saved the model weights\")"
      ],
      "metadata": {
        "id": "xFfCZtL8RtKu"
      },
      "id": "xFfCZtL8RtKu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for source_ids, source_mask, target_ids, target_mask in test_dataloader:\n",
        "        generated_ids = model.generate(\n",
        "            input_ids=source_ids,\n",
        "            attention_mask=source_mask,\n",
        "            max_length=128,\n",
        "            num_beams=3,\n",
        "            repetition_penalty=2.5,\n",
        "            length_penalty=1.0,\n",
        "            early_stopping=True,\n",
        "        )\n",
        "\n",
        "        for generated, target in zip(generated_ids, target_ids):\n",
        "            pred = tokenizer.decode(\n",
        "                generated, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
        "            )\n",
        "            actual = tokenizer.decode(\n",
        "                target, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
        "            )\n",
        "            print(\"Generated Headline Text:\", pred)\n",
        "            print(\"Actual Headline Text   :\", actual)\n",
        "        break"
      ],
      "metadata": {
        "id": "dsBMv93eRtNz"
      },
      "id": "dsBMv93eRtNz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V5zlkisqRtQb"
      },
      "id": "V5zlkisqRtQb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "29D3vlygRtS_"
      },
      "id": "29D3vlygRtS_",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}