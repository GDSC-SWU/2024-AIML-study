{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KN45lV_qxYAj",
        "outputId": "484e7812-e8de-4304-c079-470366fc70d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['현실과', '구분', '불가능한', 'cg.시각적', '즐거움은', '최고!', '더불어', 'ost는', '더더욱', '최고!!']\n"
          ]
        }
      ],
      "source": [
        "# 5.1 단어 토큰화\n",
        "review= \"현실과 구분 불가능한 cg.시각적 즐거움은 최고! 더불어 ost는 더더욱 최고!!\"\n",
        "tokenized = review.split()\n",
        "print(tokenized)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5.2 글자 토큰화\n",
        "review= \"현실과 구분 불가능한 cg.시각적 즐거움은 최고! 더불어 ost는 더더욱 최고!!\"\n",
        "tokenized = list(review)\n",
        "print(tokenized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysRhcto3x722",
        "outputId": "16876689-6990-4d62-d062-17d2c16867b9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['현', '실', '과', ' ', '구', '분', ' ', '불', '가', '능', '한', ' ', 'c', 'g', '.', '시', '각', '적', ' ', '즐', '거', '움', '은', ' ', '최', '고', '!', ' ', '더', '불', '어', ' ', 'o', 's', 't', '는', ' ', '더', '더', '욱', ' ', '최', '고', '!', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jamo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTJ4t5vQz8Zw",
        "outputId": "cd8c4e55-5e9b-4934-e4a9-7a8d73bf37f5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jamo\n",
            "  Downloading jamo-0.4.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Downloading jamo-0.4.1-py3-none-any.whl (9.5 kB)\n",
            "Installing collected packages: jamo\n",
            "Successfully installed jamo-0.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#자모 변환 함수\n",
        "from jamo import h2j, j2hcj\n",
        "review= \"현실과 구분 불가능한 cg.시각적 즐거움은 최고! 더불어 ost는 더더욱 최고!!\"\n",
        "decomposed = j2hcj(h2j(review))\n",
        "tokenized= list(decomposed)\n",
        "print(tokenized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3NsOoZGy7np",
        "outputId": "d6da6a56-1f30-479c-a141-d65a99cca7f1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ㅎ', 'ㅕ', 'ㄴ', 'ㅅ', 'ㅣ', 'ㄹ', 'ㄱ', 'ㅘ', ' ', 'ㄱ', 'ㅜ', 'ㅂ', 'ㅜ', 'ㄴ', ' ', 'ㅂ', 'ㅜ', 'ㄹ', 'ㄱ', 'ㅏ', 'ㄴ', 'ㅡ', 'ㅇ', 'ㅎ', 'ㅏ', 'ㄴ', ' ', 'c', 'g', '.', 'ㅅ', 'ㅣ', 'ㄱ', 'ㅏ', 'ㄱ', 'ㅈ', 'ㅓ', 'ㄱ', ' ', 'ㅈ', 'ㅡ', 'ㄹ', 'ㄱ', 'ㅓ', 'ㅇ', 'ㅜ', 'ㅁ', 'ㅇ', 'ㅡ', 'ㄴ', ' ', 'ㅊ', 'ㅚ', 'ㄱ', 'ㅗ', '!', ' ', 'ㄷ', 'ㅓ', 'ㅂ', 'ㅜ', 'ㄹ', 'ㅇ', 'ㅓ', ' ', 'o', 's', 't', 'ㄴ', 'ㅡ', 'ㄴ', ' ', 'ㄷ', 'ㅓ', 'ㄷ', 'ㅓ', 'ㅇ', 'ㅜ', 'ㄱ', ' ', 'ㅊ', 'ㅚ', 'ㄱ', 'ㅗ', '!', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SHtP5WUz7gm",
        "outputId": "f2aa2aa3-03b4-46cb-e420-43ad2005e59e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.4)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (24.1)\n",
            "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.6/488.6 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.5.0 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Okt토큰화\n",
        "from konlpy.tag import Okt\n",
        "okt=Okt()\n",
        "\n",
        "sentence= '무엇이든 상상할 수 있는 사람은 무엇이든 만들어 낼 수 있다.'\n",
        "\n",
        "nouns= okt.nouns(sentence)\n",
        "phrases= okt.phrases(sentence)\n",
        "morphs= okt.morphs(sentence)\n",
        "pos= okt.pos(sentence)\n",
        "\n",
        "print('명사추출:', nouns)\n",
        "print('구 추출:', phrases)\n",
        "print('형태소 추출:', morphs)\n",
        "print('품사 태깅:', pos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-S--DnEzqBc",
        "outputId": "5969aac3-4e1e-4607-d626-61d35eccf9a8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "명사추출: ['무엇', '상상', '수', '사람', '무엇', '낼', '수']\n",
            "구 추출: ['무엇', '상상', '상상할 수', '상상할 수 있는 사람', '사람']\n",
            "형태소 추출: ['무엇', '이든', '상상', '할', '수', '있는', '사람', '은', '무엇', '이든', '만들어', '낼', '수', '있다', '.']\n",
            "품사 태깅: [('무엇', 'Noun'), ('이든', 'Josa'), ('상상', 'Noun'), ('할', 'Verb'), ('수', 'Noun'), ('있는', 'Adjective'), ('사람', 'Noun'), ('은', 'Josa'), ('무엇', 'Noun'), ('이든', 'Josa'), ('만들어', 'Verb'), ('낼', 'Noun'), ('수', 'Noun'), ('있다', 'Adjective'), ('.', 'Punctuation')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#꼬꼬마 토큰화\n",
        "from konlpy.tag import Kkma\n",
        "kkma=Kkma()\n",
        "sentence='무엇이든 상상할 수 있는 사람은 무엇이든 만들어 낼 수 있다.'\n",
        "nouns= kkma.nouns(sentence)\n",
        "sentences= kkma.sentences(sentence)\n",
        "morphs= kkma.morphs(sentence)\n",
        "pos=kkma.pos(sentence)\n",
        "\n",
        "print('명사추출:', nouns)\n",
        "print('문장 추출:', sentences)\n",
        "print('형태소 추출:', morphs)\n",
        "print('품사 태깅:', pos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_4_1WSR6wfx",
        "outputId": "b6153aac-ad19-41b9-e16a-2d74b603d79c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "명사추출: ['무엇', '상상', '수', '사람', '무엇']\n",
            "문장 추출: ['무엇이든 상상할 수 있는 사람은 무엇이든 만들어 낼 수 있다.']\n",
            "형태소 추출: ['무엇', '이', '든', '상상', '하', 'ㄹ', '수', '있', '는', '사람', '은', '무엇', '이', '든', '만들', '어', '내', 'ㄹ', '수', '있', '다', '.']\n",
            "품사 태깅: [('무엇', 'NNG'), ('이', 'VCP'), ('든', 'ECE'), ('상상', 'NNG'), ('하', 'XSV'), ('ㄹ', 'ETD'), ('수', 'NNB'), ('있', 'VV'), ('는', 'ETD'), ('사람', 'NNG'), ('은', 'JX'), ('무엇', 'NP'), ('이', 'VCP'), ('든', 'ECE'), ('만들', 'VV'), ('어', 'ECD'), ('내', 'VXV'), ('ㄹ', 'ETD'), ('수', 'NNB'), ('있', 'VV'), ('다', 'EFN'), ('.', 'SF')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxGjFgAR7g5x",
        "outputId": "4498cdd9-c21a-4a25-dd0d-0328d78b67d0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#영문 토큰화\n",
        "from nltk import tokenize\n",
        "sentence='Those who can image anything, can create the impossible'\n",
        "\n",
        "word_tokens= tokenize.word_tokenize(sentence)\n",
        "sent_tokens= tokenize.sent_tokenize(sentence)\n",
        "\n",
        "print(word_tokens)\n",
        "print(sent_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXHhw8Nq74dm",
        "outputId": "ac419b09-3af4-4818-f4a4-a4e8a8054ca8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Those', 'who', 'can', 'image', 'anything', ',', 'can', 'create', 'the', 'impossible']\n",
            "['Those who can image anything, can create the impossible']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#영문 품사태깅\n",
        "from nltk import tag\n",
        "from nltk import tokenize\n",
        "\n",
        "sentence='Those who can image anything, can create the impossible'\n",
        "\n",
        "word_tokens= tokenize.word_tokenize(sentence)\n",
        "pos= tag.pos_tag(word_tokens)\n",
        "\n",
        "print(pos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjqKoBqO8PdO",
        "outputId": "1c3c71bd-6ecf-47f5-c795-a8ccf9440be5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Those', 'DT'), ('who', 'WP'), ('can', 'MD'), ('image', 'NN'), ('anything', 'NN'), (',', ','), ('can', 'MD'), ('create', 'VB'), ('the', 'DT'), ('impossible', 'JJ')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#spaCy 설치\n",
        "!pip install spaCy\n",
        "!python -m spacy download en_core_web__sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViqcMLhs8lMD",
        "outputId": "08b9e426-ff1c-4277-c0b8-50b91f478a76"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spaCy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spaCy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spaCy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spaCy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spaCy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spaCy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spaCy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spaCy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spaCy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spaCy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spaCy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spaCy) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spaCy) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spaCy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spaCy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spaCy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spaCy) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spaCy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spaCy) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spaCy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spaCy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spaCy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spaCy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spaCy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spaCy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spaCy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spaCy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spaCy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spaCy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spaCy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spaCy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spaCy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spaCy) (13.9.2)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spaCy) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spaCy) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spaCy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spaCy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spaCy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spaCy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spaCy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spaCy) (0.1.2)\n",
            "\n",
            "\u001b[38;5;1m✘ No compatible package found for 'en_core_web__sm' (spaCy v3.7.5)\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#spaCy 품사태깅\n",
        "import spacy\n",
        "\n",
        "nlp= spacy.load(\"en_core_web_sm\")\n",
        "sentence='Those who can image anything, can create the impossible'\n",
        "\n",
        "doc= nlp(sentence)\n",
        "\n",
        "for token in doc:\n",
        "  print( f\"[{token.pos_:5}-{token.tag_:3}:{token.text}]\" )\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CkMkqOc8wB4",
        "outputId": "2fee2354-12c4-4ca6-ed31-8f62ecb6c645"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PRON -DT :Those]\n",
            "[PRON -WP :who]\n",
            "[AUX  -MD :can]\n",
            "[VERB -VB :image]\n",
            "[PRON -NN :anything]\n",
            "[PUNCT-,  :,]\n",
            "[AUX  -MD :can]\n",
            "[VERB -VB :create]\n",
            "[DET  -DT :the]\n",
            "[ADJ  -JJ :impossible]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#센텐스피스,코포라 라이브러리 설치\n",
        "!pip install sentencepiece Korpora"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Ztd1NWY9R4w",
        "outputId": "a92b2f66-1fba-4a04-bfbb-c7fd88adb2cb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: Korpora in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: dataclasses>=0.6 in /usr/local/lib/python3.10/dist-packages (from Korpora) (0.6)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from Korpora) (1.26.4)\n",
            "Requirement already satisfied: tqdm>=4.46.0 in /usr/local/lib/python3.10/dist-packages (from Korpora) (4.66.5)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from Korpora) (2.32.3)\n",
            "Requirement already satisfied: xlrd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from Korpora) (2.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->Korpora) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->Korpora) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->Korpora) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->Korpora) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#청와대 정원 데이터 다운로드\n",
        "from Korpora import Korpora\n",
        "\n",
        "corpus= Korpora.load(\"korean_petitions\")\n",
        "dataset = corpus.train\n",
        "petition = dataset[0]\n",
        "\n",
        "print('청원 시작일:',petition.begin)\n",
        "print('청원 종료일:', petition.end)\n",
        "print('정원 동의 수:', petition.num_agree)\n",
        "print('정원 범쥐:', petition.category)\n",
        "print('정원제목:',petition.title)\n",
        "print('정원 본문:', petition.text[:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qb-DjYwqnpc7",
        "outputId": "0231e520-d718-446f-8143-d404155c5c5d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
            "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
            "\n",
            "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
            "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
            "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
            "\n",
            "    # Description\n",
            "    Author : Hyunjoong Kim lovit@github\n",
            "    Repository : https://github.com/lovit/petitions_archive\n",
            "    References :\n",
            "\n",
            "    청와대 국민청원 게시판의 데이터를 월별로 수집한 것입니다.\n",
            "    청원은 게시판에 글을 올린 뒤, 한달 간 청원이 진행됩니다.\n",
            "    수집되는 데이터는 청원종료가 된 이후의 데이터이며, 청원 내 댓글은 수집되지 않습니다.\n",
            "    단 청원의 동의 개수는 수집됩니다.\n",
            "    자세한 내용은 위의 repository를 참고하세요.\n",
            "\n",
            "    # License\n",
            "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
            "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[korean_petitions] download petitions_2017-08: 1.84MB [00:00, 14.3MB/s]                           \n",
            "[korean_petitions] download petitions_2017-09: 20.4MB [00:00, 102MB/s]                            \n",
            "[korean_petitions] download petitions_2017-10: 12.0MB [00:00, 73.6MB/s]                           \n",
            "[korean_petitions] download petitions_2017-11: 28.4MB [00:00, 123MB/s]                            \n",
            "[korean_petitions] download petitions_2017-12: 29.0MB [00:00, 106MB/s]                             \n",
            "[korean_petitions] download petitions_2018-01: 43.9MB [00:00, 153MB/s]                            \n",
            "[korean_petitions] download petitions_2018-02: 33.8MB [00:00, 145MB/s]                            \n",
            "[korean_petitions] download petitions_2018-03: 34.3MB [00:00, 150MB/s]                            \n",
            "[korean_petitions] download petitions_2018-04: 35.5MB [00:00, 106MB/s]                            \n",
            "[korean_petitions] download petitions_2018-05: 37.5MB [00:00, 126MB/s]                            \n",
            "[korean_petitions] download petitions_2018-06: 37.8MB [00:00, 162MB/s]                            \n",
            "[korean_petitions] download petitions_2018-07: 40.5MB [00:00, 159MB/s]                            \n",
            "[korean_petitions] download petitions_2018-08: 39.8MB [00:00, 147MB/s]                            \n",
            "[korean_petitions] download petitions_2018-09: 36.1MB [00:00, 113MB/s]                            \n",
            "[korean_petitions] download petitions_2018-10: 38.1MB [00:00, 127MB/s]                            \n",
            "[korean_petitions] download petitions_2018-11: 37.7MB [00:00, 156MB/s]                            \n",
            "[korean_petitions] download petitions_2018-12: 33.0MB [00:00, 142MB/s]                            \n",
            "[korean_petitions] download petitions_2019-01: 34.8MB [00:00, 124MB/s]                            \n",
            "[korean_petitions] download petitions_2019-02: 30.8MB [00:00, 110MB/s]                             \n",
            "[korean_petitions] download petitions_2019-03: 34.9MB [00:00, 129MB/s]                             \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "청원 시작일: 2017-08-25\n",
            "청원 종료일: 2017-09-24\n",
            "정원 동의 수: 88\n",
            "정원 범쥐: 육아/교육\n",
            "정원제목: 학교는 인력센터, 취업센터가 아닙니다. 정말 간곡히 부탁드립니다.\n",
            "정원 본문: 안녕하세요. 현재 사대, 교대 등 교원양성학교들의 예비\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#학습 데이터세트 생성\n",
        "from Korpora import Korpora\n",
        "corpus = Korpora.load('korean_petitions')\n",
        "petitions= corpus.get_all_texts()\n",
        "with open('/content/drive/MyDrive/pytorch-transformer/datasets/corpus.txt', 'w', encoding='utf-8') as f:\n",
        "  for petition in petitions:\n",
        "    f.write(petition+'\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4HCCkgnoFEd",
        "outputId": "53f1f617-0af8-4e34-8e26-759e702fcea0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
            "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
            "\n",
            "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
            "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
            "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
            "\n",
            "    # Description\n",
            "    Author : Hyunjoong Kim lovit@github\n",
            "    Repository : https://github.com/lovit/petitions_archive\n",
            "    References :\n",
            "\n",
            "    청와대 국민청원 게시판의 데이터를 월별로 수집한 것입니다.\n",
            "    청원은 게시판에 글을 올린 뒤, 한달 간 청원이 진행됩니다.\n",
            "    수집되는 데이터는 청원종료가 된 이후의 데이터이며, 청원 내 댓글은 수집되지 않습니다.\n",
            "    단 청원의 동의 개수는 수집됩니다.\n",
            "    자세한 내용은 위의 repository를 참고하세요.\n",
            "\n",
            "    # License\n",
            "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
            "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
            "\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2017-08\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2017-09\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2017-10\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2017-11\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2017-12\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-01\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-02\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-03\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-04\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-05\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-06\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-07\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-08\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-09\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-10\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-11\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-12\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2019-01\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2019-02\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2019-03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentencepiece import SentencePieceTrainer\n",
        "\n",
        "# SentencePieceTrainer 설정\n",
        "SentencePieceTrainer.Train(\n",
        "    \"--input=/content/drive/MyDrive/pytorch-transformer/datasets/corpus.txt \"\n",
        "    \"--model_prefix=petition_bpe \"  # 옵션 구문 수정\n",
        "    \"--vocab_size=8000 \"\n",
        "    \"--model_type=bpe\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "AM5pItMJprB3"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#바이트 페어 인코딩 토큰화\n",
        "from sentencepiece import SentencePieceProcessor\n",
        "\n",
        "tokenizer= SentencePieceProcessor()\n",
        "tokenizer.load('petition_bpe.model')\n",
        "\n",
        "sentence= \"안녕하세요, 토크나이저가 잘 학습 되었군요!\"\n",
        "sentences=['이렇게 입력값을 리스트로 받아서','쉽게 토크나이저를 사용할 수 있답니다']\n",
        "\n",
        "tokenized_sentence= tokenizer.encode_as_pieces(sentence)\n",
        "tokenized_sentences=tokenizer.encode_as_pieces(sentences)"
      ],
      "metadata": {
        "id": "d1RcZ1IRqlxa"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"단일 문장 토큰화:\", tokenized_sentence)\n",
        "print('여러 문장 토큰화:', tokenized_sentences)\n",
        "\n",
        "encoded_sentence= tokenizer.encode_as_ids(sentence)\n",
        "encoded_sentences= tokenizer.encode_as_ids(sentences)\n",
        "print('단일 문장 정수 인코딩:', encoded_sentence)\n",
        "print('여러 문장 정수 인코딩:', encoded_sentences)\n",
        "\n",
        "decoded_ids= tokenizer.decode_ids(encoded_sentences)\n",
        "decode_pieces= tokenizer.decode_pieces(encoded_sentences)\n",
        "print('정수 인코딩 문장 디코딩:', decoded_ids)\n",
        "print('정수 인코딩 문장 디코딩:', decode_pieces)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAy7Wfnhqlz8",
        "outputId": "d0c8c91f-df06-4b8d-a772-f2804b7d952e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단일 문장 토큰화: ['▁안녕하세요', ',', '▁토', '크', '나', '이', '저', '가', '▁잘', '▁학', '습', '▁되었', '군요', '!']\n",
            "여러 문장 토큰화: [['▁이렇게', '▁입', '력', '값을', '▁리', '스트', '로', '▁받아서'], ['▁쉽게', '▁토', '크', '나', '이', '저', '를', '▁사용할', '▁수', '▁있', '답니다']]\n",
            "단일 문장 정수 인코딩: [667, 6553, 994, 6880, 6544, 6513, 6590, 6523, 161, 110, 6554, 805, 787, 6648]\n",
            "여러 문장 정수 인코딩: [[372, 182, 6677, 4433, 1772, 1613, 6527, 4162], [1681, 994, 6880, 6544, 6513, 6590, 6536, 5852, 19, 5, 2639]]\n",
            "정수 인코딩 문장 디코딩: ['이렇게 입력값을 리스트로 받아서', '쉽게 토크나이저를 사용할 수 있답니다']\n",
            "정수 인코딩 문장 디코딩: ['이렇게 입력값을 리스트로 받아서', '쉽게 토크나이저를 사용할 수 있답니다']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#어휘사전 불러오기\n",
        "from sentencepiece import SentencePieceProcessor\n",
        "\n",
        "tokenizer= SentencePieceProcessor()\n",
        "tokenizer.load('petition_bpe.model')\n",
        "\n",
        "vocab= {idx: tokenizer.id_to_piece(idx) for idx in range(tokenizer.get_piece_size())}\n",
        "print(list(vocab.items())[:5])\n",
        "print(\"vocab size :\", len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afAdei5Uql2q",
        "outputId": "cd830aa4-2b1e-469c-98b1-5c20f2aeb1de"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, '<unk>'), (1, '<s>'), (2, '</s>'), (3, '니다'), (4, '▁이')]\n",
            "vocab size : 8000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDwwkG_-ql5O",
        "outputId": "5569bce6-0b7c-46fb-a71e-fc8a74115aa8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers) (0.24.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordPiece\n",
        "from tokenizers.normalizers import Sequence, NFD, Lowercase\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "tokenizer= Tokenizer(WordPiece())\n",
        "tokenizer.normalizer= Sequence([NFD(), Lowercase()])\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "tokenizer.train(['/content/drive/MyDrive/pytorch-transformer/datasets/corpus.txt'])\n",
        "tokenizer.save('/content/drive/MyDrive/pytorch-transformer/datasets/models/petition_wordpiece.json')"
      ],
      "metadata": {
        "id": "v0UuMfqhql8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#워드피스 토큰화\n",
        "from tokenizer import Tokenizer\n",
        "from tokenizers.decodes import WordPiece as WordPieceDecoder\n",
        "\n",
        "tokenizer= Tokenizer.from_file('')\n",
        "tokenizer.decoder=WordPieceDecoder()\n",
        "\n",
        "sentence= \"안녕하세요, 토크나이저가 잘 학습 되었군요!\"\n",
        "sentences=['이렇게 입력값을 리스트로 받아서','쉽게 토크나이저를 사용할 수 있답니다']\n",
        "\n",
        "encoded_sentence= tokenizer.encode(sentence)\n",
        "encoded_sentences=tokenizer.encode_batch(sentences)\n",
        "\n",
        "\n",
        "print('인코더 형식', type(encoded_sentence))\n",
        "\n",
        "print('단일 문장 토큰화', encoded_sentence.tokens)\n",
        "print('여러 문장 토큰화', [enc tokens for enc in encoded_sentences])\n",
        "\n",
        "print('단일 문장 정수 인코딩', encoded_sentence.ids)\n",
        "print('여러 문장 정수 인코딩', [enc.ids for enc in encoded_sentences])\n",
        "\n",
        "print('정수 인코딩 문장 변환', tokenized.decode(encoded_sentence.ids))"
      ],
      "metadata": {
        "id": "ebig0xZ8viEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j7HyaJoAql-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CISJopmEqmBE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}