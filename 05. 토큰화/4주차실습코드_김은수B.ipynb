{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fccdac7-4e3d-476a-997a-bc91dd30b52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['현실과', '구분', '불가능한', 'cg.', '시각적', '즐거음은', '최고!', '더불어', 'ost는', '더더욱', '최고!!']\n"
     ]
    }
   ],
   "source": [
    "# 05장 토큰화/예제 5.01~5.02 단어 및 글자 토큰화.ipynb\n",
    "\n",
    "review = \" 현실과 구분 불가능한 cg. 시각적 즐거음은 최고! 더불어 ost는 더더욱 최고!!\"\n",
    "tokenized = review.split()\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9600854d-7caf-422b-9e7f-9d5825fafa5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['현', '실', '과', ' ', '구', '분', ' ', '불', '가', '능', '한', ' ', 'c', 'g', '.', ' ', '시', '각', '적', ' ', '즐', '거', '음', '은', ' ', '최', '고', '!', ' ', '더', '불', '어', ' ', 'o', 's', 't', '는', ' ', '더', '더', '욱', ' ', '최', '고', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "# 05장 토큰화/예제 5.01~5.02 단어 및 글자 토큰화.ipynb\n",
    "\n",
    "review = \"현실과 구분 불가능한 cg. 시각적 즐거음은 최고! 더불어 ost는 더더욱 최고!!\"\n",
    "tokenized = list(review)\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9a07093-0b75-4cf3-837e-fbe78a99709e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ㅎ', 'ㅕ', 'ㄴ', 'ㅅ', 'ㅣ', 'ㄹ', 'ㄱ', 'ㅘ', ' ', 'ㄱ', 'ㅜ', 'ㅂ', 'ㅜ', 'ㄴ', ' ', 'ㅂ', 'ㅜ', 'ㄹ', 'ㄱ', 'ㅏ', 'ㄴ', 'ㅡ', 'ㅇ', 'ㅎ', 'ㅏ', 'ㄴ', ' ', 'c', 'g', '.', ' ', 'ㅅ', 'ㅣ', 'ㄱ', 'ㅏ', 'ㄱ', 'ㅈ', 'ㅓ', 'ㄱ', ' ', 'ㅈ', 'ㅡ', 'ㄹ', 'ㄱ', 'ㅓ', 'ㅇ', 'ㅡ', 'ㅁ', 'ㅇ', 'ㅡ', 'ㄴ', ' ', 'ㅊ', 'ㅚ', 'ㄱ', 'ㅗ', '!', ' ', 'ㄷ', 'ㅓ', 'ㅂ', 'ㅜ', 'ㄹ', 'ㅇ', 'ㅓ', ' ', 'o', 's', 't', 'ㄴ', 'ㅡ', 'ㄴ', ' ', 'ㄷ', 'ㅓ', 'ㄷ', 'ㅓ', 'ㅇ', 'ㅜ', 'ㄱ', ' ', 'ㅊ', 'ㅚ', 'ㄱ', 'ㅗ', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "# 05장 토큰화/예제 5.03 자소 단위 토큰화.ipynb\n",
    "from jamo import h2j, j2hcj\n",
    "review = \"현실과 구분 불가능한 cg. 시각적 즐거음은 최고! 더불어 ost는 더더욱 최고!!\"\n",
    "decomposed = j2hcj(h2j(review))\n",
    "tokenized = list(decomposed)\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f2b57f5-f0af-4cf9-b9f8-42590e85a373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "명사 추출 : ['무엇', '상상', '수', '사람', '무엇', '낼', '수']\n",
      "구 추출 : ['무엇', '상상', '상상할 수', '상상할 수 있는 사람', '사람']\n",
      "형태소 추출 : ['무엇', '이든', '상상', '할', '수', '있는', '사람', '은', '무엇', '이든', '만들어', '낼', '수', '있다', '.']\n",
      "품사 태깅 : [('무엇', 'Noun'), ('이든', 'Josa'), ('상상', 'Noun'), ('할', 'Verb'), ('수', 'Noun'), ('있는', 'Adjective'), ('사람', 'Noun'), ('은', 'Josa'), ('무엇', 'Noun'), ('이든', 'Josa'), ('만들어', 'Verb'), ('낼', 'Noun'), ('수', 'Noun'), ('있다', 'Adjective'), ('.', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "# 05장 토큰화/예제 5.04~5.05 KoNLPy 토큰화.ipynb\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "sentence = \"무엇이든 상상할 수 있는 사람은 무엇이든 만들어 낼 수 있다.\"\n",
    "\n",
    "nouns = okt.nouns(sentence)\n",
    "phrases = okt.phrases(sentence)\n",
    "morphs = okt.morphs(sentence)\n",
    "pos = okt.pos(sentence)\n",
    "\n",
    "print(\"명사 추출 :\", nouns)\n",
    "print(\"구 추출 :\", phrases)\n",
    "print(\"형태소 추출 :\", morphs)\n",
    "print(\"품사 태깅 :\", pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73c6a1aa-5daf-44bf-947b-a1a7717c2797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "명사 추출 : ['무엇', '상상', '수', '사람', '무엇']\n",
      "문장 추출 : ['무엇이든 상상할 수 있는 사람은 무엇이든 만들어 낼 수 있다.']\n",
      "형태소 추출 : ['무엇', '이', '든', '상상', '하', 'ㄹ', '수', '있', '는', '사람', '은', '무엇', '이', '든', '만들', '어', '내', 'ㄹ', '수', '있', '다', '.']\n",
      "품사 태깅 : [('무엇', 'NNG'), ('이', 'VCP'), ('든', 'ECE'), ('상상', 'NNG'), ('하', 'XSV'), ('ㄹ', 'ETD'), ('수', 'NNB'), ('있', 'VV'), ('는', 'ETD'), ('사람', 'NNG'), ('은', 'JX'), ('무엇', 'NP'), ('이', 'VCP'), ('든', 'ECE'), ('만들', 'VV'), ('어', 'ECD'), ('내', 'VXV'), ('ㄹ', 'ETD'), ('수', 'NNB'), ('있', 'VV'), ('다', 'EFN'), ('.', 'SF')]\n"
     ]
    }
   ],
   "source": [
    "# 05장 토큰화/예제 5.04~5.05 KoNLPy 토큰화.ipynb\n",
    "from konlpy.tag import Kkma\n",
    "\n",
    "kkma = Kkma()\n",
    "\n",
    "sentence = \"무엇이든 상상할 수 있는 사람은 무엇이든 만들어 낼 수 있다.\"\n",
    "\n",
    "nouns = kkma.nouns(sentence)\n",
    "sentences = kkma.sentences(sentence)\n",
    "morphs = kkma.morphs(sentence)\n",
    "pos = kkma.pos(sentence)\n",
    "\n",
    "print(\"명사 추출 :\", nouns)\n",
    "print(\"문장 추출 :\", sentences)\n",
    "print(\"형태소 추출 :\", morphs)\n",
    "print(\"품사 태깅 :\", pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aeb14140-9405-4b6f-a28f-18b7c557ff9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 05장 토큰화/예제 5.06~5.08 NLTK 실습.ipynb\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db22c641-d1e0-4298-87ed-5d69edcab19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Those', 'who', 'can', 'imagine', 'anything', ',', 'can', 'create', 'the', 'impossible', '.']\n",
      "['Those who can imagine anything, can create the impossible.']\n"
     ]
    }
   ],
   "source": [
    "# 05장 토큰화/예제 5.06~5.08 NLTK 실습.ipynb\n",
    "from nltk import tokenize\n",
    "\n",
    "sentence = \"Those who can imagine anything, can create the impossible.\"\n",
    "\n",
    "word_tokens = tokenize.word_tokenize(sentence)\n",
    "sent_tokens = tokenize.sent_tokenize(sentence)\n",
    "\n",
    "print(word_tokens)\n",
    "print(sent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73c15830-143e-497c-8f7b-b66cb69a141a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Those', 'DT'), ('who', 'WP'), ('can', 'MD'), ('imagine', 'VB'), ('anything', 'NN'), (',', ','), ('can', 'MD'), ('create', 'VB'), ('the', 'DT'), ('impossible', 'JJ'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# 05장 토큰화/예제 5.06~5.08 NLTK 실습.ipynb\n",
    "from nltk import tag\n",
    "from nltk import tokenize\n",
    "\n",
    "sentence = \"Those who can imagine anything, can create the impossible.\"\n",
    "\n",
    "word_tokens = tokenize.word_tokenize(sentence)\n",
    "pos = tag.pos_tag(word_tokens)\n",
    "\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "301def19-1e4f-4f49-999d-459bf4390273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PRON  - DT ] : Those\n",
      "[PRON  - WP ] : who\n",
      "[AUX   - MD ] : can\n",
      "[VERB  - VB ] : imagine\n",
      "[PRON  - NN ] : anything\n",
      "[PUNCT - ,  ] : ,\n",
      "[AUX   - MD ] : can\n",
      "[VERB  - VB ] : create\n",
      "[DET   - DT ] : the\n",
      "[ADJ   - JJ ] : impossible\n",
      "[PUNCT - .  ] : .\n"
     ]
    }
   ],
   "source": [
    "# 05장 토큰화/예제 5.09 spaCy 품사 태깅.ipynb\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "sentence = \"Those who can imagine anything, can create the impossible.\"\n",
    "doc = nlp(sentence)\n",
    "\n",
    "for token in doc:\n",
    "    print(f\"[{token.pos_:5} - {token.tag_:3}] : {token.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "594a7aa4-4174-43e3-884f-a07e57b553a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : Hyunjoong Kim lovit@github\n",
      "    Repository : https://github.com/lovit/petitions_archive\n",
      "    References :\n",
      "\n",
      "    청와대 국민청원 게시판의 데이터를 월별로 수집한 것입니다.\n",
      "    청원은 게시판에 글을 올린 뒤, 한달 간 청원이 진행됩니다.\n",
      "    수집되는 데이터는 청원종료가 된 이후의 데이터이며, 청원 내 댓글은 수집되지 않습니다.\n",
      "    단 청원의 동의 개수는 수집됩니다.\n",
      "    자세한 내용은 위의 repository를 참고하세요.\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[korean_petitions] download petitions_2017-08: 1.84MB [00:00, 5.69MB/s]                                                \n",
      "[korean_petitions] download petitions_2017-09: 20.4MB [00:01, 18.4MB/s]                                                \n",
      "[korean_petitions] download petitions_2017-10: 12.0MB [00:00, 17.0MB/s]                                                \n",
      "[korean_petitions] download petitions_2017-11: 28.4MB [00:01, 25.3MB/s]                                                \n",
      "[korean_petitions] download petitions_2017-12: 29.0MB [00:01, 24.2MB/s]                                                \n",
      "[korean_petitions] download petitions_2018-01: 43.9MB [00:01, 26.4MB/s]                                                \n",
      "[korean_petitions] download petitions_2018-02: 33.8MB [00:01, 25.5MB/s]                                                \n",
      "[korean_petitions] download petitions_2018-03: 34.3MB [00:01, 23.3MB/s]                                                \n",
      "[korean_petitions] download petitions_2018-04: 35.5MB [00:01, 23.8MB/s]                                                \n",
      "[korean_petitions] download petitions_2018-05: 37.5MB [00:01, 26.2MB/s]                                                \n",
      "[korean_petitions] download petitions_2018-06: 37.8MB [00:01, 26.8MB/s]                                                \n",
      "[korean_petitions] download petitions_2018-07: 40.5MB [00:01, 25.4MB/s]                                                \n",
      "[korean_petitions] download petitions_2018-08: 39.8MB [00:01, 27.3MB/s]                                                \n",
      "[korean_petitions] download petitions_2018-09: 36.1MB [00:01, 25.3MB/s]                                                \n",
      "[korean_petitions] download petitions_2018-10: 38.1MB [00:01, 25.8MB/s]                                                \n",
      "[korean_petitions] download petitions_2018-11: 37.7MB [00:01, 26.7MB/s]                                                \n",
      "[korean_petitions] download petitions_2018-12: 33.0MB [00:01, 22.2MB/s]                                                \n",
      "[korean_petitions] download petitions_2019-01: 34.8MB [00:01, 26.4MB/s]                                                \n",
      "[korean_petitions] download petitions_2019-02: 30.8MB [00:01, 25.9MB/s]                                                \n",
      "[korean_petitions] download petitions_2019-03: 34.9MB [00:01, 21.0MB/s]                                                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "청원 시작일 : 2017-08-25\n",
      "청원 종료일 : 2017-09-24\n",
      "청원 동의 수 : 88\n",
      "청원 범주 : 육아/교육\n",
      "청원 제목 : 학교는 인력센터, 취업센터가 아닙니다. 정말 간곡히 부탁드립니다.\n",
      "청원 본문 : 안녕하세요. 현재 사대, 교대 등 교원양성학교들의 예비\n"
     ]
    }
   ],
   "source": [
    "# 05장 토큰화/예제 5.10 청와대 청원 데이터 다운로드.ipynb\n",
    "from Korpora import Korpora\n",
    "\n",
    "corpus = Korpora.load(\"korean_petitions\")\n",
    "dataset = corpus.train\n",
    "petition = dataset[0]\n",
    "\n",
    "print(\"청원 시작일 :\", petition.begin)\n",
    "print(\"청원 종료일 :\", petition.end)\n",
    "print(\"청원 동의 수 :\", petition.num_agree)\n",
    "print(\"청원 범주 :\", petition.category)\n",
    "print(\"청원 제목 :\", petition.title)\n",
    "print(\"청원 본문 :\", petition.text[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e16d459-aaba-467b-86e6-f7f54c97a402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : Hyunjoong Kim lovit@github\n",
      "    Repository : https://github.com/lovit/petitions_archive\n",
      "    References :\n",
      "\n",
      "    청와대 국민청원 게시판의 데이터를 월별로 수집한 것입니다.\n",
      "    청원은 게시판에 글을 올린 뒤, 한달 간 청원이 진행됩니다.\n",
      "    수집되는 데이터는 청원종료가 된 이후의 데이터이며, 청원 내 댓글은 수집되지 않습니다.\n",
      "    단 청원의 동의 개수는 수집됩니다.\n",
      "    자세한 내용은 위의 repository를 참고하세요.\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Lenovo\\Korpora\\korean_petitions\\petitions_2017-08\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Lenovo\\Korpora\\korean_petitions\\petitions_2017-09\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Lenovo\\Korpora\\korean_petitions\\petitions_2017-10\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Lenovo\\Korpora\\korean_petitions\\petitions_2017-11\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Lenovo\\Korpora\\korean_petitions\\petitions_2017-12\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Lenovo\\Korpora\\korean_petitions\\petitions_2018-01\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Lenovo\\Korpora\\korean_petitions\\petitions_2018-02\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Lenovo\\Korpora\\korean_petitions\\petitions_2018-03\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Lenovo\\Korpora\\korean_petitions\\petitions_2018-04\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Lenovo\\Korpora\\korean_petitions\\petitions_2018-05\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Lenovo\\Korpora\\korean_petitions\\petitions_2018-06\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Lenovo\\Korpora\\korean_petitions\\petitions_2018-07\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Lenovo\\Korpora\\korean_petitions\\petitions_2018-08\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Lenovo\\Korpora\\korean_petitions\\petitions_2018-09\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Lenovo\\Korpora\\korean_petitions\\petitions_2018-10\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Lenovo\\Korpora\\korean_petitions\\petitions_2018-11\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Lenovo\\Korpora\\korean_petitions\\petitions_2018-12\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Lenovo\\Korpora\\korean_petitions\\petitions_2019-01\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Lenovo\\Korpora\\korean_petitions\\petitions_2019-02\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Lenovo\\Korpora\\korean_petitions\\petitions_2019-03\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../datasets/corpus.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m corpus \u001b[38;5;241m=\u001b[39m Korpora\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkorean_petitions\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m petitions \u001b[38;5;241m=\u001b[39m corpus\u001b[38;5;241m.\u001b[39mget_all_texts()\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../datasets/corpus.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m petition \u001b[38;5;129;01min\u001b[39;00m petitions:\n\u001b[0;32m      8\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(petition \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\GDG_env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../datasets/corpus.txt'"
     ]
    }
   ],
   "source": [
    "# 05장 토큰화/예제 5.11 학습 데이터세트 생성.ipynb\n",
    "from Korpora import Korpora\n",
    "\n",
    "corpus = Korpora.load(\"korean_petitions\")\n",
    "petitions = corpus.get_all_texts()\n",
    "with open(\"../datasets/corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for petition in petitions:\n",
    "        f.write(petition + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd6fe2bc-f31b-446a-a532-e38dcdfcf0b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Not found: \"../datasets/corpus.txt\": No such file or directory Error #2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 05장 토큰화/예제 5.12 토크나이저 모델 학습.ipynb\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentencepiece\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentencePieceTrainer\n\u001b[1;32m----> 4\u001b[0m SentencePieceTrainer\u001b[38;5;241m.\u001b[39mTrain(\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--input=../datasets/corpus.txt\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124m    --model_prefix=../models/petition_bpe\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124m    --vocab_size=8000 model_type=bpe\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sentencepiece\\__init__.py:1047\u001b[0m, in \u001b[0;36mSentencePieceTrainer.Train\u001b[1;34m(arg, logstream, **kwargs)\u001b[0m\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m   1045\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mTrain\u001b[39m(arg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, logstream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1046\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m _LogStream(ostream\u001b[38;5;241m=\u001b[39mlogstream):\n\u001b[1;32m-> 1047\u001b[0m     SentencePieceTrainer\u001b[38;5;241m.\u001b[39m_Train(arg\u001b[38;5;241m=\u001b[39marg, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sentencepiece\\__init__.py:1003\u001b[0m, in \u001b[0;36mSentencePieceTrainer._Train\u001b[1;34m(arg, **kwargs)\u001b[0m\n\u001b[0;32m   1001\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Train Sentencepiece model. Accept both kwargs and legacy string arg.\"\"\"\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(arg) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m-> 1003\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m SentencePieceTrainer\u001b[38;5;241m.\u001b[39m_TrainFromString(arg)\n\u001b[0;32m   1005\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_encode\u001b[39m(value):\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Encode value to CSV..\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sentencepiece\\__init__.py:981\u001b[0m, in \u001b[0;36mSentencePieceTrainer._TrainFromString\u001b[1;34m(arg)\u001b[0m\n\u001b[0;32m    979\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_TrainFromString\u001b[39m(arg):\n\u001b[1;32m--> 981\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _sentencepiece\u001b[38;5;241m.\u001b[39mSentencePieceTrainer__TrainFromString(arg)\n",
      "\u001b[1;31mOSError\u001b[0m: Not found: \"../datasets/corpus.txt\": No such file or directory Error #2"
     ]
    }
   ],
   "source": [
    "# 05장 토큰화/예제 5.12 토크나이저 모델 학습.ipynb\n",
    "from sentencepiece import SentencePieceTrainer\n",
    "\n",
    "SentencePieceTrainer.Train(\n",
    "    \"--input=../datasets/corpus.txt\\\n",
    "    --model_prefix=../models/petition_bpe\\\n",
    "    --vocab_size=8000 model_type=bpe\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1e8f6fd-8272-4917-a9ae-7b71bf294eef",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Not found: \"../models/petition_bpe.model\": No such file or directory Error #2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentencepiece\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentencePieceProcessor\n\u001b[0;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m SentencePieceProcessor()\n\u001b[1;32m----> 5\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../models/petition_bpe.model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m안녕하세요, 토크나이저가 잘 학습되었군요!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m이렇게 입력값을 리스트로 받아서\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m쉽게 토크나이저를 사용할 수 있답니다\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sentencepiece\\__init__.py:961\u001b[0m, in \u001b[0;36mSentencePieceProcessor.Load\u001b[1;34m(self, model_file, model_proto)\u001b[0m\n\u001b[0;32m    959\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_proto:\n\u001b[0;32m    960\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLoadFromSerializedProto(model_proto)\n\u001b[1;32m--> 961\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLoadFromFile(model_file)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sentencepiece\\__init__.py:316\u001b[0m, in \u001b[0;36mSentencePieceProcessor.LoadFromFile\u001b[1;34m(self, arg)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mLoadFromFile\u001b[39m(\u001b[38;5;28mself\u001b[39m, arg):\n\u001b[1;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _sentencepiece\u001b[38;5;241m.\u001b[39mSentencePieceProcessor_LoadFromFile(\u001b[38;5;28mself\u001b[39m, arg)\n",
      "\u001b[1;31mOSError\u001b[0m: Not found: \"../models/petition_bpe.model\": No such file or directory Error #2"
     ]
    }
   ],
   "source": [
    "# 05장 토큰화/예제 5.13 바이트 페어 인코딩 토큰화.ipynb\n",
    "from sentencepiece import SentencePieceProcessor\n",
    "\n",
    "tokenizer = SentencePieceProcessor()\n",
    "tokenizer.load(\"../models/petition_bpe.model\")\n",
    "\n",
    "sentence = \"안녕하세요, 토크나이저가 잘 학습되었군요!\"\n",
    "sentences = [\"이렇게 입력값을 리스트로 받아서\", \"쉽게 토크나이저를 사용할 수 있답니다\"]\n",
    "\n",
    "tokenized_sentence = tokenizer.encode_as_pieces(sentence)\n",
    "tokenized_sentences = tokenizer.encode_as_pieces(sentences)\n",
    "print(\"단일 문장 토큰화 :\", tokenized_sentence)\n",
    "print(\"여러 문장 토큰화 :\", tokenized_sentences)\n",
    "\n",
    "encoded_sentence = tokenizer.encode_as_ids(sentence)\n",
    "encoded_sentences = tokenizer.encode_as_ids(sentences)\n",
    "print(\"단일 문장 정수 인코딩 :\", encoded_sentence)\n",
    "print(\"여러 문장 정수 인코딩 :\", encoded_sentences)\n",
    "\n",
    "decode_ids = tokenizer.decode_ids(encoded_sentences)\n",
    "decode_pieces = tokenizer.decode_pieces(encoded_sentences)\n",
    "print(\"정수 인코딩에서 문장 변환 :\", decode_ids)\n",
    "print(\"하위 단어 토큰에서 문장 변환 :\", decode_pieces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "569ef4ed-a9cf-4727-939b-5dd124a1a7da",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Not found: \"../models/petition_bpe.model\": No such file or directory Error #2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentencepiece\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentencePieceProcessor\n\u001b[0;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m SentencePieceProcessor()\n\u001b[1;32m----> 5\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../models/petition_bpe.model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m vocab \u001b[38;5;241m=\u001b[39m {idx: tokenizer\u001b[38;5;241m.\u001b[39mid_to_piece(idx) \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mget_piece_size())}\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlist\u001b[39m(vocab\u001b[38;5;241m.\u001b[39mitems())[:\u001b[38;5;241m5\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sentencepiece\\__init__.py:961\u001b[0m, in \u001b[0;36mSentencePieceProcessor.Load\u001b[1;34m(self, model_file, model_proto)\u001b[0m\n\u001b[0;32m    959\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_proto:\n\u001b[0;32m    960\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLoadFromSerializedProto(model_proto)\n\u001b[1;32m--> 961\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLoadFromFile(model_file)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sentencepiece\\__init__.py:316\u001b[0m, in \u001b[0;36mSentencePieceProcessor.LoadFromFile\u001b[1;34m(self, arg)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mLoadFromFile\u001b[39m(\u001b[38;5;28mself\u001b[39m, arg):\n\u001b[1;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _sentencepiece\u001b[38;5;241m.\u001b[39mSentencePieceProcessor_LoadFromFile(\u001b[38;5;28mself\u001b[39m, arg)\n",
      "\u001b[1;31mOSError\u001b[0m: Not found: \"../models/petition_bpe.model\": No such file or directory Error #2"
     ]
    }
   ],
   "source": [
    "# 05장 토큰화/예제 5.14 어휘 사전 불러오기.ipynb\n",
    "from sentencepiece import SentencePieceProcessor\n",
    "\n",
    "tokenizer = SentencePieceProcessor()\n",
    "tokenizer.load(\"../models/petition_bpe.model\")\n",
    "\n",
    "vocab = {idx: tokenizer.id_to_piece(idx) for idx in range(tokenizer.get_piece_size())}\n",
    "print(list(vocab.items())[:5])\n",
    "print(\"vocab size :\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e0658e8-35dc-4fb5-b771-4175b468505a",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "지정된 경로를 찾을 수 없습니다. (os error 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mnormalizer \u001b[38;5;241m=\u001b[39m Sequence([NFD(), Lowercase()])\n\u001b[0;32m      9\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpre_tokenizer \u001b[38;5;241m=\u001b[39m Whitespace()\n\u001b[1;32m---> 11\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mtrain([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../datasets/corpus.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     12\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../models/petition_wordpiece.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mException\u001b[0m: 지정된 경로를 찾을 수 없습니다. (os error 3)"
     ]
    }
   ],
   "source": [
    "# 05장 토큰화/예제 5.15 워드피스 토크나이저 학습.ipynb\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.normalizers import Sequence, NFD, Lowercase\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "tokenizer = Tokenizer(WordPiece())\n",
    "tokenizer.normalizer = Sequence([NFD(), Lowercase()])\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "tokenizer.train([\"../datasets/corpus.txt\"])\n",
    "tokenizer.save(\"../models/petition_wordpiece.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a4f9ad3-3617-42b9-a3ab-3c8766cf9aae",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "지정된 경로를 찾을 수 없습니다. (os error 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtokenizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtokenizers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecoders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordPiece \u001b[38;5;28;01mas\u001b[39;00m WordPieceDecoder\n\u001b[1;32m----> 5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer\u001b[38;5;241m.\u001b[39mfrom_file(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../models/petition_wordpiece.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m WordPieceDecoder()\n\u001b[0;32m      8\u001b[0m sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m안녕하세요, 토크나이저가 잘 학습되었군요!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mException\u001b[0m: 지정된 경로를 찾을 수 없습니다. (os error 3)"
     ]
    }
   ],
   "source": [
    "# 05장 토큰화/예제 5.16 워드피스 토큰화.ipynb\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.decoders import WordPiece as WordPieceDecoder\n",
    "\n",
    "tokenizer = Tokenizer.from_file(\"../models/petition_wordpiece.json\")\n",
    "tokenizer.decoder = WordPieceDecoder()\n",
    "\n",
    "sentence = \"안녕하세요, 토크나이저가 잘 학습되었군요!\"\n",
    "sentences = [\"이렇게 입력값을 리스트로 받아서\", \"쉽게 토크나이저를 사용할 수 있답니다\"]\n",
    "\n",
    "encoded_sentence = tokenizer.encode(sentence)\n",
    "encoded_sentences = tokenizer.encode_batch(sentences)\n",
    "\n",
    "print(\"인코더 형식 :\", type(encoded_sentence))\n",
    "\n",
    "print(\"단일 문장 토큰화 :\", encoded_sentence.tokens)\n",
    "print(\"여러 문장 토큰화 :\", [enc.tokens for enc in encoded_sentences])\n",
    "\n",
    "print(\"단일 문장 정수 인코딩 :\", encoded_sentence.ids)\n",
    "print(\"여러 문장 정수 인코딩 :\", [enc.ids for enc in encoded_sentences])\n",
    "\n",
    "print(\"정수 인코딩에서 문장 변환 :\", tokenizer.decode(encoded_sentence.ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cbc153-ada1-4b88-946c-349dbe129932",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GDG_env)",
   "language": "python",
   "name": "gdg_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
