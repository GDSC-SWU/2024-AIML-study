{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMicZzqx1c4x",
        "outputId": "8f353821-46f4-45e3-82b5-7af0bc57fe3c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 14135599746533502644\n",
              " xla_global_id: -1,\n",
              " name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 14626652160\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 9543946156236132401\n",
              " physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\n",
              " xla_global_id: 416903419]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import tensorflow\n",
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9.01~9.09 Faster R-CNN 모델 실습"
      ],
      "metadata": {
        "id": "WoLa6rTN2pjS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 데이터셋 클래스 정의"
      ],
      "metadata": {
        "id": "3E8fFETG2wRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from PIL import Image\n",
        "from pycocotools.coco import COCO\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class COCODataset(Dataset):\n",
        "    def __init__(self, root, train, transform=None):\n",
        "        super().__init__()\n",
        "        # 훈련 데이터인지 검증 데이터인지 설정함\n",
        "        directory = \"train\" if train else \"val\"\n",
        "        # 주어진 경로에서 어노테이션 파일을 불러옴\n",
        "        annotations = os.path.join(root, \"annotations\", f\"{directory}_annotations.json\")\n",
        "\n",
        "        self.coco = COCO(annotations)  # COCO API로 어노테이션 로드함\n",
        "        self.iamge_path = os.path.join(root, directory)  # 이미지 경로 설정함\n",
        "        self.transform = transform  # 이미지 변환 설정\n",
        "\n",
        "        self.categories = self._get_categories()  # 카테고리 이름 로드함\n",
        "        self.data = self._load_data()  # 데이터셋 로드함\n",
        "\n",
        "    def _get_categories(self):\n",
        "        # COCO 데이터셋의 카테고리 정보를 딕셔너리로 저장함\n",
        "        categories = {0: \"background\"}\n",
        "        for category in self.coco.cats.values():\n",
        "            categories[category[\"id\"]] = category[\"name\"]\n",
        "        return categories\n",
        "\n",
        "    def _load_data(self):\n",
        "        # 이미지와 라벨 데이터를 리스트 형태로 로드함\n",
        "        data = []\n",
        "        for _id in self.coco.imgs:\n",
        "            file_name = self.coco.loadImgs(_id)[0][\"file_name\"]  # 파일 이름 가져옴\n",
        "            image_path = os.path.join(self.iamge_path, file_name)  # 파일 경로 생성\n",
        "            image = Image.open(image_path).convert(\"RGB\")  # 이미지를 RGB로 변환\n",
        "\n",
        "            boxes = []  # 박스 좌표 저장할 리스트\n",
        "            labels = []  # 라벨 저장할 리스트\n",
        "            anns = self.coco.loadAnns(self.coco.getAnnIds(_id))  # 이미지에 연결된 어노테이션 가져옴\n",
        "            for ann in anns:\n",
        "                x, y, w, h = ann[\"bbox\"]  # 바운딩 박스 정보 추출\n",
        "\n",
        "                boxes.append([x, y, x + w, y + h])  # 좌표 저장함\n",
        "                labels.append(ann[\"category_id\"])  # 라벨 저장함\n",
        "\n",
        "            # PyTorch 모델에 입력하기 위한 타겟 포맷 생성함\n",
        "            target = {\n",
        "                \"image_id\": torch.LongTensor([_id]),\n",
        "                \"boxes\": torch.FloatTensor(boxes),\n",
        "                \"labels\": torch.LongTensor(labels)\n",
        "            }\n",
        "            data.append([image, target])  # 데이터 리스트에 추가\n",
        "        return data\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # 인덱스에 해당하는 데이터 반환\n",
        "        image, target = self.data[index]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)  # 변환 적용\n",
        "        return image, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)  # 데이터셋 길이 반환"
      ],
      "metadata": {
        "id": "r07zItfR1z4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. 데이터 변환 및 데이터로더 설정"
      ],
      "metadata": {
        "id": "eZ_udFTX2y_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "def collator(batch):\n",
        "    # 배치 데이터 병합 함수 정의함\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "# 이미지 변환 적용\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.PILToTensor(),  # 이미지를 텐서로 변환\n",
        "        transforms.ConvertImageDtype(dtype=torch.float)  # dtype을 float으로 변경\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 데이터셋 정의\n",
        "train_dataset = COCODataset(\"../datasets/coco\", train=True, transform=transform)\n",
        "test_dataset = COCODataset(\"../datasets/coco\", train=False, transform=transform)\n",
        "\n",
        "# 데이터로더 정의\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset, batch_size=4, shuffle=True, drop_last=True, collate_fn=collator\n",
        ")\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset, batch_size=1, shuffle=True, drop_last=True, collate_fn=collator\n",
        ")"
      ],
      "metadata": {
        "id": "gAoBq_5p2ueq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Faster R-CNN 모델 정의"
      ],
      "metadata": {
        "id": "FFecJxYp21tx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models\n",
        "from torchvision import ops\n",
        "from torchvision.models.detection import rpn\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "\n",
        "\n",
        "# VGG16 백본 모델을 가져와 특징 추출기로 활용함\n",
        "backbone = models.vgg16(weights=\"VGG16_Weights.IMAGENET1K_V1\").features\n",
        "backbone.out_channels = 512  # 백본 출력 채널 수 설정\n",
        "\n",
        "# 앵커 생성기 설정\n",
        "anchor_generator = rpn.AnchorGenerator(\n",
        "    sizes=((32, 64, 128, 256, 512),),  # 다양한 크기의 앵커 생성\n",
        "    aspect_ratios=((0.5, 1.0, 2.0),)  # 가로세로 비율 설정\n",
        ")\n",
        "\n",
        "# RoI Pooler 설정 (특징 맵의 크기를 고정함)\n",
        "roi_pooler = ops.MultiScaleRoIAlign(\n",
        "    featmap_names=[\"0\"],  # 사용할 특징 맵 레이어 이름\n",
        "    output_size=(7, 7),  # 출력 크기\n",
        "    sampling_ratio=2  # 샘플링 비율\n",
        ")\n",
        "\n",
        "# 모델 정의\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = FasterRCNN(\n",
        "    backbone=backbone,\n",
        "    num_classes=3,  # 클래스 수 정의 (예: 배경 포함 3개)\n",
        "    rpn_anchor_generator=anchor_generator,\n",
        "    box_roi_pool=roi_pooler\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "kq-kghAh23Jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. 모델 학습 설정 및 훈련 루프\n",
        "\n"
      ],
      "metadata": {
        "id": "TiAt508A24ps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "\n",
        "\n",
        "# 학습 가능한 파라미터를 옵티마이저에 전달함\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)  # SGD 설정\n",
        "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)  # 학습률 감소 스케줄러\n",
        "\n",
        "# 모델 훈련 루프\n",
        "for epoch in range(5):\n",
        "    cost = 0.0\n",
        "    for idx, (images, targets) in enumerate(train_dataloader):\n",
        "        images = list(image.to(device) for image in images)  # 이미지를 장치로 이동\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]  # 타겟도 이동\n",
        "\n",
        "        loss_dict = model(images, targets)  # 손실 계산\n",
        "        losses = sum(loss for loss in loss_dict.values())  # 모든 손실 합산\n",
        "\n",
        "        optimizer.zero_grad()  # 그래디언트 초기화\n",
        "        losses.backward()  # 역전파 수행\n",
        "        optimizer.step()  # 파라미터 업데이트\n",
        "\n",
        "        cost += losses\n",
        "\n",
        "    lr_scheduler.step()  # 학습률 스케줄러 업데이트\n",
        "    cost = cost / len(train_dataloader)  # 평균 손실 계산\n",
        "    print(f\"Epoch : {epoch+1:4d}, Cost : {cost:.3f}\")  # 손실 출력"
      ],
      "metadata": {
        "id": "r5BLcUuh254y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. 결과 시각화"
      ],
      "metadata": {
        "id": "nQT9wHAg27oP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "\n",
        "\n",
        "# 바운딩 박스를 이미지에 그리는 함수 정의\n",
        "def draw_bbox(ax, box, text, color):\n",
        "    ax.add_patch(\n",
        "        plt.Rectangle(\n",
        "            xy=(box[0], box[1]),\n",
        "            width=box[2] - box[0],\n",
        "            height=box[3] - box[1],\n",
        "            fill=False,\n",
        "            edgecolor=color,\n",
        "            linewidth=2,\n",
        "        )\n",
        "    )\n",
        "    ax.annotate(\n",
        "        text=text,\n",
        "        xy=(box[0] - 5, box[1] - 5),\n",
        "        color=color,\n",
        "        weight=\"bold\",\n",
        "        fontsize=13,\n",
        "    )\n",
        "\n",
        "# 모델 평가 및 결과 시각화\n",
        "threshold = 0.5  # 예측 임계값 설정\n",
        "categories = test_dataset.categories\n",
        "with torch.no_grad():\n",
        "    model.eval()  # 평가 모드 전환\n",
        "    for images, targets in test_dataloader:\n",
        "        images = [image.to(device) for image in images]  # 이미지 장치로 이동\n",
        "        outputs = model(images)  # 모델 예측 수행\n",
        "\n",
        "        boxes = outputs[0][\"boxes\"].to(\"cpu\").numpy()  # 예측된 박스\n",
        "        labels = outputs[0][\"labels\"].to(\"cpu\").numpy()  # 예측된 라벨\n",
        "        scores = outputs[0][\"scores\"].to(\"cpu\").numpy()  # 예측된 점수\n",
        "\n",
        "        boxes = boxes[scores >= threshold].astype(np.int32)  # 임계값 기준 필터링\n",
        "        labels = labels[scores >= threshold]\n",
        "        scores = scores[scores >= threshold]\n",
        "\n",
        "        fig = plt.figure(figsize=(8, 8))\n",
        "        ax = fig.add_subplot(1, 1, 1)\n",
        "        plt.imshow(to_pil_image(images[0]))  # 이미지를 표시\n",
        "\n",
        "        for box, label, score in zip(boxes, labels, scores):\n",
        "            draw_bbox(ax, box, f\"{categories[label]} - {score:.4f}\", \"red\")  # 예측 박스 표시\n",
        "\n",
        "        tboxes = targets[0][\"boxes\"].numpy()  # 실제 박스\n",
        "        tlabels = targets[0][\"labels\"].numpy()  # 실제 라벨\n",
        "        for box, label in zip(tboxes, tlabels):\n",
        "            draw_bbox(ax, box, f\"{categories[label]}\", \"blue\")  # 실제 박스 표시\n",
        "\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "KsHwV4A9283Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 예제 9.10~9.12 SSD 모델 실습."
      ],
      "metadata": {
        "id": "m3HcCExe3DSA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. SSD Backbone 정의\n"
      ],
      "metadata": {
        "id": "oNfy5mGO3L_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from collections import OrderedDict\n",
        "\n",
        "class SSDBackbone(nn.Module):\n",
        "    def __init__(self, backbone):\n",
        "        super().__init__()\n",
        "        # ResNet의 각 레이어를 SSD에 적합하게 분리함\n",
        "        layer0 = nn.Sequential(backbone.conv1, backbone.bn1, backbone.relu)\n",
        "        layer1 = backbone.layer1\n",
        "        layer2 = backbone.layer2\n",
        "        layer3 = backbone.layer3\n",
        "        layer4 = backbone.layer4\n",
        "\n",
        "        self.features = nn.Sequential(layer0, layer1, layer2, layer3)  # 기본 특징 추출 레이어\n",
        "        self.upsampling = nn.Sequential(  # 업샘플링 레이어 정의\n",
        "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        # Extra layers를 정의하여 추가적인 특징을 추출함\n",
        "        self.extra = nn.ModuleList(\n",
        "            [\n",
        "                nn.Sequential(\n",
        "                    layer4,  # 기본 특징 레이어에서 시작\n",
        "                    nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=1),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                ),\n",
        "                nn.Sequential(\n",
        "                    nn.Conv2d(1024, 256, kernel_size=1),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                    nn.Conv2d(256, 512, kernel_size=3, padding=1, stride=2),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                ),\n",
        "                nn.Sequential(\n",
        "                    nn.Conv2d(512, 128, kernel_size=1),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                    nn.Conv2d(128, 256, kernel_size=3, padding=1, stride=2),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                ),\n",
        "                nn.Sequential(\n",
        "                    nn.Conv2d(256, 128, kernel_size=1),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                    nn.Conv2d(128, 256, kernel_size=3),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                ),\n",
        "                nn.Sequential(\n",
        "                    nn.Conv2d(256, 128, kernel_size=1),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                    nn.Conv2d(128, 256, kernel_size=3),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                ),\n",
        "                nn.Sequential(\n",
        "                    nn.Conv2d(256, 128, kernel_size=1),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                    nn.Conv2d(128, 256, kernel_size=4),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                )\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)  # 기본 특징 추출\n",
        "        output = [self.upsampling(x)]  # 업샘플링 레이어 출력 저장\n",
        "\n",
        "        for block in self.extra:\n",
        "            x = block(x)  # Extra layers 통과\n",
        "            output.append(x)  # 각 결과를 저장\n",
        "\n",
        "        return OrderedDict([(str(i), v) for i, v in enumerate(output)])  # OrderedDict로 반환"
      ],
      "metadata": {
        "id": "dxt_ivVZ3FLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. SDD 모델 구성"
      ],
      "metadata": {
        "id": "h-r7QyhN3Oxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision.models import resnet34\n",
        "from torchvision.models.detection import ssd\n",
        "from torchvision.models.detection.anchor_utils import DefaultBoxGenerator\n",
        "\n",
        "# ResNet34를 기반으로 SSD Backbone 구성\n",
        "backbone_base = resnet34(weights=\"ResNet34_Weights.IMAGENET1K_V1\")\n",
        "backbone = SSDBackbone(backbone_base)\n",
        "\n",
        "# 앵커 박스 생성기 정의\n",
        "anchor_generator = DefaultBoxGenerator(\n",
        "    aspect_ratios=[[2], [2, 3], [2, 3], [2, 3], [2, 3], [2], [2]],  # 각 레이어에 대해 다른 비율 설정\n",
        "    scales=[0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05, 1.20],  # 스케일 정의\n",
        "    steps=[8, 16, 32, 64, 100, 300, 512],  # 각 레이어의 격자 크기 설정\n",
        ")\n",
        "\n",
        "# SSD 모델 정의\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = ssd.SSD(\n",
        "    backbone=backbone,\n",
        "    anchor_generator=anchor_generator,\n",
        "    size=(512, 512),  # 입력 이미지 크기 설정\n",
        "    num_classes=3  # 클래스 수 설정 (배경 포함)\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "kTXi8zQ73RzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. COCO 데이터셋 클래스 정의"
      ],
      "metadata": {
        "id": "E7OLSuyN3Ts4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from PIL import Image\n",
        "from pycocotools.coco import COCO\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class COCODataset(Dataset):\n",
        "    def __init__(self, root, train, transform=None):\n",
        "        super().__init__()\n",
        "        directory = \"train\" if train else \"val\"  # 훈련 또는 검증 경로 설정\n",
        "        annotations = os.path.join(root, \"annotations\", f\"{directory}_annotations.json\")\n",
        "\n",
        "        self.coco = COCO(annotations)  # COCO 어노테이션 로드\n",
        "        self.iamge_path = os.path.join(root, directory)  # 이미지 경로 설정\n",
        "        self.transform = transform  # 변환 설정\n",
        "\n",
        "        self.categories = self._get_categories()  # 카테고리 로드\n",
        "        self.data = self._load_data()  # 데이터 로드\n",
        "\n",
        "    def _get_categories(self):\n",
        "        categories = {0: \"background\"}  # 배경 추가\n",
        "        for category in self.coco.cats.values():\n",
        "            categories[category[\"id\"]] = category[\"name\"]  # 카테고리 매핑\n",
        "        return categories\n",
        "\n",
        "    def _load_data(self):\n",
        "        data = []\n",
        "        for _id in self.coco.imgs:\n",
        "            file_name = self.coco.loadImgs(_id)[0][\"file_name\"]  # 이미지 파일 이름 가져오기\n",
        "            image_path = os.path.join(self.iamge_path, file_name)  # 경로 생성\n",
        "            image = Image.open(image_path).convert(\"RGB\")  # RGB 이미지로 변환\n",
        "\n",
        "            boxes = []\n",
        "            labels = []\n",
        "            anns = self.coco.loadAnns(self.coco.getAnnIds(_id))  # 어노테이션 로드\n",
        "            for ann in anns:\n",
        "                x, y, w, h = ann[\"bbox\"]  # 바운딩 박스 추출\n",
        "\n",
        "                boxes.append([x, y, x + w, y + h])  # 좌표 저장\n",
        "                labels.append(ann[\"category_id\"])  # 라벨 저장\n",
        "\n",
        "            target = {  # 타겟 구성\n",
        "                \"image_id\": torch.LongTensor([_id]),\n",
        "                \"boxes\": torch.FloatTensor(boxes),\n",
        "                \"labels\": torch.LongTensor(labels)\n",
        "            }\n",
        "            data.append([image, target])  # 데이터 추가\n",
        "        return data\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image, target = self.data[index]  # 이미지와 타겟 반환\n",
        "        if self.transform:\n",
        "            image = self.transform(image)  # 변환 적용\n",
        "        return image, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)  # 데이터 길이 반환"
      ],
      "metadata": {
        "id": "bhh1oLVt3VOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. 데이터로더 구성"
      ],
      "metadata": {
        "id": "E-DtqvLD3WyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def collator(batch):\n",
        "    return tuple(zip(*batch))  # 배치 데이터를 병합\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.PILToTensor(),  # 이미지를 텐서로 변환\n",
        "        transforms.ConvertImageDtype(dtype=torch.float)  # 데이터 타입 변경\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 훈련 및 검증 데이터셋 정의\n",
        "train_dataset = COCODataset(\"../datasets/coco\", train=True, transform=transform)\n",
        "test_dataset = COCODataset(\"../datasets/coco\", train=False, transform=transform)\n",
        "\n",
        "# 데이터로더 정의\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset, batch_size=4, shuffle=True, drop_last=True, collate_fn=collator\n",
        ")\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset, batch_size=1, shuffle=True, drop_last=True, collate_fn=collator\n",
        ")"
      ],
      "metadata": {
        "id": "nJh4v06T3X9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. 모델 학습"
      ],
      "metadata": {
        "id": "mxK2SNYO3Zyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "\n",
        "# 학습 가능한 파라미터 설정\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)  # 옵티마이저 정의\n",
        "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)  # 학습률 스케줄러 정의\n",
        "\n",
        "# 학습 루프\n",
        "for epoch in range(10):\n",
        "    cost = 0.0\n",
        "    for idx, (images, targets) in enumerate(train_dataloader):\n",
        "        images = list(image.to(device) for image in images)  # 이미지를 장치로 이동\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]  # 타겟도 장치로 이동\n",
        "\n",
        "        loss_dict = model(images, targets)  # 손실 계산\n",
        "        losses = sum(loss for loss in loss_dict.values())  # 모든 손실 합산\n",
        "\n",
        "        optimizer.zero_grad()  # 그래디언트 초기화\n",
        "        losses.backward()  # 역전파 수행\n",
        "        optimizer.step()  # 파라미터 업데이트\n",
        "\n",
        "        cost += losses  # 손실 누적\n",
        "\n",
        "    lr_scheduler.step()  # 학습률 업데이트\n",
        "    cost = cost / len(train_dataloader)  # 평균 손실 계산\n",
        "    print(f\"Epoch : {epoch+1:4d}, Cost : {cost:.3f}\")  # 학습 상태 출력"
      ],
      "metadata": {
        "id": "19Jo8bQk3b8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. 결과 시각화 및 평ㅇ가"
      ],
      "metadata": {
        "id": "bRZRrUIk3gI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "\n",
        "# 바운딩 박스를 그리는 함수 정의\n",
        "def draw_bbox(ax, box, text, color):\n",
        "    ax.add_patch(\n",
        "        plt.Rectangle(\n",
        "            xy=(box[0], box[1]),\n",
        "            width=box[2] - box[0],\n",
        "            height=box[3] - box[1],\n",
        "            fill=False,\n",
        "            edgecolor=color,\n",
        "            linewidth=2,\n",
        "        )\n",
        "    )\n",
        "    ax.annotate(\n",
        "        text=text,\n",
        "        xy=(box[0] - 5, box[1] - 5),\n",
        "        color=color,\n",
        "        weight=\"bold\",\n",
        "        fontsize=13,\n",
        "    )\n",
        "\n",
        "# 예측 결과를 시각화\n",
        "threshold = 0.5  # 점수 임계값\n",
        "categories = test_dataset.categories\n",
        "with torch.no_grad():\n",
        "    model.eval()  # 모델을 평가 모드로 설정\n",
        "    for images, targets in test_dataloader:\n",
        "        images = [image.to(device) for image in images]  # 이미지를 장치로 이동\n",
        "        outputs = model(images)  # 모델 예측 수행\n",
        "\n",
        "        boxes = outputs[0][\"boxes\"].to(\"cpu\").numpy()  # 예측된 바운딩 박스\n",
        "        labels = outputs[0][\"labels\"].to(\"cpu\").numpy()  # 예측된 라벨\n",
        "        scores = outputs[0][\"scores\"].to(\"cpu\").numpy()  # 예측된 점수\n",
        "\n",
        "        boxes = boxes[scores >= threshold].astype(np.int32)  # 점수 기준 필터링\n",
        "        labels = labels[scores >= threshold]\n",
        "        scores = scores[scores >= threshold]\n",
        "\n",
        "        fig = plt.figure(figsize=(8, 8))\n",
        "        ax = fig.add_subplot(1, 1, 1)\n",
        "        plt.imshow(to_pil_image(images[0]))  # 이미지를 출력\n",
        "\n",
        "        for box, label, score in zip(boxes, labels, scores):\n",
        "            draw_bbox(ax, box, f\"{categories[label]} - {score:.4f}\", \"red\")  # 예측 박스 출력\n",
        "\n",
        "        tboxes = targets[0][\"boxes\"].numpy()  # 실제 바운딩 박스\n",
        "        tlabels = targets[0][\"labels\"].numpy()  # 실제 라벨\n",
        "        for box, label in zip(tboxes, tlabels):\n",
        "            draw_bbox(ax, box, f\"{categories[label]}\", \"blue\")  # 실제 박스 출력\n",
        "\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "zPP9Si9u3h5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 예제 9.13~9.19 FCN 모델 실습"
      ],
      "metadata": {
        "id": "z9-hLpR_35Y0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================ 데이터셋 정의 =================================\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# SegmentationDataset 클래스 정의\n",
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, root, train, transform=None, target_transform=None):\n",
        "        super().__init__()\n",
        "        self.root = os.path.join(root, \"VOCdevkit\", \"VOC2012\")  # VOC 데이터 경로 설정\n",
        "        file_type = \"train\" if train else \"val\"  # 훈련 데이터인지 검증 데이터인지 선택\n",
        "        file_path = os.path.join(\n",
        "            self.root, \"ImageSets\", \"Segmentation\", f\"{file_type}.txt\"\n",
        "        )\n",
        "        with open(os.path.join(self.root, \"classes.json\"), \"r\") as file:\n",
        "            self.categories = json.load(file)  # 클래스 정보 로드\n",
        "        self.files = open(file_path).read().splitlines()  # 데이터 파일 이름 로드\n",
        "        self.transform = transform  # 이미지 변환 설정\n",
        "        self.target_transform = target_transform  # 타겟 변환 설정\n",
        "        self.data = self._load_data()  # 데이터 로드\n",
        "\n",
        "    # 데이터 로드 함수\n",
        "    def _load_data(self):\n",
        "        data = []\n",
        "        for file in self.files:\n",
        "            image_path = os.path.join(self.root, \"JPEGImages\", f\"{file}.jpg\")\n",
        "            mask_path = os.path.join(self.root, \"SegmentationClass\", f\"{file}.png\")\n",
        "            image = Image.open(image_path).convert(\"RGB\")  # 이미지 로드\n",
        "            mask = np.array(Image.open(mask_path))  # 마스크 로드\n",
        "            mask = np.where(mask == 255, 0, mask)  # 배경 클래스 조정\n",
        "            target = torch.LongTensor(mask).unsqueeze(0)  # 마스크 텐서로 변환\n",
        "            data.append([image, target])  # 데이터 추가\n",
        "        return data\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image, mask = self.data[index]\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)  # 이미지 변환\n",
        "        if self.target_transform is not None:\n",
        "            mask = self.target_transform(mask)  # 타겟 변환\n",
        "        return image, mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)  # 데이터셋 크기 반환\n",
        "\n",
        "# ============================= 데이터셋 로더 설정 ================================\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 데이터 변환 정의\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.PILToTensor(),  # 이미지를 텐서로 변환\n",
        "        transforms.ConvertImageDtype(dtype=torch.float),  # 데이터 타입 변경\n",
        "        transforms.Resize(size=(224, 224)),  # 이미지 크기 조정\n",
        "    ]\n",
        ")\n",
        "target_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(\n",
        "            size=(224, 224),\n",
        "            interpolation=transforms.InterpolationMode.NEAREST,  # 최근접 이웃 보간법\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 데이터셋 및 데이터로더 생성\n",
        "train_dataset = SegmentationDataset(\n",
        "    \"../datasets\", train=True, transform=transform, target_transform=target_transform\n",
        ")\n",
        "test_dataset = SegmentationDataset(\n",
        "    \"../datasets\", train=False, transform=transform, target_transform=target_transform\n",
        ")\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset, batch_size=8, shuffle=True, drop_last=True\n",
        ")\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset, batch_size=4, shuffle=True, drop_last=True\n",
        ")\n",
        "\n",
        "# ============================ 시각화 함수 정의 =================================\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 마스크를 시각화하는 함수 정의\n",
        "def draw_mask(images, masks, outputs=None, plot_size=4):\n",
        "    def color_mask(image, target):\n",
        "        m = target.squeeze().numpy().astype(np.uint8)\n",
        "        cm = np.zeros_like(image, dtype=np.uint8)\n",
        "\n",
        "        for i in range(1, 21):  # 각 클래스 색상 적용\n",
        "            cm[m == i] = train_dataset.categories[str(i)][\"color\"]\n",
        "\n",
        "        classes = [train_dataset.categories[str(idx)][\"class\"] for idx in np.unique(m)]\n",
        "        return cm, classes\n",
        "\n",
        "    col = 3 if outputs is not None else 2\n",
        "    figsize = 20 if outputs is not None else 28\n",
        "    fig, ax = plt.subplots(plot_size, col, figsize=(14, figsize), constrained_layout=True)\n",
        "\n",
        "    for batch in range(plot_size):\n",
        "        im = images[batch].numpy().transpose(1, 2, 0)  # 이미지 차원 변환\n",
        "        ax[batch][0].imshow(im)  # 원본 이미지 표시\n",
        "        ax[batch][0].axis(\"off\")\n",
        "\n",
        "        cm, classes = color_mask(im, masks[batch])  # 마스크 생성\n",
        "        ax[batch][1].set_title(classes)\n",
        "        ax[batch][1].imshow(cm)\n",
        "        ax[batch][1].axis(\"off\")\n",
        "\n",
        "        if outputs is not None:  # 모델 출력이 있을 경우\n",
        "            cm, classes = color_mask(im, outputs[batch])\n",
        "            ax[batch][2].set_title(classes)\n",
        "            ax[batch][2].imshow(cm)\n",
        "            ax[batch][2].axis(\"off\")\n",
        "\n",
        "# 훈련 데이터의 이미지와 마스크 시각화\n",
        "images, masks = next(iter(train_dataloader))\n",
        "draw_mask(images, masks, plot_size=4)\n",
        "\n",
        "# ============================ 모델 학습 및 검증 =================================\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torchvision.models import segmentation\n",
        "\n",
        "# FCN 모델 정의\n",
        "num_classes = 21  # 클래스 수\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = segmentation.fcn_resnet50(\n",
        "    weight=\"FCN_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1\",  # 사전 학습 가중치\n",
        "    num_classes=21,\n",
        ").to(device)\n",
        "\n",
        "# 손실 함수와 옵티마이저 정의\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 모델 학습 루프\n",
        "for epoch in range(30):\n",
        "    model.train()\n",
        "    cost = 0.0\n",
        "\n",
        "    for images, targets in train_dataloader:\n",
        "        images = images.to(device)  # 이미지 장치로 이동\n",
        "        targets = targets.to(device)  # 타겟 장치로 이동\n",
        "\n",
        "        output = model(images)  # 모델 예측\n",
        "        output = output[\"out\"].permute(0, 2, 3, 1).contiguous().view(-1, num_classes)\n",
        "        targets = targets.permute(0, 2, 3, 1).contiguous().view(-1)\n",
        "\n",
        "        loss = criterion(output, targets)  # 손실 계산\n",
        "\n",
        "        optimizer.zero_grad()  # 그래디언트 초기화\n",
        "        loss.backward()  # 역전파\n",
        "        optimizer.step()  # 가중치 업데이트\n",
        "\n",
        "        cost += loss\n",
        "    cost = cost / len(train_dataloader)  # 평균 손실\n",
        "    print(f\"Epoch : {epoch+1:4d}, Cost : {cost:.3f}\")\n",
        "\n",
        "# ============================ 모델 평가 및 시각화 ================================\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    images, masks = next(iter(test_dataloader))\n",
        "    outputs = model(images.to(device))[\"out\"]  # 모델 출력\n",
        "    outputs = outputs.argmax(axis=1).to(\"cpu\")  # 클래스 예측\n",
        "    draw_mask(images, masks, outputs, 4)  # 시각화\n",
        "\n",
        "# ============================ mIoU 계산 =======================================\n",
        "from collections import defaultdict\n",
        "\n",
        "# IoU 계산 함수 정의\n",
        "def calculate_iou(targets, outputs, ious, class_count, num_classes=21):\n",
        "    for i in range(num_classes):\n",
        "        intersection = np.float32(np.sum((outputs == targets) * (targets == i)))  # 교집합\n",
        "        union = np.sum(targets == i) + np.sum(outputs == i) - intersection  # 합집합\n",
        "        if union > 0:\n",
        "            ious[i] += intersection / union  # IoU 계산\n",
        "            class_count[i] += 1\n",
        "    return ious, class_count\n",
        "\n",
        "# mIoU 계산\n",
        "ious = np.zeros(21)\n",
        "class_count = defaultdict(int)\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    for images, targets in test_dataloader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)[\"out\"].permute(0, 2, 3, 1).detach().to(\"cpu\").numpy()\n",
        "        targets = targets.permute(0, 2, 3, 1).squeeze().detach().to(\"cpu\").numpy()\n",
        "        outputs = outputs.argmax(-1)\n",
        "\n",
        "        ious, class_count = calculate_iou(targets, outputs, ious, class_count, 21)\n",
        "\n",
        "# 평균 IoU 계산 및 출력\n",
        "miou = 0.0\n",
        "for idx in range(1, 21):  # 배경 제외 클래스\n",
        "    miou += ious[idx] / class_count[idx]\n",
        "miou /= 20\n",
        "print(f\"mIoU 계산 결과 : {miou}\")"
      ],
      "metadata": {
        "id": "BzFqmcEI374U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 예제 9.20~9.24 Mask R-CNN 모델 실습"
      ],
      "metadata": {
        "id": "PFytQ7fJ39S2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================== COCO 데이터셋 정의 ===================================\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from pycocotools.coco import COCO\n",
        "from torch.utils.data import Dataset\n",
        "from pycocotools import mask as maskUtils\n",
        "\n",
        "\n",
        "class COCODataset(Dataset):\n",
        "    def __init__(self, root, train, transform=None):\n",
        "        super().__init__()\n",
        "        directory = \"train\" if train else \"val\"  # 훈련 또는 검증 데이터 경로 선택\n",
        "        annotations = os.path.join(root, \"annotations\", f\"{directory}_annotations.json\")  # 어노테이션 파일 경로\n",
        "\n",
        "        self.coco = COCO(annotations)  # COCO 어노테이션 로드\n",
        "        self.iamge_path = os.path.join(root, directory)  # 이미지 경로 설정\n",
        "        self.transform = transform  # 이미지 변환 설정\n",
        "\n",
        "        self.categories = self._get_categories()  # 카테고리 로드\n",
        "        self.data = self._load_data()  # 데이터 로드\n",
        "\n",
        "    def _get_categories(self):\n",
        "        # 카테고리 정보를 로드하여 딕셔너리로 반환\n",
        "        categories = {0: \"background\"}\n",
        "        for category in self.coco.cats.values():\n",
        "            categories[category[\"id\"]] = category[\"name\"]\n",
        "        return categories\n",
        "\n",
        "    def _load_data(self):\n",
        "        # 이미지와 관련된 바운딩 박스, 라벨, 마스크 데이터를 로드\n",
        "        data = []\n",
        "        for _id in self.coco.imgs:\n",
        "            file_name = self.coco.loadImgs(_id)[0][\"file_name\"]\n",
        "            image_path = os.path.join(self.iamge_path, file_name)\n",
        "            image = Image.open(image_path).convert(\"RGB\")\n",
        "            width, height = image.size  # 이미지 크기\n",
        "\n",
        "            boxes = []\n",
        "            labels = []\n",
        "            masks = []\n",
        "            anns = self.coco.loadAnns(self.coco.getAnnIds(_id))  # 어노테이션 로드\n",
        "            for ann in anns:\n",
        "                x, y, w, h = ann[\"bbox\"]  # 바운딩 박스\n",
        "                segmentations = ann[\"segmentation\"]  # 분할 정보\n",
        "                try:\n",
        "                    mask = self._polygon_to_mask(segmentations, width, height)  # 마스크 생성\n",
        "                except Exception as e:\n",
        "                    pass\n",
        "\n",
        "                boxes.append([x, y, x + w, y + h])  # 바운딩 박스 좌표\n",
        "                labels.append(ann[\"category_id\"])  # 라벨\n",
        "                masks.append(mask)  # 마스크 추가\n",
        "\n",
        "            target = {  # 타겟 데이터 생성\n",
        "                \"image_id\": torch.LongTensor([_id]),\n",
        "                \"boxes\": torch.FloatTensor(boxes),\n",
        "                \"labels\": torch.LongTensor(labels),\n",
        "                \"masks\": torch.FloatTensor(masks)\n",
        "            }\n",
        "            data.append([image, target])  # 데이터 추가\n",
        "        return data\n",
        "\n",
        "    def _polygon_to_mask(self, segmentations, width, height):\n",
        "        # 폴리곤을 마스크로 변환\n",
        "        binary_mask = []\n",
        "        for seg in segmentations:\n",
        "            rles = maskUtils.frPyObjects([seg], height, width)\n",
        "            binary_mask.append(maskUtils.decode(rles))\n",
        "\n",
        "        combined_mask = np.sum(binary_mask, axis=0).squeeze()  # 여러 마스크 결합\n",
        "        return combined_mask\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # 인덱스에 해당하는 데이터 반환\n",
        "        image, target = self.data[index]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)  # 변환 적용\n",
        "        return image, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)  # 데이터셋 크기 반환\n",
        "\n",
        "\n",
        "# ============================= 데이터셋 및 데이터로더 설정 ================================\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def collator(batch):\n",
        "    return tuple(zip(*batch))  # 배치 데이터를 병합\n",
        "\n",
        "# 이미지 변환 설정\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.PILToTensor(),\n",
        "        transforms.ConvertImageDtype(dtype=torch.float)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 데이터셋 정의\n",
        "train_dataset = COCODataset(\"../datasets/coco\", train=True, transform=transform)\n",
        "test_dataset = COCODataset(\"../datasets/coco\", train=False, transform=transform)\n",
        "\n",
        "# 데이터로더 생성\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset, batch_size=4, shuffle=True, drop_last=True, collate_fn=collator\n",
        ")\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset, batch_size=1, shuffle=True, drop_last=True, collate_fn=collator\n",
        ")\n",
        "\n",
        "\n",
        "# =================================== Mask R-CNN 모델 정의 ==================================\n",
        "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "\n",
        "num_classes = 3  # 클래스 수 (배경 포함)\n",
        "hidden_layer = 256  # 마스크 예측 레이어 크기\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Mask R-CNN 모델 불러오기\n",
        "model = maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
        "\n",
        "# 바운딩 박스 예측기 수정\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(\n",
        "    in_channels=model.roi_heads.box_predictor.cls_score.in_features,\n",
        "    num_classes=num_classes\n",
        ")\n",
        "\n",
        "# 마스크 예측기 수정\n",
        "model.roi_heads.mask_predictor = MaskRCNNPredictor(\n",
        "    in_channels=model.roi_heads.mask_predictor.conv5_mask.in_channels,\n",
        "    dim_reduced=hidden_layer,\n",
        "    num_classes=num_classes\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "# ================================= 모델 학습 설정 ====================================\n",
        "from torch import optim\n",
        "\n",
        "# 옵티마이저 및 학습률 스케줄러 설정\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "# 모델 학습 루프\n",
        "for epoch in range(5):\n",
        "    cost = 0.0\n",
        "    for idx, (images, targets) in enumerate(train_dataloader):\n",
        "        images = list(image.to(device) for image in images)  # 이미지 장치로 이동\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]  # 타겟 이동\n",
        "\n",
        "        loss_dict = model(images, targets)  # 손실 계산\n",
        "        losses = sum(loss for loss in loss_dict.values())  # 총 손실 계산\n",
        "\n",
        "        optimizer.zero_grad()  # 그래디언트 초기화\n",
        "        losses.backward()  # 역전파\n",
        "        optimizer.step()  # 가중치 업데이트\n",
        "\n",
        "        cost += losses  # 손실 누적\n",
        "\n",
        "    lr_scheduler.step()  # 학습률 감소\n",
        "    cost = cost / len(train_dataloader)  # 평균 손실\n",
        "    print(f\"Epoch : {epoch+1:4d}, Cost : {cost:.3f}\")\n",
        "\n",
        "\n",
        "# ========================== 시각화 함수 및 모델 예측 시각화 ============================\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "\n",
        "# 바운딩 박스 및 마스크를 시각화하는 함수\n",
        "def draw_bbox(ax, box, text, color, mask):\n",
        "    ax.add_patch(\n",
        "        plt.Rectangle(\n",
        "            xy=(box[0], box[1]),\n",
        "            width=box[2] - box[0],\n",
        "            height=box[3] - box[1],\n",
        "            fill=False,\n",
        "            edgecolor=color,\n",
        "            linewidth=2,\n",
        "        )\n",
        "    )\n",
        "    ax.annotate(\n",
        "        text=text,\n",
        "        xy=(box[0] - 5, box[1] - 5),\n",
        "        color=color,\n",
        "        weight=\"bold\",\n",
        "        fontsize=13,\n",
        "    )\n",
        "\n",
        "    mask = np.ma.masked_where(mask == 0, mask)\n",
        "    mask_color = {\"blue\": \"Blues\", \"red\" : \"Reds\"}\n",
        "\n",
        "    cmap = plt.cm.get_cmap(mask_color.get(color, \"Greens\"))\n",
        "    norm = plt.Normalize(vmin=0, vmax=1)\n",
        "    rgba = cmap(norm(mask))\n",
        "    ax.imshow(rgba, interpolation=\"nearest\", alpha=0.3)\n",
        "\n",
        "\n",
        "# 모델 예측 시각화\n",
        "threshold = 0.5  # 임계값\n",
        "categories = test_dataset.categories\n",
        "\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    for images, targets in test_dataloader:\n",
        "        images = [image.to(device) for image in images]  # 이미지 장치로 이동\n",
        "        outputs = model(images)  # 모델 예측\n",
        "\n",
        "        # 예측 결과 추출\n",
        "        boxes = outputs[0][\"boxes\"].to(\"cpu\").numpy()\n",
        "        labels = outputs[0][\"labels\"].to(\"cpu\").numpy()\n",
        "        scores = outputs[0][\"scores\"].to(\"cpu\").numpy()\n",
        "        masks = outputs[0][\"masks\"].squeeze(1).to(\"cpu\").numpy()\n",
        "\n",
        "        # 임계값 기준 필터링\n",
        "        boxes = boxes[scores >= threshold].astype(np.int32)\n",
        "        labels = labels[scores >= threshold]\n",
        "        scores = scores[scores >= threshold]\n",
        "        masks[masks >= threshold] = 1.0\n",
        "        masks[masks < threshold] = 0.0\n",
        "\n",
        "        fig = plt.figure(figsize=(8, 8))\n",
        "        ax = fig.add_subplot(1, 1, 1)\n",
        "        plt.imshow(to_pil_image(images[0]))  # 원본 이미지 표시\n",
        "\n",
        "        # 예측 결과 시각화\n",
        "        for box, mask, label, score in zip(boxes, masks, labels, scores):\n",
        "            draw_bbox(ax, box, f\"{categories[label]} - {score:.4f}\", \"red\", mask)\n",
        "\n",
        "        # 실제 결과 시각화\n",
        "        tboxes = targets[0][\"boxes\"].numpy()\n",
        "        tmask = targets[0][\"masks\"].numpy()\n",
        "        tlabels = targets[0][\"labels\"].numpy()\n",
        "\n",
        "        for box, mask, label in zip(tboxes, tmask, tlabels):\n",
        "            draw_bbox(ax, box, f\"{categories[label]}\", \"blue\", mask)\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "# ============================= COCO 평가 지표 계산 ================================\n",
        "import numpy as np\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "\n",
        "# COCO 평가\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    coco_detections = []\n",
        "    for images, targets in test_dataloader:\n",
        "        images = [img.to(device) for img in images]\n",
        "        outputs = model(images)  # 모델 예측\n",
        "\n",
        "        for i in range(len(targets)):\n",
        "            image_id = targets[i][\"image_id\"].data.cpu().numpy().tolist()[0]\n",
        "            boxes = outputs[i][\"boxes\"].data.cpu().numpy()\n",
        "            boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n",
        "            boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n",
        "            scores = outputs[i][\"scores\"].data.cpu().numpy()\n",
        "            labels = outputs[i][\"labels\"].data.cpu().numpy()\n",
        "            masks = outputs[i][\"masks\"].squeeze(1).data.cpu().numpy()\n",
        "\n",
        "            for instance_id in range(len(boxes)):\n",
        "                segmentation_mask = masks[instance_id]\n",
        "                binary_mask = segmentation_mask > 0.5\n",
        "                binary_mask = binary_mask.astype(np.uint8)\n",
        "                binary_mask_encoded = maskUtils.encode(\n",
        "                    np.asfortranarray(binary_mask)\n",
        "                )\n",
        "\n",
        "                prediction = {\n",
        "                    \"image_id\": int(image_id),\n",
        "                    \"category_id\": int(labels[instance_id]),\n",
        "                    \"bbox\": [round(coord, 2) for coord in boxes[instance_id]],\n",
        "                    \"score\": float(scores[instance_id]),\n",
        "                    \"segmentation\": binary_mask_encoded\n",
        "                }\n",
        "                coco_detections.append(prediction)\n",
        "\n",
        "    # COCO 평가 실행\n",
        "    coco_gt = test_dataloader.dataset.coco\n",
        "    coco_dt = coco_gt.loadRes(coco_detections)\n",
        "    coco_evaluator = COCOeval(coco_gt, coco_dt, iouType=\"segm\")\n",
        "    coco_evaluator.evaluate()\n",
        "    coco_evaluator.accumulate()\n",
        "    coco_evaluator.summarize()"
      ],
      "metadata": {
        "id": "0HXHZO_E4AUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 예제 9.25~9.30 YOLOv8 모델 실습"
      ],
      "metadata": {
        "id": "md4aD6JW4-Pt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================= 영상 출력 및 YOLOv8 모델 설정 ==============================\n",
        "from google.colab.patches import cv2_imshow\n",
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import torch\n",
        "\n",
        "# YOLO 모델 로드\n",
        "model = YOLO(\"../models/yolov8m-pose.pt\")\n",
        "\n",
        "# ============================= 동영상 예제 기본 출력 =====================================\n",
        "# 동영상 파일 로드\n",
        "capture = cv2.VideoCapture(\"../datasets/woman.mp4\")\n",
        "\n",
        "while cv2.waitKey(10) < 0:\n",
        "    if capture.get(cv2.CAP_PROP_POS_FRAMES) == capture.get(cv2.CAP_PROP_FRAME_COUNT):  # 마지막 프레임일 경우\n",
        "        capture.set(cv2.CAP_PROP_POS_FRAMES, 0)  # 프레임을 처음으로 리셋\n",
        "\n",
        "    ret, frame = capture.read()  # 프레임 읽기\n",
        "    cv2.imshow(\"VideoFrame\", frame)  # 프레임 출력\n",
        "\n",
        "capture.release()  # 리소스 해제\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "# ============================ YOLO 예측 함수 정의 =====================================\n",
        "def predict(frame, iou=0.7, conf=0.25):\n",
        "    # YOLO를 사용하여 예측 수행\n",
        "    results = model(\n",
        "        source=frame,\n",
        "        device=\"0\" if torch.cuda.is_available() else \"cpu\",  # GPU/CPU 설정\n",
        "        iou=iou,  # IoU 임계값\n",
        "        conf=conf,  # Confidence 임계값\n",
        "        verbose=False,  # 출력 생략\n",
        "    )\n",
        "    result = results[0]  # 첫 번째 결과 가져오기\n",
        "    return result\n",
        "\n",
        "# ============================= 바운딩 박스 그리기 함수 정의 ==============================\n",
        "def draw_boxes(result, frame):\n",
        "    for boxes in result.boxes:  # 예측된 바운딩 박스 순회\n",
        "        x1, y1, x2, y2, score, classes = boxes.data.squeeze().cpu().numpy()  # 좌표와 클래스 정보 추출\n",
        "        cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 0, 255), 1)  # 바운딩 박스 그림\n",
        "    return frame\n",
        "\n",
        "# ============================ 바운딩 박스 시각화 =====================================\n",
        "capture = cv2.VideoCapture(\"../datasets/woman.mp4\")\n",
        "\n",
        "while cv2.waitKey(10) < 0:\n",
        "    if capture.get(cv2.CAP_PROP_POS_FRAMES) == capture.get(cv2.CAP_PROP_FRAME_COUNT):  # 마지막 프레임일 경우\n",
        "        capture.set(cv2.CAP_PROP_POS_FRAMES, 0)  # 프레임 리셋\n",
        "\n",
        "    ret, frame = capture.read()  # 프레임 읽기\n",
        "    result = predict(frame)  # YOLO 예측\n",
        "    frame = draw_boxes(result, frame)  # 바운딩 박스 그리기\n",
        "    cv2.imshow(\"VideoFrame\", frame)  # 결과 출력\n",
        "\n",
        "capture.release()  # 리소스 해제\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "# ========================== 키포인트 그리기 함수 정의 ============================\n",
        "from ultralytics.yolo.utils.plotting import Annotator\n",
        "\n",
        "def draw_keypoints(result, frame):\n",
        "    annotator = Annotator(frame, line_width=1)  # 이미지 주석 생성기\n",
        "    for kps in result.keypoints:  # 예측된 키포인트 순회\n",
        "        kps = kps.data.squeeze()  # 키포인트 데이터 추출\n",
        "        annotator.kpts(kps)  # 키포인트 시각화\n",
        "\n",
        "        nkps = kps.cpu().numpy()  # NumPy로 변환\n",
        "        for idx, (x, y, score) in enumerate(nkps):  # 키포인트 데이터 순회\n",
        "            if score > 0.5:  # Confidence 임계값 이상일 경우\n",
        "                cv2.circle(frame, (int(x), int(y)), 3, (0, 0, 255), cv2.FILLED)  # 키포인트 원 표시\n",
        "                cv2.putText(frame, str(idx), (int(x), int(y)), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 0, 255), 1)  # 인덱스 추가\n",
        "\n",
        "    return frame\n",
        "\n",
        "# ============================= 바운딩 박스 + 키포인트 시각화 ==============================\n",
        "capture = cv2.VideoCapture(\"../datasets/woman.mp4\")\n",
        "\n",
        "while cv2.waitKey(10) < 0:\n",
        "    if capture.get(cv2.CAP_PROP_POS_FRAMES) == capture.get(cv2.CAP_PROP_FRAME_COUNT):  # 마지막 프레임일 경우\n",
        "        capture.set(cv2.CAP_PROP_POS_FRAMES, 0)  # 프레임 리셋\n",
        "\n",
        "    ret, frame = capture.read()  # 프레임 읽기\n",
        "    result = predict(frame)  # YOLO 예측\n",
        "    frame = draw_boxes(result, frame)  # 바운딩 박스 그리기\n",
        "    frame = draw_keypoints(result, frame)  # 키포인트 그리기\n",
        "    cv2.imshow(\"VideoFrame\", frame)  # 결과 출력\n",
        "\n",
        "capture.release()  # 리소스 해제\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "KXdV-YT-4-u1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 예제 10.01~10.07 ViT 모델 실습 (1)"
      ],
      "metadata": {
        "id": "4jzRqP_B5dcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================== 데이터셋 로드 및 서브셋 생성 ===================================\n",
        "from itertools import chain\n",
        "from collections import defaultdict\n",
        "from torch.utils.data import Subset\n",
        "from torchvision import datasets\n",
        "\n",
        "# 클래스별 서브셋 생성 함수 정의\n",
        "def subset_sampler(dataset, classes, max_len):\n",
        "    target_idx = defaultdict(list)\n",
        "    for idx, label in enumerate(dataset.train_labels):  # 각 라벨별 인덱스 수집\n",
        "        target_idx[int(label)].append(idx)\n",
        "\n",
        "    indices = list(\n",
        "        chain.from_iterable(\n",
        "            [target_idx[idx][:max_len] for idx in range(len(classes))]  # 최대 max_len만큼 인덱스 선택\n",
        "        )\n",
        "    )\n",
        "    return Subset(dataset, indices)  # Subset 객체 반환\n",
        "\n",
        "# FashionMNIST 데이터셋 로드\n",
        "train_dataset = datasets.FashionMNIST(root=\"../datasets\", download=True, train=True)\n",
        "test_dataset = datasets.FashionMNIST(root=\"../datasets\", download=True, train=False)\n",
        "\n",
        "# 클래스 정보 확인\n",
        "classes = train_dataset.classes\n",
        "class_to_idx = train_dataset.class_to_idx\n",
        "print(classes)\n",
        "print(class_to_idx)\n",
        "\n",
        "# 서브셋 생성\n",
        "subset_train_dataset = subset_sampler(\n",
        "    dataset=train_dataset, classes=train_dataset.classes, max_len=1000\n",
        ")\n",
        "subset_test_dataset = subset_sampler(\n",
        "    dataset=test_dataset, classes=test_dataset.classes, max_len=100\n",
        ")\n",
        "\n",
        "# 서브셋 크기 출력\n",
        "print(f\"Training Data Size : {len(subset_train_dataset)}\")\n",
        "print(f\"Testing Data Size : {len(subset_test_dataset)}\")\n",
        "print(train_dataset[0])  # 샘플 데이터 확인\n",
        "\n",
        "# =================================== 이미지 변환 정의 ===================================\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from transformers import AutoImageProcessor\n",
        "\n",
        "# AutoImageProcessor를 사용하여 이미지 처리 정보 로드\n",
        "image_processor = AutoImageProcessor.from_pretrained(\n",
        "    pretrained_model_name_or_path=\"google/vit-base-patch16-224-in21k\"\n",
        ")\n",
        "\n",
        "# 이미지 변환 파이프라인 정의\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),  # 이미지를 텐서로 변환\n",
        "        transforms.Resize(  # 이미지 크기 조정\n",
        "            size=(\n",
        "                image_processor.size[\"height\"],\n",
        "                image_processor.size[\"width\"]\n",
        "            )\n",
        "        ),\n",
        "        transforms.Lambda(\n",
        "            lambda x: torch.cat([x, x, x], 0)  # 흑백 이미지를 3채널로 확장\n",
        "        ),\n",
        "        transforms.Normalize(  # 정규화\n",
        "            mean=image_processor.image_mean,\n",
        "            std=image_processor.image_std\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 이미지 프로세서 정보 출력\n",
        "print(f\"size : {image_processor.size}\")\n",
        "print(f\"mean : {image_processor.image_mean}\")\n",
        "print(f\"std : {image_processor.image_std}\")\n",
        "\n",
        "# =================================== 데이터로더 정의 ===================================\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 배치 데이터를 변환 및 병합하는 함수 정의\n",
        "def collator(data, transform):\n",
        "    images, labels = zip(*data)  # 이미지와 라벨 분리\n",
        "    pixel_values = torch.stack([transform(image) for image in images])  # 이미지 변환\n",
        "    labels = torch.tensor([label for label in labels])  # 라벨 텐서화\n",
        "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
        "\n",
        "# 데이터로더 정의\n",
        "train_dataloader = DataLoader(\n",
        "    subset_train_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda x: collator(x, transform),\n",
        "    drop_last=True\n",
        ")\n",
        "valid_dataloader = DataLoader(\n",
        "    subset_test_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda x: collator(x, transform),\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "# 배치 데이터 확인\n",
        "batch = next(iter(train_dataloader))\n",
        "for key, value in batch.items():\n",
        "    print(f\"{key} : {value.shape}\")\n",
        "\n",
        "# =================================== ViT 모델 정의 ===================================\n",
        "from transformers import ViTForImageClassification\n",
        "\n",
        "# 사전학습된 ViT 모델 로드 및 재학습 설정\n",
        "model = ViTForImageClassification.from_pretrained(\n",
        "    pretrained_model_name_or_path=\"google/vit-base-patch16-224-in21k\",\n",
        "    num_labels=len(classes),  # 클래스 수 설정\n",
        "    id2label={idx: label for label, idx in class_to_idx.items()},  # id-라벨 매핑\n",
        "    label2id=class_to_idx,  # 라벨-id 매핑\n",
        "    ignore_mismatched_sizes=True  # 크기 불일치 무시\n",
        ")\n",
        "\n",
        "# 모델 구성 정보 출력\n",
        "print(model.classifier)\n",
        "print(model.vit.embeddings)\n",
        "\n",
        "# 배치 데이터 형태 확인\n",
        "batch = next(iter(train_dataloader))\n",
        "print(\"image shape :\", batch[\"pixel_values\"].shape)  # 이미지 크기\n",
        "print(\"patch embeddings shape :\",\n",
        "    model.vit.embeddings.patch_embeddings(batch[\"pixel_values\"]).shape\n",
        ")  # 패치 임베딩 크기\n",
        "print(\"[CLS] + patch embeddings shape :\",\n",
        "    model.vit.embeddings(batch[\"pixel_values\"]).shape\n",
        ")  # [CLS] 토큰 포함 임베딩 크기\n",
        "\n",
        "# =================================== 훈련 설정 ===================================\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# TrainingArguments 정의\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"../models/ViT-FashionMNIST\",  # 출력 디렉토리\n",
        "    save_strategy=\"epoch\",  # 에포크마다 저장\n",
        "    evaluation_strategy=\"epoch\",  # 에포크마다 평가\n",
        "    learning_rate=1e-5,  # 학습률\n",
        "    per_device_train_batch_size=16,  # 훈련 배치 크기\n",
        "    per_device_eval_batch_size=16,  # 평가 배치 크기\n",
        "    num_train_epochs=3,  # 에포크 수\n",
        "    weight_decay=0.001,  # 가중치 감소\n",
        "    load_best_model_at_end=True,  # 최상의 모델 로드\n",
        "    metric_for_best_model=\"f1\",  # 최상의 모델 기준 메트릭\n",
        "    logging_dir=\"logs\",  # 로그 디렉토리\n",
        "    logging_steps=125,  # 로그 기록 간격\n",
        "    remove_unused_columns=False,  # 불필요한 열 제거\n",
        "    seed=7  # 랜덤 시드 설정\n",
        ")\n",
        "\n",
        "# =================================== 평가 메트릭 정의 ===================================\n",
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "# F1 스코어 계산 함수 정의\n",
        "def compute_metrics(eval_pred):\n",
        "    metric = evaluate.load(\"f1\")  # F1 스코어 메트릭 로드\n",
        "    predictions, labels = eval_pred  # 예측값과 라벨 분리\n",
        "    predictions = np.argmax(predictions, axis=1)  # 예측값에서 가장 높은 클래스 선택\n",
        "    macro_f1 = metric.compute(\n",
        "        predictions=predictions, references=labels, average=\"macro\"\n",
        "    )  # 매크로 F1 계산\n",
        "    return macro_f1"
      ],
      "metadata": {
        "id": "6ssK21tz5frX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 예제 10.08~10.09 ViT 모델 실습 (2)"
      ],
      "metadata": {
        "id": "CIXYkTNx51l7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================= 필요한 라이브러리 임포트 =============================\n",
        "import torch\n",
        "import evaluate\n",
        "import numpy as np\n",
        "from itertools import chain\n",
        "from collections import defaultdict\n",
        "from torch.utils.data import Subset\n",
        "from torchvision import datasets, transforms\n",
        "from transformers import (\n",
        "    AutoImageProcessor,\n",
        "    ViTForImageClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# =========================== 서브셋 샘플링 함수 정의 ============================\n",
        "def subset_sampler(dataset, classes, max_len):\n",
        "    # 각 클래스별 인덱스를 수집하고 최대 max_len만큼 샘플링\n",
        "    target_idx = defaultdict(list)\n",
        "    for idx, label in enumerate(dataset.train_labels):  # 클래스별 인덱스 저장\n",
        "        target_idx[int(label)].append(idx)\n",
        "\n",
        "    indices = list(\n",
        "        chain.from_iterable(\n",
        "            [target_idx[idx][:max_len] for idx in range(len(classes))]  # 클래스별 최대 max_len 샘플링\n",
        "        )\n",
        "    )\n",
        "    return Subset(dataset, indices)  # 서브셋 반환\n",
        "\n",
        "# ========================= 모델 초기화 함수 정의 ===========================\n",
        "def model_init(classes, class_to_idx):\n",
        "    # ViT 모델 초기화\n",
        "    model = ViTForImageClassification.from_pretrained(\n",
        "        pretrained_model_name_or_path=\"google/vit-base-patch16-224-in21k\",  # 사전학습 모델 로드\n",
        "        num_labels=len(classes),  # 클래스 수 설정\n",
        "        id2label={idx: label for label, idx in class_to_idx.items()},  # ID-라벨 매핑\n",
        "        label2id=class_to_idx,  # 라벨-ID 매핑\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# =========================== 데이터 병합 함수 정의 ===========================\n",
        "def collator(data, transform):\n",
        "    # 배치 데이터를 변환 및 병합\n",
        "    images, labels = zip(*data)  # 이미지와 라벨 분리\n",
        "    pixel_values = torch.stack([transform(image) for image in images])  # 이미지 변환\n",
        "    labels = torch.tensor([label for label in labels])  # 라벨 텐서화\n",
        "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
        "\n",
        "# ========================== 평가 메트릭 계산 함수 정의 ==========================\n",
        "def compute_metrics(eval_pred):\n",
        "    # F1 스코어 계산\n",
        "    metric = evaluate.load(\"f1\")  # F1 스코어 메트릭 로드\n",
        "    predictions, labels = eval_pred  # 예측값과 라벨 분리\n",
        "    predictions = np.argmax(predictions, axis=1)  # 예측값에서 최대값 인덱스 선택\n",
        "    macro_f1 = metric.compute(\n",
        "        predictions=predictions, references=labels, average=\"macro\"\n",
        "    )  # 매크로 F1 계산\n",
        "    return macro_f1\n",
        "\n",
        "# ============================= 데이터셋 로드 ==============================\n",
        "train_dataset = datasets.FashionMNIST(root=\"../datasets\", download=True, train=True)\n",
        "test_dataset = datasets.FashionMNIST(root=\"../datasets\", download=True, train=False)\n",
        "\n",
        "classes = train_dataset.classes  # 클래스 이름\n",
        "class_to_idx = train_dataset.class_to_idx  # 클래스-인덱스 매핑\n",
        "\n",
        "# 서브셋 생성\n",
        "subset_train_dataset = subset_sampler(\n",
        "    dataset=train_dataset, classes=train_dataset.classes, max_len=1000\n",
        ")\n",
        "subset_test_dataset = subset_sampler(\n",
        "    dataset=test_dataset, classes=test_dataset.classes, max_len=100\n",
        ")\n",
        "\n",
        "# ========================== 이미지 변환 정의 ===========================\n",
        "# AutoImageProcessor를 사용하여 이미지 처리 정보 로드\n",
        "image_processor = AutoImageProcessor.from_pretrained(\n",
        "    pretrained_model_name_or_path=\"google/vit-base-patch16-224-in21k\"\n",
        ")\n",
        "\n",
        "# 이미지 변환 파이프라인 정의\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),  # 이미지를 텐서로 변환\n",
        "        transforms.Resize(  # 이미지 크기 조정\n",
        "            size=(\n",
        "                image_processor.size[\"height\"],\n",
        "                image_processor.size[\"width\"],\n",
        "            )\n",
        "        ),\n",
        "        transforms.Lambda(\n",
        "            lambda x: torch.cat([x, x, x], 0)  # 흑백 이미지를 3채널로 확장\n",
        "        ),\n",
        "        transforms.Normalize(  # 정규화\n",
        "            mean=image_processor.image_mean,\n",
        "            std=image_processor.image_std,\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# ============================ 훈련 인자 정의 ============================\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"../models/ViT-FashionMNIST\",  # 모델 출력 경로\n",
        "    save_strategy=\"epoch\",  # 에포크마다 저장\n",
        "    evaluation_strategy=\"epoch\",  # 에포크마다 평가\n",
        "    learning_rate=1e-5,  # 학습률\n",
        "    per_device_train_batch_size=16,  # 훈련 배치 크기\n",
        "    per_device_eval_batch_size=16,  # 평가 배치 크기\n",
        "    num_train_epochs=3,  # 에포크 수\n",
        "    weight_decay=0.001,  # 가중치 감소\n",
        "    load_best_model_at_end=True,  # 최상의 모델 로드\n",
        "    metric_for_best_model=\"f1\",  # 평가 메트릭\n",
        "    logging_dir=\"logs\",  # 로그 디렉토리\n",
        "    logging_steps=125,  # 로그 출력 간격\n",
        "    remove_unused_columns=False,  # 불필요한 열 제거\n",
        "    seed=7,  # 랜덤 시드\n",
        ")\n",
        "\n",
        "# ============================= Trainer 초기화 ==============================\n",
        "trainer = Trainer(\n",
        "    model_init=lambda x: model_init(classes, class_to_idx),  # 모델 초기화\n",
        "    args=args,  # 훈련 인자\n",
        "    train_dataset=subset_train_dataset,  # 훈련 데이터\n",
        "    eval_dataset=subset_test_dataset,  # 평가 데이터\n",
        "    data_collator=lambda x: collator(x, transform),  # 데이터 병합 함수\n",
        "    compute_metrics=compute_metrics,  # 메트릭 계산 함수\n",
        "    tokenizer=image_processor,  # 이미지 프로세서\n",
        ")\n",
        "\n",
        "# ============================= 모델 훈련 ==============================\n",
        "trainer.train()\n",
        "\n",
        "# ============================= 예측 및 평가 ==============================\n",
        "outputs = trainer.predict(subset_test_dataset)\n",
        "print(outputs)\n",
        "\n",
        "# =========================== 혼동 행렬 시각화 ===========================\n",
        "y_true = outputs.label_ids  # 실제 라벨\n",
        "y_pred = outputs.predictions.argmax(1)  # 예측된 라벨\n",
        "\n",
        "# 혼동 행렬 계산 및 시각화\n",
        "labels = list(classes)\n",
        "matrix = confusion_matrix(y_true, y_pred)  # 혼동 행렬 계산\n",
        "display = ConfusionMatrixDisplay(confusion_matrix=matrix, display_labels=labels)  # 시각화 객체 생성\n",
        "_, ax = plt.subplots(figsize=(10, 10))  # 플롯 크기 설정\n",
        "display.plot(xticks_rotation=45, ax=ax)  # 혼동 행렬 시각화\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QK-VDbac54Ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 예제 10.10~10.12 스윈 트랜스포머 모델 구조"
      ],
      "metadata": {
        "id": "5VpIETYT6H52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================= 상대 위치 좌표 계산 =============================\n",
        "import torch\n",
        "\n",
        "# 윈도우 크기 설정\n",
        "window_size = 2\n",
        "\n",
        "# 좌표 생성\n",
        "coords_h = torch.arange(window_size)  # 높이 좌표\n",
        "coords_w = torch.arange(window_size)  # 너비 좌표\n",
        "coords = torch.stack(torch.meshgrid([coords_h, coords_w], indexing=\"ij\"))  # 격자 좌표 생성\n",
        "coords_flatten = torch.flatten(coords, 1)  # 1차원으로 펼침\n",
        "\n",
        "# 상대 좌표 계산\n",
        "relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 상대 좌표 계산\n",
        "\n",
        "# 결과 출력\n",
        "print(relative_coords)\n",
        "print(relative_coords.shape)\n",
        "\n",
        "# ============================= X, Y 축 상대 좌표 계산 =============================\n",
        "x_coords = relative_coords[0, :, :]  # X축 상대 좌표\n",
        "y_coords = relative_coords[1, :, :]  # Y축 상대 좌표\n",
        "\n",
        "# 상대 좌표 보정 (③번 연산 과정)\n",
        "x_coords += window_size - 1\n",
        "y_coords += window_size - 1\n",
        "\n",
        "# X축 좌표 조정 (④번 연산 과정)\n",
        "x_coords *= 2 * window_size - 1\n",
        "\n",
        "# X, Y축 상대 좌표 행렬 출력\n",
        "print(f\"X축에 대한 행렬:\\n{x_coords}\\n\")\n",
        "print(f\"Y축에 대한 행렬:\\n{y_coords}\\n\")\n",
        "\n",
        "# X축과 Y축 좌표를 합산하여 위치 행렬 계산 (⑤번 연산 과정)\n",
        "relative_position_index = x_coords + y_coords\n",
        "print(f\"X, Y축에 대한 위치 행렬:\\n{relative_position_index}\")\n",
        "\n",
        "# ============================= 상대 위치 바이어스 테이블 생성 =============================\n",
        "num_heads = 1  # 헤드 수\n",
        "relative_position_bias_table = torch.Tensor(\n",
        "    torch.zeros((2 * window_size - 1) * (2 * window_size - 1), num_heads)  # 바이어스 테이블 초기화\n",
        ")\n",
        "\n",
        "# 상대 위치 바이어스 추출 및 재구성\n",
        "relative_position_bias = relative_position_bias_table[relative_position_index.view(-1)]\n",
        "relative_position_bias = relative_position_bias.view(\n",
        "    window_size * window_size, window_size * window_size, -1\n",
        ")\n",
        "\n",
        "# 결과 출력\n",
        "print(relative_position_bias.shape)"
      ],
      "metadata": {
        "id": "DmG6iTRT6Kau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 예제 10.13~10.18 스윈 트랜스포머 모델 실습 (1)"
      ],
      "metadata": {
        "id": "w79V_n-V6YrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================== 데이터셋 로드 및 서브셋 생성 ============================\n",
        "from itertools import chain\n",
        "from collections import defaultdict\n",
        "from torch.utils.data import Subset\n",
        "from torchvision import datasets\n",
        "\n",
        "# 서브셋 샘플링 함수 정의\n",
        "def subset_sampler(dataset, classes, max_len):\n",
        "    target_idx = defaultdict(list)  # 클래스별 인덱스를 저장할 딕셔너리\n",
        "    for idx, label in enumerate(dataset.train_labels):  # 데이터셋의 라벨을 순회하며 저장\n",
        "        target_idx[int(label)].append(idx)\n",
        "\n",
        "    indices = list(\n",
        "        chain.from_iterable(\n",
        "            [target_idx[idx][:max_len] for idx in range(len(classes))]  # 클래스별로 max_len만큼 샘플링\n",
        "        )\n",
        "    )\n",
        "    return Subset(dataset, indices)  # Subset 객체 반환\n",
        "\n",
        "# FashionMNIST 데이터셋 로드\n",
        "train_dataset = datasets.FashionMNIST(root=\"../datasets\", download=True, train=True)\n",
        "test_dataset = datasets.FashionMNIST(root=\"../datasets\", download=True, train=False)\n",
        "\n",
        "# 클래스 정보 추출\n",
        "classes = train_dataset.classes\n",
        "class_to_idx = train_dataset.class_to_idx\n",
        "\n",
        "# 서브셋 생성\n",
        "subset_train_dataset = subset_sampler(\n",
        "    dataset=train_dataset, classes=train_dataset.classes, max_len=1000\n",
        ")\n",
        "subset_test_dataset = subset_sampler(\n",
        "    dataset=test_dataset, classes=test_dataset.classes, max_len=100\n",
        ")\n",
        "\n",
        "# =========================== 이미지 변환 정의 ============================\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from transformers import AutoImageProcessor\n",
        "\n",
        "# 이미지 프로세서 초기화\n",
        "image_processor = AutoImageProcessor.from_pretrained(\n",
        "    pretrained_model_name_or_path=\"google/vit-base-patch16-224-in21k\"\n",
        ")\n",
        "\n",
        "# 이미지 변환 파이프라인 정의\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),  # 이미지를 텐서로 변환\n",
        "        transforms.Resize(  # 이미지 크기 조정\n",
        "            size=(\n",
        "                image_processor.size[\"height\"],\n",
        "                image_processor.size[\"width\"]\n",
        "            )\n",
        "        ),\n",
        "        transforms.Lambda(\n",
        "            lambda x: torch.cat([x, x, x], 0)  # 흑백 이미지를 3채널로 확장\n",
        "        ),\n",
        "        transforms.Normalize(  # 정규화\n",
        "            mean=image_processor.image_mean,\n",
        "            std=image_processor.image_std\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "# ========================== 데이터로더 정의 ==========================\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 데이터 배치 병합 함수 정의\n",
        "def collator(data, transform):\n",
        "    images, labels = zip(*data)  # 이미지와 라벨 분리\n",
        "    pixel_values = torch.stack([transform(image) for image in images])  # 이미지 변환\n",
        "    labels = torch.tensor([label for label in labels])  # 라벨 텐서화\n",
        "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
        "\n",
        "# 데이터로더 정의\n",
        "train_dataloader = DataLoader(\n",
        "    subset_train_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda x: collator(x, transform),\n",
        "    drop_last=True\n",
        ")\n",
        "valid_dataloader = DataLoader(\n",
        "    subset_test_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda x: collator(x, transform),\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "# ========================== Swin Transformer 모델 정의 ==========================\n",
        "from transformers import SwinForImageClassification\n",
        "\n",
        "# 사전학습된 Swin Transformer 모델 로드\n",
        "model = SwinForImageClassification.from_pretrained(\n",
        "    pretrained_model_name_or_path=\"microsoft/swin-tiny-patch4-window7-224\",\n",
        "    num_labels=len(train_dataset.classes),  # 클래스 수 설정\n",
        "    id2label={idx: label for label, idx in train_dataset.class_to_idx.items()},  # id-라벨 매핑\n",
        "    label2id=train_dataset.class_to_idx,  # 라벨-id 매핑\n",
        "    ignore_mismatched_sizes=True  # 크기 불일치 무시\n",
        ")\n",
        "\n",
        "# 모델 구조 출력\n",
        "for main_name, main_module in model.named_children():\n",
        "    print(main_name)\n",
        "    for sub_name, sub_module in main_module.named_children():\n",
        "        print(\"└\", sub_name)\n",
        "        for ssub_name, ssub_module in sub_module.named_children():\n",
        "            print(\"│  └\", ssub_name)\n",
        "            for sssub_name, sssub_module in ssub_module.named_children():\n",
        "                if sssub_name == \"projection\":\n",
        "                    print(\"│  │  └\", sssub_name, sssub_module)\n",
        "                else:\n",
        "                    print(\"│  │  └\", sssub_name)\n",
        "\n",
        "# ========================== 패치 임베딩 차원 확인 ==========================\n",
        "batch = next(iter(train_dataloader))\n",
        "print(\"이미지 차원 :\", batch[\"pixel_values\"].shape)\n",
        "\n",
        "# 패치 임베딩 모듈 적용\n",
        "patch_emb_output, shape = model.swin.embeddings.patch_embeddings(batch[\"pixel_values\"])\n",
        "print(\"모듈:\", model.swin.embeddings.patch_embeddings)\n",
        "print(\"패치 임베딩 차원 :\", patch_emb_output.shape)\n",
        "\n",
        "# =========================== Swin Layer 내부 모듈 구조 ============================\n",
        "for main_name, main_module in model.swin.encoder.layers[0].named_children():\n",
        "    print(main_name)\n",
        "    for sub_name, sub_module in main_module.named_children():\n",
        "        print(\"└\", sub_name)\n",
        "        for ssub_name, ssub_module in sub_module.named_children():\n",
        "            print(\"│ └\", ssub_name)\n",
        "\n",
        "print(model.swin.encoder.layers[0].blocks[0])  # 첫 번째 Swin Layer의 블록 출력\n",
        "\n",
        "# ========================== W-MSA 및 SW-MSA 적용 ==========================\n",
        "W_MSA = model.swin.encoder.layers[0].blocks[0]  # Window Multi-head Self-Attention\n",
        "SW_MSA = model.swin.encoder.layers[0].blocks[1]  # Shifted Window Multi-head Self-Attention\n",
        "\n",
        "W_MSA_output = W_MSA(patch_emb_output, W_MSA.input_resolution)[0]\n",
        "SW_MSA_output = SW_MSA(W_MSA_output, SW_MSA.input_resolution)[0]\n",
        "\n",
        "print(\"W-MSA 결과 차원 :\", W_MSA_output.shape)\n",
        "print(\"SW-MSA 결과 차원 :\", SW_MSA_output.shape)\n",
        "\n",
        "# ========================== 패치 병합 모듈 확인 ==========================\n",
        "patch_merge = model.swin.encoder.layers[0].downsample\n",
        "print(\"patch_merge 모듈 :\", patch_merge)\n",
        "\n",
        "# 패치 병합 적용\n",
        "output = patch_merge(SW_MSA_output, patch_merge.input_resolution)\n",
        "print(\"patch_merge 결과 차원 :\", output.shape)"
      ],
      "metadata": {
        "id": "JzqIGDaH6bGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 예제 10.19 스윈 트랜스포머 모델 실습 (2)"
      ],
      "metadata": {
        "id": "YHbdpOJE6-tU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================= 환경 설정 =============================\n",
        "# 애플 실리콘 사용자를 위한 PyTorch MPS 설정 (필요시 활성화)\n",
        "# import os\n",
        "# os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
        "\n",
        "# 필요한 라이브러리 임포트\n",
        "import torch\n",
        "import evaluate\n",
        "import numpy as np\n",
        "from itertools import chain\n",
        "from collections import defaultdict\n",
        "from torch.utils.data import Subset\n",
        "from torchvision import datasets, transforms\n",
        "from transformers import AutoImageProcessor, SwinForImageClassification\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# =========================== 서브셋 샘플링 함수 정의 ============================\n",
        "def subset_sampler(dataset, classes, max_len):\n",
        "    target_idx = defaultdict(list)  # 클래스별 인덱스 저장\n",
        "    for idx, label in enumerate(dataset.train_labels):  # 데이터셋 라벨 확인\n",
        "        target_idx[int(label)].append(idx)\n",
        "\n",
        "    indices = list(\n",
        "        chain.from_iterable(\n",
        "            [target_idx[idx][:max_len] for idx in range(len(classes))]  # 클래스별 최대 max_len 샘플링\n",
        "        )\n",
        "    )\n",
        "    return Subset(dataset, indices)  # Subset 반환\n",
        "\n",
        "# =========================== 모델 초기화 함수 정의 ============================\n",
        "def model_init(classes, class_to_idx):\n",
        "    model = SwinForImageClassification.from_pretrained(\n",
        "        pretrained_model_name_or_path=\"microsoft/swin-tiny-patch4-window7-224\",\n",
        "        num_labels=len(classes),  # 클래스 수\n",
        "        id2label={idx: label for label, idx in class_to_idx.items()},  # ID-라벨 매핑\n",
        "        label2id=class_to_idx,  # 라벨-ID 매핑\n",
        "        ignore_mismatched_sizes=True  # 크기 불일치 무시\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# ========================= 데이터 병합 함수 정의 ===========================\n",
        "def collator(data, transform):\n",
        "    images, labels = zip(*data)  # 이미지와 라벨 분리\n",
        "    pixel_values = torch.stack([transform(image) for image in images])  # 이미지 변환\n",
        "    labels = torch.tensor([label for label in labels])  # 라벨 텐서화\n",
        "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
        "\n",
        "# =========================== 평가 메트릭 함수 정의 ============================\n",
        "def compute_metrics(eval_pred):\n",
        "    metric = evaluate.load(\"f1\")  # F1 스코어 메트릭 로드\n",
        "    predictions, labels = eval_pred  # 예측값과 라벨 분리\n",
        "    predictions = np.argmax(predictions, axis=1)  # 가장 높은 값의 인덱스 선택\n",
        "    macro_f1 = metric.compute(\n",
        "        predictions=predictions, references=labels, average=\"macro\"  # 매크로 F1 스코어 계산\n",
        "    )\n",
        "    return macro_f1\n",
        "\n",
        "# ============================= 데이터셋 로드 ==============================\n",
        "train_dataset = datasets.FashionMNIST(root=\"../datasets\", download=True, train=True)\n",
        "test_dataset = datasets.FashionMNIST(root=\"../datasets\", download=True, train=False)\n",
        "\n",
        "# 클래스 정보 추출\n",
        "classes = train_dataset.classes\n",
        "class_to_idx = train_dataset.class_to_idx\n",
        "\n",
        "# 서브셋 생성\n",
        "subset_train_dataset = subset_sampler(\n",
        "    dataset=train_dataset, classes=train_dataset.classes, max_len=1000\n",
        ")\n",
        "subset_test_dataset = subset_sampler(\n",
        "    dataset=test_dataset, classes=test_dataset.classes, max_len=100\n",
        ")\n",
        "\n",
        "# ========================= 이미지 프로세서 및 변환 정의 ===========================\n",
        "image_processor = AutoImageProcessor.from_pretrained(\n",
        "    pretrained_model_name_or_path=\"microsoft/swin-tiny-patch4-window7-224\"\n",
        ")\n",
        "\n",
        "# 이미지 변환 파이프라인 정의\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),  # 이미지를 텐서로 변환\n",
        "        transforms.Resize(  # 이미지 크기 조정\n",
        "            size=(\n",
        "                image_processor.size[\"height\"],\n",
        "                image_processor.size[\"width\"]\n",
        "            )\n",
        "        ),\n",
        "        transforms.Lambda(\n",
        "            lambda x: torch.cat([x, x, x], 0)  # 흑백 이미지를 3채널로 확장\n",
        "        ),\n",
        "        transforms.Normalize(  # 정규화\n",
        "            mean=image_processor.image_mean,\n",
        "            std=image_processor.image_std\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "# ============================ 훈련 인자 정의 =============================\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"../models/Swin-FashionMNIST\",  # 모델 출력 경로\n",
        "    save_strategy=\"epoch\",  # 에포크마다 저장\n",
        "    evaluation_strategy=\"epoch\",  # 에포크마다 평가\n",
        "    learning_rate=1e-5,  # 학습률\n",
        "    per_device_train_batch_size=16,  # 훈련 배치 크기\n",
        "    per_device_eval_batch_size=16,  # 평가 배치 크기\n",
        "    num_train_epochs=3,  # 에포크 수\n",
        "    weight_decay=0.001,  # 가중치 감소\n",
        "    load_best_model_at_end=True,  # 최상의 모델 로드\n",
        "    metric_for_best_model=\"f1\",  # 평가 메트릭\n",
        "    logging_dir=\"logs\",  # 로그 경로\n",
        "    logging_steps=125,  # 로그 출력 간격\n",
        "    remove_unused_columns=False,  # 불필요한 열 제거\n",
        "    seed=7  # 랜덤 시드\n",
        ")\n",
        "\n",
        "# ============================= Trainer 초기화 ==============================\n",
        "trainer = Trainer(\n",
        "    model_init=lambda x: model_init(classes, class_to_idx),  # 모델 초기화\n",
        "    args=args,  # 훈련 인자\n",
        "    train_dataset=subset_train_dataset,  # 훈련 데이터\n",
        "    eval_dataset=subset_test_dataset,  # 평가 데이터\n",
        "    data_collator=lambda x: collator(x, transform),  # 데이터 병합 함수\n",
        "    compute_metrics=compute_metrics,  # 메트릭 계산 함수\n",
        "    tokenizer=image_processor,  # 이미지 프로세서\n",
        ")\n",
        "\n",
        "# ============================= 모델 훈련 ==============================\n",
        "trainer.train()\n",
        "\n",
        "# ============================= 예측 및 평가 ==============================\n",
        "outputs = trainer.predict(subset_test_dataset)\n",
        "print(outputs)\n",
        "\n",
        "# =========================== 혼동 행렬 시각화 ===========================\n",
        "y_true = outputs.label_ids  # 실제 라벨\n",
        "y_pred = outputs.predictions.argmax(1)  # 예측된 라벨\n",
        "\n",
        "# 혼동 행렬 계산 및 시각화\n",
        "labels = list(classes)\n",
        "matrix = confusion_matrix(y_true, y_pred)  # 혼동 행렬 계산\n",
        "display = ConfusionMatrixDisplay(confusion_matrix=matrix, display_labels=labels)  # 시각화 객체 생성\n",
        "_, ax = plt.subplots(figsize=(10, 10))  # 플롯 크기 설정\n",
        "display.plot(xticks_rotation=45, ax=ax)  # 혼동 행렬 시각화\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "h4uUH5FV7DrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 예제 10.20~10.23 CvT 모델 실습 (1)"
      ],
      "metadata": {
        "id": "udQJbHVf8ZjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================== 데이터셋 로드 및 서브셋 생성 ============================\n",
        "from itertools import chain\n",
        "from collections import defaultdict\n",
        "from torch.utils.data import Subset\n",
        "from torchvision import datasets\n",
        "\n",
        "# 서브셋 샘플링 함수 정의\n",
        "def subset_sampler(dataset, classes, max_len):\n",
        "    target_idx = defaultdict(list)  # 클래스별 인덱스를 저장할 딕셔너리\n",
        "    for idx, label in enumerate(dataset.train_labels):  # 데이터셋 라벨 확인\n",
        "        target_idx[int(label)].append(idx)\n",
        "\n",
        "    indices = list(\n",
        "        chain.from_iterable(\n",
        "            [target_idx[idx][:max_len] for idx in range(len(classes))]  # 클래스별 최대 max_len 샘플링\n",
        "        )\n",
        "    )\n",
        "    return Subset(dataset, indices)  # Subset 반환\n",
        "\n",
        "# FashionMNIST 데이터셋 로드\n",
        "train_dataset = datasets.FashionMNIST(root=\"../datasets\", download=True, train=True)\n",
        "test_dataset = datasets.FashionMNIST(root=\"../datasets\", download=True, train=False)\n",
        "\n",
        "# 클래스 정보 추출\n",
        "classes = train_dataset.classes\n",
        "class_to_idx = train_dataset.class_to_idx\n",
        "\n",
        "# 서브셋 생성\n",
        "subset_train_dataset = subset_sampler(\n",
        "    dataset=train_dataset, classes=train_dataset.classes, max_len=1000\n",
        ")\n",
        "subset_test_dataset = subset_sampler(\n",
        "    dataset=test_dataset, classes=test_dataset.classes, max_len=100\n",
        ")\n",
        "\n",
        "# =========================== 이미지 변환 정의 ============================\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from transformers import AutoImageProcessor\n",
        "\n",
        "# 이미지 프로세서 초기화\n",
        "image_processor = AutoImageProcessor.from_pretrained(\n",
        "    pretrained_model_name_or_path=\"microsoft/cvt-21\"\n",
        ")\n",
        "\n",
        "# 이미지 변환 파이프라인 정의\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),  # 이미지를 텐서로 변환\n",
        "        transforms.Resize(  # 이미지 크기 조정\n",
        "            size=(\n",
        "                image_processor.size[\"shortest_edge\"],  # 짧은 가장자리 크기 사용\n",
        "                image_processor.size[\"shortest_edge\"]\n",
        "            )\n",
        "        ),\n",
        "        transforms.Lambda(lambda x: torch.cat([x, x, x], 0)),  # 흑백 이미지를 3채널로 확장\n",
        "        transforms.Normalize(  # 정규화\n",
        "            mean=image_processor.image_mean,\n",
        "            std=image_processor.image_std\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "# ========================== 데이터로더 정의 ==========================\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 데이터 배치 병합 함수 정의\n",
        "def collator(data, transform):\n",
        "    images, labels = zip(*data)  # 이미지와 라벨 분리\n",
        "    pixel_values = torch.stack([transform(image) for image in images])  # 이미지 변환\n",
        "    labels = torch.tensor([label for label in labels])  # 라벨 텐서화\n",
        "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
        "\n",
        "# 데이터로더 정의\n",
        "train_dataloader = DataLoader(\n",
        "    subset_train_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda x: collator(x, transform),\n",
        "    drop_last=True\n",
        ")\n",
        "valid_dataloader = DataLoader(\n",
        "    subset_test_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda x: collator(x, transform),\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "# ========================== CvT 모델 정의 및 구조 확인 ==========================\n",
        "from transformers import CvtForImageClassification\n",
        "\n",
        "# 사전학습된 CvT 모델 로드\n",
        "model = CvtForImageClassification.from_pretrained(\n",
        "    pretrained_model_name_or_path=\"microsoft/cvt-21\",\n",
        "    num_labels=len(train_dataset.classes),  # 클래스 수 설정\n",
        "    id2label={idx: label for label, idx in train_dataset.class_to_idx.items()},  # ID-라벨 매핑\n",
        "    label2id=train_dataset.class_to_idx,  # 라벨-ID 매핑\n",
        "    ignore_mismatched_sizes=True  # 크기 불일치 무시\n",
        ")\n",
        "\n",
        "# 모델 구조 출력\n",
        "for main_name, main_module in model.named_children():\n",
        "    print(main_name)\n",
        "    for sub_name, sub_module in main_module.named_children():\n",
        "        print(\"└\", sub_name)\n",
        "        for ssub_name, ssub_module in sub_module.named_children():\n",
        "            print(\"   └\", ssub_name)\n",
        "            for sssub_name, sssub_module in ssub_module.named_children():\n",
        "                print(\"     └\", sssub_name)\n",
        "\n",
        "# ========================== CvT 모델 스테이지 확인 ==========================\n",
        "stages = model.cvt.encoder.stages\n",
        "print(stages[0])  # 첫 번째 스테이지 확인\n",
        "\n",
        "# ========================== 패치 임베딩 적용 및 출력 ==========================\n",
        "batch = next(iter(train_dataloader))\n",
        "print(\"이미지 차원 :\", batch[\"pixel_values\"].shape)\n",
        "\n",
        "# 패치 임베딩 모듈 적용\n",
        "patch_emb_output = stages[0].embedding(batch[\"pixel_values\"])\n",
        "print(\"패치 임베딩 차원 :\", patch_emb_output.shape)\n",
        "\n",
        "# ========================== 셀프 어텐션 입력 확인 ==========================\n",
        "batch_size, num_channels, height, width = patch_emb_output.shape\n",
        "hidden_state = patch_emb_output.view(batch_size, num_channels, height * width).permute(0, 2, 1)\n",
        "print(\"셀프 어텐션 입력 차원 :\", hidden_state.shape)\n",
        "\n",
        "# ========================== 셀프 어텐션 출력 확인 ==========================\n",
        "attention_output = stages[0].layers[0].attention.attention(hidden_state, height, width)\n",
        "print(\"셀프 어텐션 출력 차원 :\", attention_output.shape)"
      ],
      "metadata": {
        "id": "KIgL6M9F8bWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 예제 10.24 CvT 모델 실습 (2)"
      ],
      "metadata": {
        "id": "zpIbgh8B8dIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================= 필요한 라이브러리 임포트 =============================\n",
        "import torch\n",
        "import evaluate\n",
        "import numpy as np\n",
        "from itertools import chain\n",
        "from collections import defaultdict\n",
        "from torch.utils.data import Subset\n",
        "from torchvision import datasets, transforms\n",
        "from transformers import AutoImageProcessor, CvtForImageClassification\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# =========================== 서브셋 샘플링 함수 정의 ============================\n",
        "def subset_sampler(dataset, classes, max_len):\n",
        "    # 클래스별로 max_len만큼 데이터 샘플링\n",
        "    target_idx = defaultdict(list)  # 클래스별 인덱스를 저장할 딕셔너리\n",
        "    for idx, label in enumerate(dataset.train_labels):  # 데이터셋 라벨 순회\n",
        "        target_idx[int(label)].append(idx)\n",
        "\n",
        "    indices = list(\n",
        "        chain.from_iterable(\n",
        "            [target_idx[idx][:max_len] for idx in range(len(classes))]  # 클래스별 최대 max_len 샘플링\n",
        "        )\n",
        "    )\n",
        "    return Subset(dataset, indices)  # Subset 반환\n",
        "\n",
        "# =========================== 모델 초기화 함수 정의 ============================\n",
        "def model_init(classes, class_to_idx):\n",
        "    # CvT 모델 초기화\n",
        "    model = CvtForImageClassification.from_pretrained(\n",
        "        pretrained_model_name_or_path=\"microsoft/cvt-21\",\n",
        "        num_labels=len(classes),  # 클래스 수\n",
        "        id2label={idx: label for label, idx in class_to_idx.items()},  # ID-라벨 매핑\n",
        "        label2id=class_to_idx,  # 라벨-ID 매핑\n",
        "        ignore_mismatched_sizes=True  # 크기 불일치 무시\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# ========================= 데이터 병합 함수 정의 ===========================\n",
        "def collator(data, transform):\n",
        "    # 배치 데이터를 변환 및 병합\n",
        "    images, labels = zip(*data)  # 이미지와 라벨 분리\n",
        "    pixel_values = torch.stack([transform(image) for image in images])  # 이미지 변환\n",
        "    labels = torch.tensor([label for label in labels])  # 라벨 텐서화\n",
        "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
        "\n",
        "# =========================== 평가 메트릭 함수 정의 ============================\n",
        "def compute_metrics(eval_pred):\n",
        "    metric = evaluate.load(\"f1\")  # F1 스코어 메트릭 로드\n",
        "    predictions, labels = eval_pred  # 예측값과 라벨 분리\n",
        "    predictions = np.argmax(predictions, axis=1)  # 가장 높은 값의 인덱스 선택\n",
        "    macro_f1 = metric.compute(\n",
        "        predictions=predictions, references=labels, average=\"macro\"  # 매크로 F1 스코어 계산\n",
        "    )\n",
        "    return macro_f1\n",
        "\n",
        "# ============================= 데이터셋 로드 ==============================\n",
        "train_dataset = datasets.FashionMNIST(root=\"../datasets\", download=True, train=True)\n",
        "test_dataset = datasets.FashionMNIST(root=\"../datasets\", download=True, train=False)\n",
        "\n",
        "# 클래스 정보 추출\n",
        "classes = train_dataset.classes\n",
        "class_to_idx = train_dataset.class_to_idx\n",
        "\n",
        "# 서브셋 생성\n",
        "subset_train_dataset = subset_sampler(\n",
        "    dataset=train_dataset, classes=train_dataset.classes, max_len=1000\n",
        ")\n",
        "subset_test_dataset = subset_sampler(\n",
        "    dataset=test_dataset, classes=test_dataset.classes, max_len=100\n",
        ")\n",
        "\n",
        "# ========================= 이미지 프로세서 및 변환 정의 ===========================\n",
        "image_processor = AutoImageProcessor.from_pretrained(\n",
        "    pretrained_model_name_or_path=\"microsoft/cvt-21\"\n",
        ")\n",
        "\n",
        "# 이미지 변환 파이프라인 정의\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),  # 이미지를 텐서로 변환\n",
        "        transforms.Resize(  # 이미지 크기 조정\n",
        "            size=(\n",
        "                image_processor.size[\"shortest_edge\"],  # 짧은 가장자리 크기 사용\n",
        "                image_processor.size[\"shortest_edge\"]\n",
        "            )\n",
        "        ),\n",
        "        transforms.Lambda(\n",
        "            lambda x: torch.cat([x, x, x], 0)  # 흑백 이미지를 3채널로 확장\n",
        "        ),\n",
        "        transforms.Normalize(  # 정규화\n",
        "            mean=image_processor.image_mean,\n",
        "            std=image_processor.image_std\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "# ============================ 훈련 인자 정의 =============================\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"../models/CvT-FashionMNIST\",  # 모델 출력 경로\n",
        "    save_strategy=\"epoch\",  # 에포크마다 저장\n",
        "    evaluation_strategy=\"epoch\",  # 에포크마다 평가\n",
        "    learning_rate=1e-5,  # 학습률\n",
        "    per_device_train_batch_size=16,  # 훈련 배치 크기\n",
        "    per_device_eval_batch_size=16,  # 평가 배치 크기\n",
        "    num_train_epochs=3,  # 에포크 수\n",
        "    weight_decay=0.001,  # 가중치 감소\n",
        "    load_best_model_at_end=True,  # 최상의 모델 로드\n",
        "    metric_for_best_model=\"f1\",  # 평가 메트릭\n",
        "    logging_dir=\"logs\",  # 로그 경로\n",
        "    logging_steps=125,  # 로그 출력 간격\n",
        "    remove_unused_columns=False,  # 불필요한 열 제거\n",
        "    seed=7  # 랜덤 시드\n",
        ")\n",
        "\n",
        "# ============================= Trainer 초기화 ==============================\n",
        "trainer = Trainer(\n",
        "    model_init=lambda x: model_init(classes, class_to_idx),  # 모델 초기화\n",
        "    args=args,  # 훈련 인자\n",
        "    train_dataset=subset_train_dataset,  # 훈련 데이터\n",
        "    eval_dataset=subset_test_dataset,  # 평가 데이터\n",
        "    data_collator=lambda x: collator(x, transform),  # 데이터 병합 함수\n",
        "    compute_metrics=compute_metrics,  # 메트릭 계산 함수\n",
        "    tokenizer=image_processor,  # 이미지 프로세서\n",
        ")\n",
        "\n",
        "# ============================= 모델 훈련 ==============================\n",
        "trainer.train()\n",
        "\n",
        "# ============================= 예측 및 평가 ==============================\n",
        "outputs = trainer.predict(subset_test_dataset)\n",
        "print(outputs)\n",
        "\n",
        "# =========================== 혼동 행렬 시각화 ===========================\n",
        "y_true = outputs.label_ids  # 실제 라벨\n",
        "y_pred = outputs.predictions.argmax(1)  # 예측된 라벨\n",
        "\n",
        "# 혼동 행렬 계산 및 시각화\n",
        "labels = list(classes)\n",
        "matrix = confusion_matrix(y_true, y_pred)  # 혼동 행렬 계산\n",
        "display = ConfusionMatrixDisplay(confusion_matrix=matrix, display_labels=labels)  # 시각화 객체 생성\n",
        "_, ax = plt.subplots(figsize=(10, 10))  # 플롯 크기 설정\n",
        "display.plot(xticks_rotation=45, ax=ax)  # 혼동 행렬 시각화\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3FUNRqzw8fO_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}