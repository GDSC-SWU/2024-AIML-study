{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# N-gram"
      ],
      "metadata": {
        "id": "z6_u62kxmeBR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfeHcC1Bmcoe",
        "outputId": "3738479d-41ad-4c5e-8352-5be50b06dd67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('안녕하세요',), ('넌',), ('누구세요?',), ('대신',), ('알고리즘',), ('시험',), ('봐주세요.',)]\n",
            "[('안녕하세요', '넌'), ('넌', '누구세요?'), ('누구세요?', '대신'), ('대신', '알고리즘'), ('알고리즘', '시험'), ('시험', '봐주세요.')]\n",
            "[('안녕하세요', '넌', '누구세요?'), ('넌', '누구세요?', '대신'), ('누구세요?', '대신', '알고리즘'), ('대신', '알고리즘', '시험'), ('알고리즘', '시험', '봐주세요.')]\n",
            "[('안녕하세요',), ('넌',), ('누구세요?',), ('대신',), ('알고리즘',), ('시험',), ('봐주세요.',)]\n",
            "[('안녕하세요', '넌'), ('넌', '누구세요?'), ('누구세요?', '대신'), ('대신', '알고리즘'), ('알고리즘', '시험'), ('시험', '봐주세요.')]\n",
            "[('안녕하세요', '넌', '누구세요?'), ('넌', '누구세요?', '대신'), ('누구세요?', '대신', '알고리즘'), ('대신', '알고리즘', '시험'), ('알고리즘', '시험', '봐주세요.')]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "\n",
        "def ngrams(sentence, n):\n",
        "    words = sentence.split()\n",
        "    ngrams = zip(*[words[i:] for i in range(n)])\n",
        "    return list(ngrams)\n",
        "\n",
        "sentence = \"안녕하세요 넌 누구세요? 대신 알고리즘 시험 봐주세요.\"\n",
        "\n",
        "unigram = ngrams(sentence, 1)\n",
        "bigram = ngrams(sentence, 2)\n",
        "trigram = ngrams(sentence, 3)\n",
        "\n",
        "print(unigram)\n",
        "print(bigram)\n",
        "print(trigram)\n",
        "\n",
        "unigram = nltk.ngrams(sentence.split(), 1)\n",
        "bigram = nltk.ngrams(sentence.split(), 2)\n",
        "trigram = nltk.ngrams(sentence.split(), 3)\n",
        "\n",
        "print(list(unigram))\n",
        "print(list(bigram))\n",
        "print(list(trigram))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF"
      ],
      "metadata": {
        "id": "sOzc5ESEk7ZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISy9-d-vmt3e",
        "outputId": "c97e6ae0-4dd8-4ca2-9f29-65d9b9e4d273"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TF-IDF 클래스\n",
        "import sklearn\n",
        "\n",
        "tfidf_vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(\n",
        "    input=\"content\",\n",
        "    encoding = \"utf-8\",\n",
        "    lowercase=True,\n",
        "    stop_words=None,\n",
        "    ngram_range=(1,1),\n",
        "    max_df=1.0,\n",
        "    min_df=1,\n",
        "    vocabulary=None,\n",
        "    smooth_idf=True,\n",
        ")"
      ],
      "metadata": {
        "id": "uHXPM3dWk9bN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TF-IDF 계산\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "corpus = [\n",
        "    \"That movie is famous movie\",\n",
        "    \"I like that actor\",\n",
        "    \"I don’t like that actor\"\n",
        "]\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_vectorizer.fit(corpus)\n",
        "tfidf_matrix = tfidf_vectorizer.transform(corpus)\n",
        "# tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(tfidf_matrix.toarray())\n",
        "print(tfidf_vectorizer.vocabulary_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "othN8GpvmZ3V",
        "outputId": "edb796d2-31a9-4e12-d7ef-2a9ba1c784c3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.         0.39687454 0.39687454 0.         0.79374908\n",
            "  0.2344005 ]\n",
            " [0.61980538 0.         0.         0.         0.61980538 0.\n",
            "  0.48133417]\n",
            " [0.4804584  0.63174505 0.         0.         0.4804584  0.\n",
            "  0.37311881]]\n",
            "{'that': 6, 'movie': 5, 'is': 3, 'famous': 2, 'like': 4, 'actor': 0, 'don': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Skip-gram 모델"
      ],
      "metadata": {
        "id": "enSSGrpq5zmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "num_embeddings = 1000  # 임베딩할 단어의 수\n",
        "embedding_dim = 64     # 각 단어의 임베딩 차원\n",
        "\n",
        "embedding = torch.nn.Embedding(\n",
        "    num_embeddings,\n",
        "    embedding_dim,\n",
        "    padding_idx=None,\n",
        "    max_norm=None,\n",
        "    norm_type=2.0\n",
        ")"
      ],
      "metadata": {
        "id": "AsZVjxLB40Od"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#기본 Skip-gram 클래스\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class VanillaSkipgram(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=vocab_size, #임베딩의 수(단어 사전의 크기)\n",
        "            embedding_dim=embedding_dim #임베딩 베터의 크기\n",
        "        )\n",
        "        self.linear = nn.Linear( #선형변환 클래스\n",
        "            in_features=embedding_dim,\n",
        "            out_features=vocab_size\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        embeddings = self.embedding(input_ids)\n",
        "        output = self.linear(embeddings)\n",
        "        return output"
      ],
      "metadata": {
        "id": "PvwF2ZSQ6epS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "apt-get update\n",
        "apt-get install g++ openjdk-8-jdk python-dev python3-dev\n",
        "pip3 install JPype1\n",
        "pip3 install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADywSpQ98m90",
        "outputId": "6f73423d-d496-46cf-e43e-1833dcf5ca68"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:5 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:6 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Ign:10 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:11 https://r2u.stat.illinois.edu/ubuntu jammy Release\n",
            "Fetched 129 kB in 1s (92.2 kB/s)\n",
            "Reading package lists...\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "Package python-dev is not available, but is referred to by another package.\n",
            "This may mean that the package is missing, has been obsoleted, or\n",
            "is only available from another source\n",
            "However the following packages replace it:\n",
            "  python2-dev python2 python-dev-is-python3\n",
            "\n",
            "Requirement already satisfied: JPype1 in /usr/local/lib/python3.10/dist-packages (1.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1) (24.1)\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.5.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.4)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (24.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "E: Package 'python-dev' has no installation candidate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%env JAVA_HOME \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCmXaj-o8-rt",
        "outputId": "2bd8ed95-aeab-4ef8-9598-9c7302a51aed"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: JAVA_HOME=\"/usr/lib/jvm/java-8-openjdk-amd64\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install korpora"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTepkjpA8udW",
        "outputId": "4675fa14-94e7-4d90-94bf-42f6f338ac63"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: korpora in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: dataclasses>=0.6 in /usr/local/lib/python3.10/dist-packages (from korpora) (0.6)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from korpora) (1.26.4)\n",
            "Requirement already satisfied: tqdm>=4.46.0 in /usr/local/lib/python3.10/dist-packages (from korpora) (4.66.5)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from korpora) (2.32.3)\n",
            "Requirement already satisfied: xlrd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from korpora) (2.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->korpora) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->korpora) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->korpora) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->korpora) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#영화리뷰 데이터세트 전처리\n",
        "import pandas as pd\n",
        "from Korpora import Korpora\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "#영화리뷰 감정 분석 데이터세트 불러옴\n",
        "corpus = Korpora.load(\"nsmc\")\n",
        "corpus = pd.DataFrame(corpus.test)\n",
        "\n",
        "#OKT 토크나이저를 사용해 형태소를 추출함.\n",
        "tokenizer = Okt()\n",
        "tokens = [tokenizer.morphs(review) for review in corpus.text]\n",
        "print(tokens[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InpgTxGp7Kt1",
        "outputId": "62ebcfe6-8c70-4f47-fe7e-4dfeea8d647b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
            "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
            "\n",
            "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
            "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
            "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
            "\n",
            "    # Description\n",
            "    Author : e9t@github\n",
            "    Repository : https://github.com/e9t/nsmc\n",
            "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
            "\n",
            "    Naver sentiment movie corpus v1.0\n",
            "    This is a movie review dataset in the Korean language.\n",
            "    Reviews were scraped from Naver Movies.\n",
            "\n",
            "    The dataset construction is based on the method noted in\n",
            "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
            "\n",
            "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
            "\n",
            "    # License\n",
            "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
            "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nsmc] download ratings_train.txt: 14.6MB [00:00, 159MB/s]\n",
            "[nsmc] download ratings_test.txt: 4.90MB [00:00, 49.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['굳', 'ㅋ'], ['GDNTOPCLASSINTHECLUB'], ['뭐', '야', '이', '평점', '들', '은', '....', '나쁘진', '않지만', '10', '점', '짜', '리', '는', '더', '더욱', '아니잖아']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#단어 사전 구축\n",
        "from collections import Counter\n",
        "\n",
        "def build_vocab(corpus, n_vocab, special_tokens): #build_vocab 함수로 단어 사전 구축\n",
        "    counter = Counter()\n",
        "    for tokens in corpus:\n",
        "        counter.update(tokens)\n",
        "    vocab = special_tokens\n",
        "    for token, count in counter.most_common(n_vocab):\n",
        "        vocab.append(token)\n",
        "    return vocab\n",
        "\n",
        " #n_vocab 매개변수는 구축할 단어 사전의 크기\n",
        " #special_tokens는 현재 1개임.\n",
        "vocab = build_vocab(corpus=tokens, n_vocab=5000, special_tokens=[\"<unk>\"])\n",
        "\n",
        "token_to_id = {token: idx for idx, token in enumerate(vocab)} #각 토큰을 해당하는 인덱스(ID)로 매핑\n",
        "id_to_token = {idx: token for idx, token in enumerate(vocab)} #각 인덱스(ID)를 해당하는 토큰으로 매핑\n",
        "\n",
        "print(vocab[:10]) #10개 단어 추출\n",
        "print(len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-30ltZM9rhI",
        "outputId": "0565e64a-29e1-431f-eae3-7185072b6122"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<unk>', '.', '이', '영화', '의', '..', '가', '에', '...', '을']\n",
            "5001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#skip-gram의 단어 쌍 추출\n",
        "def get_word_pairs(tokens, window_size): #window_size는 주변 단어를 몇 개까지 고려할 것인지\n",
        "    pairs = []\n",
        "    for sentence in tokens:\n",
        "        sentence_length = len(sentence)\n",
        "        for idx, center_word in enumerate(sentence): #idx는 현재 단어의 인덱스, center_word는 중심 단어\n",
        "            #현재 단어에서 얼마나 멀리 떨어진 주변 단어를 고려할 것인지 ㄱㄹ정\n",
        "            window_start = max(0, idx - window_size)\n",
        "            window_end = min(sentence_length, idx + window_size + 1)\n",
        "            center_word = sentence[idx]  #idx는 현재 단어의 인덱스, center_word는 중심 단어\n",
        "            context_words = sentence[window_start:idx] + sentence[idx+1:window_end]\n",
        "            for context_word in context_words:\n",
        "                pairs.append([center_word, context_word])\n",
        "    return pairs\n",
        "\n",
        "\n",
        "word_pairs = get_word_pairs(tokens, window_size=2)\n",
        "print(word_pairs[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZ2OQrVG9yOj",
        "outputId": "f0374f94-255e-4182-aa02-f8f0e80d0662"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['굳', 'ㅋ'], ['ㅋ', '굳'], ['뭐', '야'], ['뭐', '이'], ['야', '뭐']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#인덱스 쌍 변환\n",
        "def get_index_pairs(word_pairs, token_to_id): #생성된 단어쌍을 토큰 인덱스 쌍으로 변환\n",
        "# 앞에서 생성한 word_pairs와 ID를 매핑한 디셔너리인 token_to_id로 인덱스 쌍을 생성한다.\n",
        "    pairs = []\n",
        "    unk_index = token_to_id[\"<unk>\"] #단어 사전 내에 없으면 unk토큰 인덱스 반환\n",
        "    for word_pair in word_pairs:\n",
        "        center_word, context_word = word_pair\n",
        "        center_index = token_to_id.get(center_word, unk_index)\n",
        "        context_index = token_to_id.get(context_word, unk_index)\n",
        "        pairs.append([center_index, context_index])\n",
        "    return pairs\n",
        "\n",
        "\n",
        "index_pairs = get_index_pairs(word_pairs, token_to_id)\n",
        "print(index_pairs[:5])\n",
        "print(len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2n0snLQAsVM",
        "outputId": "3b62daa7-fd0a-40aa-e030-c4b95672b53d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[595, 100], [100, 595], [77, 176], [77, 2], [176, 77]]\n",
            "5001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터로더 설정\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "\n",
        "index_pairs = torch.tensor(index_pairs) #get_index_pairs 함수에서 생성된 중심 단어와 주변 단어 토큰의 인덱스 쌍으로 이루는 리스트\n",
        "# 텐서 형식으로 변환\n",
        "center_indexs = index_pairs[:, 0]\n",
        "contenxt_indexs = index_pairs[:, 1]\n",
        "\n",
        "dataset = TensorDataset(center_indexs, contenxt_indexs)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "wLvNBCo6Cxiz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#skip-gram 모델 준비 작업\n",
        "from torch import optim\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "word2vec = VanillaSkipgram(vocab_size=len(token_to_id), embedding_dim=128).to(device)\n",
        "# 전체 단어 집합의 크기를 전달, 임베딩 크기는 128로 할당\n",
        "criterion = nn.CrossEntropyLoss().to(device) #교차 엔트로피 사용\n",
        "optimizer = optim.SGD(word2vec.parameters(), lr=0.1)"
      ],
      "metadata": {
        "id": "PqD31DQMDQ3v"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 학습\n",
        "for epoch in range(10):\n",
        "    cost = 0.0\n",
        "    for input_ids, target_ids in dataloader:\n",
        "        input_ids = input_ids.to(device)\n",
        "        target_ids = target_ids.to(device)\n",
        "\n",
        "        logits = word2vec(input_ids)\n",
        "        loss = criterion(logits, target_ids)\n",
        "\n",
        "        #가중치와 편향을 갱신하기\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        cost += loss\n",
        "\n",
        "    cost = cost / len(dataloader)\n",
        "    print(f\"Epoch : {epoch+1:4d}, Cost : {cost:.3f}\")"
      ],
      "metadata": {
        "id": "O2usYvElZco_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#임베딩 값 추출\n",
        "token_to_embedding = dict()\n",
        "embedding_matrix = word2vec.embedding.weight.detach().cpu().numpy()\n",
        "\n",
        "for word, embedding in zip(vocab, embedding_matrix):\n",
        "    token_to_embedding[word] = embedding\n",
        "\n",
        "index = 30\n",
        "token = vocab[30]\n",
        "token_embedding = token_to_embedding[token] #임베딩 값 추출하기\n",
        "print(token)\n",
        "print(token_embedding)"
      ],
      "metadata": {
        "id": "pgfPZdcQZ6pj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#단어 임베딩 유사도 계산\n",
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "\n",
        "\n",
        "def cosine_similarity(a, b):\n",
        "    cosine = np.dot(b, a) / (norm(b, axis=1) * norm(a)) #a는 임베딩 토큰, b는 임베딩 행렬을 의미\n",
        "    return cosine\n",
        "\n",
        "def top_n_index(cosine_matrix, n): #유사도 행렬을 내림차순으로 정렬해 어떤 단어가 가장 가까운 단어인지 반환한다 .\n",
        "    closest_indexes = cosine_matrix.argsort()[::-1]\n",
        "    top_n = closest_indexes[1 : n + 1]\n",
        "    return top_n\n",
        "\n",
        "\n",
        "cosine_matrix = cosine_similarity(token_embedding, embedding_matrix)\n",
        "top_n = top_n_index(cosine_matrix, n=5)\n",
        "\n",
        "print(f\"{token}와 가장 유사한 5 개 단어\")\n",
        "for index in top_n:\n",
        "    print(f\"{id_to_token[index]} - 유사도 : {cosine_matrix[index]:.4f}\")"
      ],
      "metadata": {
        "id": "DO1jrTDabP9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gensim 모델 실습"
      ],
      "metadata": {
        "id": "gs4DWd8_lC92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gensim"
      ],
      "metadata": {
        "id": "9QhK8ZURWdvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install korpora"
      ],
      "metadata": {
        "id": "njL1NXNZZ3qQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "apt-get update\n",
        "apt-get install g++ openjdk-8-jdk python-dev python3-dev\n",
        "pip3 install JPype1\n",
        "pip3 install konlpy"
      ],
      "metadata": {
        "id": "b1No2rehaHfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Korpora 사용하기\n",
        "import pandas as pd\n",
        "from Korpora import Korpora\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "\n",
        "corpus = Korpora.load(\"nsmc\")\n",
        "corpus = pd.DataFrame(corpus.test)"
      ],
      "metadata": {
        "id": "OZWn4auYZurA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#okt토크나이저 사용하기\n",
        "tokenizer = Okt()\n",
        "tokens = [tokenizer.morphs(review) for review in corpus.text]"
      ],
      "metadata": {
        "id": "zEEO9I7LZ936"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#모델학습\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "\n",
        "word2vec = Word2Vec(\n",
        "    sentences=tokens, #입력문장으로 토큰 리스트로 표현된다.\n",
        "    vector_size=128, #벡터의 크기\n",
        "    window=5, #윈독가 5이니 5만큼 떨어진 단어까지 주변 단어로 고려해서 보겠다\n",
        "    min_count=1, #최소빈도는 고려하지 않는다.\n",
        "    sg=1,#skip-gram을 사용한다.\n",
        "    epochs=3, #학습 에폭의 수를 3으로 두겠다\n",
        "    max_final_vocab=10000 #최대 단어의 사전 크기\n",
        ")"
      ],
      "metadata": {
        "id": "HWReaMuoaJ0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#임베딩 추출 및 유사도 계산하기\n",
        "ord = \"연기\"\n",
        "print(word2vec.wv[word])\n",
        "print(word2vec.wv.most_similar(word, topn=5)) #주어진 단어에 대해 가장 유사한 단어를 찾는다.\n",
        "print(word2vec.wv.similarity(w1=word, w2=\"연기력\")) #두 단어 간의 유사도를 계산한다."
      ],
      "metadata": {
        "id": "S99ff9F-bB65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# fastText 모델 실습"
      ],
      "metadata": {
        "id": "MUUMSHV5opdn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#KorNLI 데이터세트 전처리\n",
        "from Korpora import Korpora\n",
        "\n",
        "\n",
        "corpus = Korpora.load(\"kornli\")\n",
        "corpus_texts = corpus.get_all_texts() + corpus.get_all_pairs()\n",
        "tokens = [sentence.split() for sentence in corpus_texts]\n",
        "\n",
        "print(tokens[:3])"
      ],
      "metadata": {
        "id": "apKmspKLbvjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 실습\n",
        "from gensim.models import FastText\n",
        "\n",
        "\n",
        "fastText = FastText(\n",
        "    sentences=tokens,\n",
        "    vector_size=128,\n",
        "    window=5,\n",
        "    min_count=5,\n",
        "    sg=1,\n",
        "    epochs=3,\n",
        "    min_n=2, #n-gram 범위를 [2,6]으로 설정했다.\n",
        "    max_n=6\n",
        ")\n",
        "\n",
        "# fastText.save(\"../models/fastText.model\")\n",
        "# fastText = FastText.load(\"../models/fastText.model\")"
      ],
      "metadata": {
        "id": "8U0QozgYqkXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oov_token = \"사랑해요\"\n",
        "oov_vector = fastText.wv[oov_token]\n",
        "\n",
        "print(oov_token in fastText.wv.index_to_key) #학습된 단어 사전을 나타내는 리스트인데 oov니까 false로 출력됨.\n",
        "print(fastText.wv.most_similar(oov_vector, topn=5))"
      ],
      "metadata": {
        "id": "wNUyDEZ8qzYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 순환 신경망"
      ],
      "metadata": {
        "id": "YG5W04mvlB4N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#양방향 다층 신경망\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "input_size = 128\n",
        "ouput_size = 256\n",
        "num_layers = 3\n",
        "bidirectional = True\n",
        "\n",
        "model = nn.RNN(\n",
        "    input_size=input_size,\n",
        "    hidden_size=ouput_size,\n",
        "    num_layers=num_layers,\n",
        "    nonlinearity=\"tanh\",\n",
        "    batch_first=True,\n",
        "    bidirectional=bidirectional,\n",
        ")\n",
        "\n",
        "batch_size = 4\n",
        "sequence_len = 6\n",
        "\n",
        "inputs = torch.randn(batch_size, sequence_len, input_size) #입력값 차원은 [배치크기, 시퀀스 크기, 입력 특성 크기]\n",
        "h_0 = torch.rand(num_layers * (int(bidirectional) + 1), batch_size, ouput_size) #은닉 상태는 [계층수*양방향 여부+1, 배치크기, 은닉 상태크기]\n",
        "\n",
        "outputs, hidden = model(inputs, h_0) #입력값과 초기 은닉 상태로 순방향 연산을 수행해 출력값과 최종 은닉 상태를 반환\n",
        "print(outputs.shape)\n",
        "print(hidden.shape)"
      ],
      "metadata": {
        "id": "T4Q9Rm4_saKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 장기 메모리"
      ],
      "metadata": {
        "id": "nQDryKyQCBZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#양방향 다층 장단기 메모리\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "input_size = 128\n",
        "ouput_size = 256\n",
        "num_layers = 3\n",
        "bidirectional = True\n",
        "proj_size = 64\n",
        "\n",
        "model = nn.LSTM(\n",
        "    input_size=input_size,\n",
        "    hidden_size=ouput_size,\n",
        "    num_layers=num_layers,\n",
        "    batch_first=True,\n",
        "    bidirectional=bidirectional,\n",
        "    proj_size=proj_size,\n",
        ")\n",
        "\n",
        "batch_size = 4\n",
        "sequence_len = 6\n",
        "\n",
        "inputs = torch.randn(batch_size, sequence_len, input_size) #입력값\n",
        "h_0 = torch.rand( #초기 은닉 상태\n",
        "    num_layers * (int(bidirectional) + 1),\n",
        "    batch_size,\n",
        "    proj_size if proj_size > 0 else ouput_size,\n",
        ")\n",
        "#초기 메모리 셀 상태\n",
        "c_0 = torch.rand(num_layers * (int(bidirectional) + 1), batch_size, ouput_size)\n",
        "\n",
        "outputs, (h_n, c_n) = model(inputs, (h_0, c_0))\n",
        "\n",
        "print(outputs.shape) #출력값\n",
        "print(h_n.shape) #최종 은닉 상태\n",
        "print(c_n.shape) #최종 메모리 셀 상태"
      ],
      "metadata": {
        "id": "M9TsMbK29rQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 순환 신경망/장단기 신경망 모델실습 1) 무작위 값으로 할당된 임베딩 계층으로 모델 구성"
      ],
      "metadata": {
        "id": "1PQDVN9hCsxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#문장 분류 모델\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class SentenceClassifier(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_vocab, #단어 사전 크기\n",
        "        hidden_dim,\n",
        "        embedding_dim,\n",
        "        n_layers,\n",
        "        dropout=0.5,\n",
        "        bidirectional=True,\n",
        "        model_type=\"lstm\" #모델 종류는 장단기 메모리 사용\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=n_vocab,\n",
        "            embedding_dim=embedding_dim,\n",
        "            padding_idx=0\n",
        "        )\n",
        "        if model_type == \"rnn\": #순환신경망\n",
        "            self.model = nn.RNN(\n",
        "                input_size=embedding_dim,\n",
        "                hidden_size=hidden_dim,\n",
        "                num_layers=n_layers,\n",
        "                bidirectional=bidirectional,\n",
        "                dropout=dropout,\n",
        "                batch_first=True,\n",
        "            )\n",
        "        elif model_type == \"lstm\": #장기메모리라면?\n",
        "            self.model = nn.LSTM(\n",
        "                input_size=embedding_dim,\n",
        "                hidden_size=hidden_dim,\n",
        "                num_layers=n_layers,\n",
        "                bidirectional=bidirectional,\n",
        "                dropout=dropout,\n",
        "                batch_first=True,\n",
        "            )\n",
        "\n",
        "        if bidirectional:\n",
        "            self.classifier = nn.Linear(hidden_dim * 2, 1)\n",
        "        else:\n",
        "            self.classifier = nn.Linear(hidden_dim, 1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeddings = self.embedding(inputs)\n",
        "        output, _ = self.model(embeddings)\n",
        "        last_output = output[:, -1, :]\n",
        "        last_output = self.dropout(last_output)\n",
        "        logits = self.classifier(last_output)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "GgVOIcZlCxBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터세트 불러오기\n",
        "import pandas as pd\n",
        "from Korpora import Korpora\n",
        "\n",
        "\n",
        "corpus = Korpora.load(\"nsmc\")\n",
        "corpus_df = pd.DataFrame(corpus.test)"
      ],
      "metadata": {
        "id": "cv71n238ES6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = corpus_df.sample(frac=0.9, random_state=42) #테스트데이터 불러오기\n",
        "test = corpus_df.drop(train.index)\n",
        "\n",
        "print(train.head(5).to_markdown())\n",
        "print(\"Training Data Size :\", len(train))\n",
        "print(\"Testing Data Size :\", len(test))"
      ],
      "metadata": {
        "id": "t_ardv9_EWpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터 토큰화 및 단어 사전 구축\n",
        "from konlpy.tag import Okt\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "def build_vocab(corpus, n_vocab, special_tokens): #문장의 길이를 맞추기 위해 스페셜토큰 추가\n",
        "    counter = Counter()\n",
        "    for tokens in corpus:\n",
        "        counter.update(tokens)\n",
        "    vocab = special_tokens\n",
        "    for token, count in counter.most_common(n_vocab):\n",
        "        vocab.append(token)\n",
        "    return vocab\n",
        "\n",
        "\n",
        "tokenizer = Okt()\n",
        "train_tokens = [tokenizer.morphs(review) for review in train.text]\n",
        "test_tokens = [tokenizer.morphs(review) for review in test.text]\n",
        "\n",
        "vocab = build_vocab(corpus=train_tokens, n_vocab=5000, special_tokens=[\"<pad>\", \"<unk>\"])\n",
        "token_to_id = {token: idx for idx, token in enumerate(vocab)}\n",
        "id_to_token = {idx: token for idx, token in enumerate(vocab)}\n",
        "\n",
        "print(vocab[:10])\n",
        "print(len(vocab))"
      ],
      "metadata": {
        "id": "wwZWWWEzEpu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#정수 인코딩 및 패딩\n",
        "import numpy as np\n",
        "\n",
        "def pad_sequences(sequences, max_length, pad_value):\n",
        "    result = list()\n",
        "    for sequence in sequences:\n",
        "        sequence = sequence[:max_length]\n",
        "        pad_length = max_length - len(sequence)\n",
        "        padded_sequence = sequence + [pad_value] * pad_length\n",
        "        result.append(padded_sequence)\n",
        "    return np.asarray(result)\n",
        "\n",
        "\n",
        "unk_id = token_to_id[\"<unk>\"]\n",
        "train_ids = [\n",
        "    [token_to_id.get(token, unk_id) for token in review] for review in train_tokens\n",
        "]\n",
        "test_ids = [\n",
        "    [token_to_id.get(token, unk_id) for token in review] for review in test_tokens\n",
        "]\n",
        "\n",
        "max_length = 32 #입력 텍스트의 길이를 고려해 결정한다.\n",
        "pad_id = token_to_id[\"<pad>\"] #너무 긴 문장은 최대길이로 줄이기\n",
        "train_ids = pad_sequences(train_ids, max_length, pad_id) #시퀀스를 최대 길이로 잘라내게함.\n",
        "test_ids = pad_sequences(test_ids, max_length, pad_id)\n",
        "\n",
        "print(train_ids[0])\n",
        "print(test_ids[0])"
      ],
      "metadata": {
        "id": "YgS79YXUEsga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터로더 적용\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "\n",
        "train_ids = torch.tensor(train_ids)\n",
        "test_ids = torch.tensor(test_ids)\n",
        "\n",
        "train_labels = torch.tensor(train.label.values, dtype=torch.float32)\n",
        "test_labels = torch.tensor(test.label.values, dtype=torch.float32)\n",
        "\n",
        "train_dataset = TensorDataset(train_ids, train_labels) #정수 인코딩과 라벨값을 파이토치 텐서 형태로 변환한다.\n",
        "test_dataset = TensorDataset(test_ids, test_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "id": "3ifO5ccuFaJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "\n",
        "n_vocab = len(token_to_id)\n",
        "hidden_dim = 64 #은닉상태 크기\n",
        "embedding_dim = 128 #임베딩 벡터의 크기\n",
        "n_layers = 2 # -> 신경망은 두개의 층으로 구성된다!!\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "classifier = SentenceClassifier(\n",
        "    n_vocab=n_vocab, hidden_dim=hidden_dim, embedding_dim=embedding_dim, n_layers=n_layers\n",
        ").to(device)\n",
        "criterion = nn.BCEWithLogitsLoss().to(device) #손실함수는 이진 교차엔트로피 함수 적용\n",
        "optimizer = optim.RMSprop(classifier.parameters(), lr=0.001) #최적화 함수는 지수 가중 이동 평균을 사용한다."
      ],
      "metadata": {
        "id": "5IHmylMxFxsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 학습 및 테스트\n",
        "def train(model, datasets, criterion, optimizer, device, interval):\n",
        "    model.train()\n",
        "    losses = list()\n",
        "\n",
        "    for step, (input_ids, labels) in enumerate(datasets):\n",
        "        input_ids = input_ids.to(device)\n",
        "        labels = labels.to(device).unsqueeze(1)\n",
        "\n",
        "        logits = model(input_ids)\n",
        "        loss = criterion(logits, labels)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % interval == 0:\n",
        "            print(f\"Train Loss {step} : {np.mean(losses)}\")\n",
        "\n",
        "\n",
        "def test(model, datasets, criterion, device):\n",
        "    model.eval()\n",
        "    losses = list() #검증 손실\n",
        "    corrects = list() #검증 정확도\n",
        "\n",
        "    for step, (input_ids, labels) in enumerate(datasets):\n",
        "        input_ids = input_ids.to(device)\n",
        "        labels = labels.to(device).unsqueeze(1)\n",
        "\n",
        "        logits = model(input_ids)\n",
        "        loss = criterion(logits, labels)\n",
        "        losses.append(loss.item())\n",
        "        yhat = torch.sigmoid(logits)>.5\n",
        "        corrects.extend(\n",
        "            torch.eq(yhat, labels).cpu().tolist()\n",
        "        )\n",
        "\n",
        "    print(f\"Val Loss : {np.mean(losses)}, Val Accuracy : {np.mean(corrects)}\")\n",
        "    # 검증손실과 검증 정확도를 확인한다.\n",
        "\n",
        "\n",
        "epochs = 5\n",
        "interval = 500\n",
        "\n",
        "for epoch in range(epochs): #에폭마다 테스트 데이터 세트로\n",
        "    train(classifier, train_loader, criterion, optimizer, device, interval)\n",
        "    test(classifier, test_loader, criterion, device)"
      ],
      "metadata": {
        "id": "flHqsS-mHtb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습된 모델로부터 임베딩 추출\n",
        "token_to_embedding = dict()\n",
        "embedding_matrix = classifier.embedding.weight.detach().cpu().numpy()\n",
        "\n",
        "for word, emb in zip(vocab, embedding_matrix):\n",
        "    token_to_embedding[word] = emb\n",
        "\n",
        "token = vocab[1000]\n",
        "print(token, token_to_embedding[token])"
      ],
      "metadata": {
        "id": "2pzGDgHfJTkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) 사전에 학습된 모델로 학습"
      ],
      "metadata": {
        "id": "pd04WcVCJ5W1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#임베딩 계층 초기화\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Word2Vec 모델 불러오기\n",
        "word2vec = Word2Vec.load(\"word2vec.model\")\n",
        "\n",
        "# 필요한 변수들을 정의합니다. 'n_vocab'은 어휘의 총 수, 'embedding_dim'은 임베딩 차원을 의미합니다.\n",
        "n_vocab = len(id_to_token)  # id_to_token은 {id: token} 형식의 사전이어야 합니다.\n",
        "embedding_dim = word2vec.vector_size  # Word2Vec 모델의 벡터 크기를 사용\n",
        "\n",
        "# 임베딩 행렬 초기화\n",
        "init_embeddings = np.zeros((n_vocab, embedding_dim))\n",
        "\n",
        "# 임베딩 행렬 채우기\n",
        "for index, token in id_to_token.items():\n",
        "    if token in word2vec.wv:  # token이 word2vec 모델에 있으면 그 벡터를 사용\n",
        "        init_embeddings[index] = word2vec.wv[token]\n",
        "    else:\n",
        "        # Word2Vec 모델에 토큰이 없는 경우 (예: \"<pad>\", \"<unk>\")\n",
        "        # 여기서는 제로 벡터로 두거나 무작위로 초기화할 수 있습니다.\n",
        "        init_embeddings[index] = np.random.normal(size=(embedding_dim,))\n",
        "\n",
        "# PyTorch 임베딩 레이어 생성\n",
        "embedding_layer = nn.Embedding.from_pretrained(\n",
        "    torch.tensor(init_embeddings, dtype=torch.float32),\n",
        "    freeze=False  # 필요에 따라 가중치를 동결하거나 동결 해제\n",
        ")"
      ],
      "metadata": {
        "id": "dpSYxSP-J39j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#사전 학습된 임베딩 계층 적용\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class SentenceClassifier(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_vocab,\n",
        "        hidden_dim,\n",
        "        embedding_dim,\n",
        "        n_layers,\n",
        "        dropout=0.5,\n",
        "        bidirectional=True,\n",
        "        model_type=\"lstm\",\n",
        "        pretrained_embedding=None #사전 학습된 임베딩 매개변수가 None이 아니라면 전달된 값을 임베딩 계층으로 초기화한다.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if pretrained_embedding is not None:\n",
        "            self.embedding = nn.Embedding.from_pretrained(\n",
        "                torch.tensor(pretrained_embedding, dtype=torch.float32)\n",
        "            )\n",
        "        else:\n",
        "            self.embedding = nn.Embedding(\n",
        "                num_embeddings=n_vocab,\n",
        "                embedding_dim=embedding_dim,\n",
        "                padding_idx=0\n",
        "            )\n",
        "\n",
        "        if model_type == \"rnn\":\n",
        "            self.model = nn.RNN(\n",
        "                input_size=embedding_dim,\n",
        "                hidden_size=hidden_dim,\n",
        "                num_layers=n_layers,\n",
        "                bidirectional=bidirectional,\n",
        "                dropout=dropout,\n",
        "                batch_first=True,\n",
        "            )\n",
        "        elif model_type == \"lstm\":\n",
        "            self.model = nn.LSTM(\n",
        "                input_size=embedding_dim,\n",
        "                hidden_size=hidden_dim,\n",
        "                num_layers=n_layers,\n",
        "                bidirectional=bidirectional,\n",
        "                dropout=dropout,\n",
        "                batch_first=True,\n",
        "            )\n",
        "\n",
        "        if bidirectional:\n",
        "            self.classifier = nn.Linear(hidden_dim * 2, 1)\n",
        "        else:\n",
        "            self.classifier = nn.Linear(hidden_dim, 1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeddings = self.embedding(inputs)\n",
        "        output, _ = self.model(embeddings)\n",
        "        last_output = output[:, -1, :]\n",
        "        last_output = self.dropout(last_output)\n",
        "        logits = self.classifier(last_output)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "PQ5pvr7-Kepe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 학습\n",
        "from torch import optim\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "classifier = SentenceClassifier(\n",
        "    n_vocab=n_vocab, hidden_dim=hidden_dim, embedding_dim=embedding_dim,\n",
        "n_layers=n_layers, pretrained_embedding=init_embeddings\n",
        ").to(device)\n",
        "criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "optimizer = optim.RMSprop(classifier.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 5\n",
        "interval = 500\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train(classifier, train_loader, criterion, optimizer, device, interval)\n",
        "    test(classifier, test_loader, criterion, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "cZIBzJkfLE69",
        "outputId": "3e0cf0a3-f4b3-41ec-b851-279f97c4e4fd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'n_vocab' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-5e6a4015b289>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m classifier = SentenceClassifier(\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mn_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_embedding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m ).to(device)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'n_vocab' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 합성곱"
      ],
      "metadata": {
        "id": "qNspjhoJsGNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 합성곱 모델\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Sequential( #선형 변환으로 구성\n",
        "            nn.Conv2d(\n",
        "                in_channels=3, out_channels=16, kernel_size=3, stride=2, padding=1\n",
        "            ),\n",
        "            nn.ReLU(), #렐루 함수 사용\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(32 * 32 * 32, 10) #완전 연결 계층의 입력 데이터 차원의 크기는\n",
        "        # 마지막 특징 맵의 높이 X 너비 X 채널로 계산된다.\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = torch.flatten(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "fVK2eroXsLqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 합성곱 기반 문장 분류 모델 정의\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class SentenceClassifier(nn.Module):\n",
        "    def __init__(self, pretrained_embedding, filter_sizes, max_length, dropout=0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding.from_pretrained(\n",
        "            torch.tensor(pretrained_embedding, dtype=torch.float32)\n",
        "        )\n",
        "        embedding_dim = self.embedding.weight.shape[1]\n",
        "\n",
        "        conv = []\n",
        "        for size in filter_sizes:\n",
        "            conv.append(\n",
        "                nn.Sequential(\n",
        "                    nn.Conv1d( #1차원 합성곱 계층\n",
        "                        in_channels=embedding_dim,\n",
        "                        out_channels=1,  #출력 채널 수 1\n",
        "                        kernel_size=size\n",
        "                    ),\n",
        "                    nn.ReLU(), #렐루함수 사용\n",
        "                    nn.MaxPool1d(kernel_size=max_length-size-1), #최댓값 플링 구성 ': 합성곱 연산의 영향을 받았으므로\n",
        "                    #최대길이 - 필터크기 - 1로 설정\n",
        "                )\n",
        "            )\n",
        "        self.conv_filters = nn.ModuleList(conv) #여러 개의 서브모듈을 리스트 형태로 저장하는 기능 제공\n",
        "\n",
        "        output_size = len(filter_sizes)\n",
        "        self.pre_classifier = nn.Linear(output_size, output_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.classifier = nn.Linear(output_size, 1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeddings = self.embedding(inputs)\n",
        "        embeddings = embeddings.permute(0, 2, 1) #permute 메서드를 이용해 차원을 2에서 1로 변경하고 여러 개의 합성곱 필터에 통과시킴\n",
        "\n",
        "        conv_outputs = [conv(embeddings) for conv in self.conv_filters]\n",
        "        concat_outputs = torch.cat([conv.squeeze(-1) for conv in conv_outputs], dim=1)\n",
        "\n",
        "        logits = self.pre_classifier(concat_outputs)\n",
        "        logits = self.dropout(logits)\n",
        "        logits = self.classifier(logits)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "ExjkWwN6zhIy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install korpora"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tc7eDr735x3L",
        "outputId": "9898948e-ce00-4a73-d6ae-a4bc7461e989"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: korpora in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: dataclasses>=0.6 in /usr/local/lib/python3.10/dist-packages (from korpora) (0.6)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from korpora) (1.26.4)\n",
            "Requirement already satisfied: tqdm>=4.46.0 in /usr/local/lib/python3.10/dist-packages (from korpora) (4.66.5)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from korpora) (2.32.3)\n",
            "Requirement already satisfied: xlrd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from korpora) (2.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->korpora) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->korpora) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->korpora) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->korpora) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from Korpora import Korpora\n",
        "\n",
        "\n",
        "corpus = Korpora.load(\"nsmc\")\n",
        "corpus_df = pd.DataFrame(corpus.test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CETjWs035ODE",
        "outputId": "7d661f5a-9d7b-482c-8551-81d8325e8ba7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
            "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
            "\n",
            "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
            "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
            "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
            "\n",
            "    # Description\n",
            "    Author : e9t@github\n",
            "    Repository : https://github.com/e9t/nsmc\n",
            "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
            "\n",
            "    Naver sentiment movie corpus v1.0\n",
            "    This is a movie review dataset in the Korean language.\n",
            "    Reviews were scraped from Naver Movies.\n",
            "\n",
            "    The dataset construction is based on the method noted in\n",
            "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
            "\n",
            "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
            "\n",
            "    # License\n",
            "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
            "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
            "\n",
            "[Korpora] Corpus `nsmc` is already installed at /root/Korpora/nsmc/ratings_train.txt\n",
            "[Korpora] Corpus `nsmc` is already installed at /root/Korpora/nsmc/ratings_test.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = corpus_df.sample(frac=0.9, random_state=42)\n",
        "test = corpus_df.drop(train.index)\n",
        "\n",
        "print(train.head(5).to_markdown())\n",
        "print(\"Training Data Size :\", len(train))\n",
        "print(\"Testing Data Size :\", len(test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyy6vted5p1k",
        "outputId": "dad464ee-7732-4634-d595-1453a1404322"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|       | text                                                                                     |   label |\n",
            "|------:|:-----------------------------------------------------------------------------------------|--------:|\n",
            "| 33553 | 모든 편견을 날려 버리는 가슴 따뜻한 영화. 로버트 드 니로, 필립 세이모어 호프만 영원하라. |       1 |\n",
            "|  9427 | 무한 리메이크의 소재. 감독의 역량은 항상 그 자리에...                                    |       0 |\n",
            "|   199 | 신날 것 없는 애니.                                                                       |       0 |\n",
            "| 12447 | 잔잔 격동                                                                                |       1 |\n",
            "| 39489 | 오랜만에 찾은 주말의 명화의 보석                                                         |       1 |\n",
            "Training Data Size : 45000\n",
            "Testing Data Size : 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "apt-get update\n",
        "apt-get install g++ openjdk-8-jdk python-dev python3-dev\n",
        "pip3 install JPype1\n",
        "pip3 install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9apOLAt6XMy",
        "outputId": "03237a4e-5c77-4944-eee5-56ca712f956f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:5 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Ign:10 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:11 https://r2u.stat.illinois.edu/ubuntu jammy Release\n",
            "Fetched 128 kB in 2s (80.9 kB/s)\n",
            "Reading package lists...\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "Package python-dev is not available, but is referred to by another package.\n",
            "This may mean that the package is missing, has been obsoleted, or\n",
            "is only available from another source\n",
            "However the following packages replace it:\n",
            "  python2-dev python2 python-dev-is-python3\n",
            "\n",
            "Requirement already satisfied: JPype1 in /usr/local/lib/python3.10/dist-packages (1.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1) (24.1)\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.5.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.4)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (24.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "E: Package 'python-dev' has no installation candidate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "def build_vocab(corpus, n_vocab, special_tokens):\n",
        "    counter = Counter()\n",
        "    for tokens in corpus:\n",
        "        counter.update(tokens)\n",
        "    vocab = special_tokens\n",
        "    for token, count in counter.most_common(n_vocab):\n",
        "        vocab.append(token)\n",
        "    return vocab\n",
        "\n",
        "\n",
        "tokenizer = Okt()\n",
        "train_tokens = [tokenizer.morphs(review) for review in train.text]\n",
        "test_tokens = [tokenizer.morphs(review) for review in test.text]\n",
        "\n",
        "vocab = build_vocab(corpus=train_tokens, n_vocab=5000, special_tokens=[\"<pad>\", \"<unk>\"])\n",
        "token_to_id = {token: idx for idx, token in enumerate(vocab)}\n",
        "id_to_token = {idx: token for idx, token in enumerate(vocab)}\n",
        "\n",
        "print(vocab[:10])\n",
        "print(len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vxz6-OPs5-Qh",
        "outputId": "eb0c739d-10e0-41c0-f995-538c9e158c16"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<pad>', '<unk>', '.', '이', '영화', '의', '..', '가', '에', '...']\n",
            "5002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def pad_sequences(sequences, max_length, pad_value):\n",
        "    result = list()\n",
        "    for sequence in sequences:\n",
        "        sequence = sequence[:max_length]\n",
        "        pad_length = max_length - len(sequence)\n",
        "        padded_sequence = sequence + [pad_value] * pad_length\n",
        "        result.append(padded_sequence)\n",
        "    return np.asarray(result)\n",
        "\n",
        "\n",
        "unk_id = token_to_id[\"<unk>\"]\n",
        "train_ids = [\n",
        "    [token_to_id.get(token, unk_id) for token in review] for review in train_tokens\n",
        "]\n",
        "test_ids = [\n",
        "    [token_to_id.get(token, unk_id) for token in review] for review in test_tokens\n",
        "]\n",
        "\n",
        "max_length = 32\n",
        "pad_id = token_to_id[\"<pad>\"]\n",
        "train_ids = pad_sequences(train_ids, max_length, pad_id)\n",
        "test_ids = pad_sequences(test_ids, max_length, pad_id)\n",
        "\n",
        "print(train_ids[0])\n",
        "print(test_ids[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qN6iMkPc6a_n",
        "outputId": "021918db-55b5-465d-bd6a-31d181ba15df"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 223 1716   10 4036 2095  193  755    4    2 2330 1031  220   26   13\n",
            " 4839    1    1    1    2    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0]\n",
            "[3307    5 1997  456    8    1 1013 3906    5    1    1   13  223   51\n",
            "    3    1 4684    6    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "\n",
        "train_ids = torch.tensor(train_ids)\n",
        "test_ids = torch.tensor(test_ids)\n",
        "\n",
        "train_labels = torch.tensor(train.label.values, dtype=torch.float32)\n",
        "test_labels = torch.tensor(test.label.values, dtype=torch.float32)\n",
        "\n",
        "train_dataset = TensorDataset(train_ids, train_labels)\n",
        "test_dataset = TensorDataset(test_ids, test_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "id": "CYnQp6vB7i5V"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "filter_sizes = [3, 3, 4, 4, 5, 5]\n",
        "classifier = SentenceClassifier(\n",
        "    pretrained_embedding=init_embeddings,\n",
        "    filter_sizes=filter_sizes,\n",
        "    max_length=max_length\n",
        ").to(device)\n",
        "criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "optimizer = optim.RMSprop(classifier.parameters(), lr=0.001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "KZcv49Vo7pbO",
        "outputId": "27ba18aa-382d-42e2-d703-fa498ea96a77"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'init_embeddings' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-302dadae9266>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfilter_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m classifier = SentenceClassifier(\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mpretrained_embedding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mfilter_sizes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilter_sizes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'init_embeddings' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, datasets, criterion, optimizer, device, interval):\n",
        "    model.train()\n",
        "    losses = list()\n",
        "\n",
        "    for step, (input_ids, labels) in enumerate(datasets):\n",
        "        input_ids = input_ids.to(device)\n",
        "        labels = labels.to(device).unsqueeze(1)\n",
        "\n",
        "        logits = model(input_ids)\n",
        "        loss = criterion(logits, labels)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % interval == 0:\n",
        "            print(f\"Train Loss {step} : {np.mean(losses)}\")\n",
        "\n",
        "\n",
        "def test(model, datasets, criterion, device):\n",
        "    model.eval()\n",
        "    losses = list()\n",
        "    corrects = list()\n",
        "\n",
        "    for step, (input_ids, labels) in enumerate(datasets):\n",
        "        input_ids = input_ids.to(device)\n",
        "        labels = labels.to(device).unsqueeze(1)\n",
        "\n",
        "        logits = model(input_ids)\n",
        "        loss = criterion(logits, labels)\n",
        "        losses.append(loss.item())\n",
        "        yhat = torch.sigmoid(logits)>.5\n",
        "        corrects.extend(\n",
        "            torch.eq(yhat, labels).cpu().tolist()\n",
        "        )\n",
        "\n",
        "    print(f\"Val Loss : {np.mean(losses)}, Val Accuracy : {np.mean(corrects)}\")\n",
        "\n",
        "\n",
        "epochs = 5\n",
        "interval = 500\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train(classifier, train_loader, criterion, optimizer, device, interval)\n",
        "    test(classifier, test_loader, criterion, device)"
      ],
      "metadata": {
        "id": "eqxK8MoX625o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "filter_sizes = [3, 3, 4, 4, 5, 5]\n",
        "classifier = SentenceClassifier(\n",
        "    pretrained_embedding=init_embeddings,\n",
        "    filter_sizes=filter_sizes,\n",
        "    max_length=max_length\n",
        ").to(device)\n",
        "criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "optimizer = optim.RMSprop(classifier.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "RkdmwTWA6LWO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}