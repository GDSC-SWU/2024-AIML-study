{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 6.01 N-gram\n",
        "\n"
      ],
      "metadata": {
        "id": "x-DI5UDoIVsn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OySV3mhOIBC9",
        "outputId": "b5dd2071-d90d-40e5-a20c-be5f93932978"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('안녕',), ('오랜만이야!',), ('만나서',), ('반가워~',)]\n",
            "[('안녕', '오랜만이야!'), ('오랜만이야!', '만나서'), ('만나서', '반가워~')]\n",
            "[('안녕', '오랜만이야!', '만나서'), ('오랜만이야!', '만나서', '반가워~')]\n",
            "[('안녕',), ('오랜만이야!',), ('만나서',), ('반가워~',)]\n",
            "[('안녕', '오랜만이야!'), ('오랜만이야!', '만나서'), ('만나서', '반가워~')]\n",
            "[('안녕', '오랜만이야!', '만나서'), ('오랜만이야!', '만나서', '반가워~')]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "\n",
        "# 이 함수는 문장을 입력받아 n-그램을 생성하는 함수임\n",
        "def ngrams(sentence, n):\n",
        "    # 문장을 공백을 기준으로 나누어 단어 리스트로 변환함\n",
        "    words = sentence.split()\n",
        "    # 주어진 n값에 따라 단어들을 묶어 n-그램 리스트를 생성함\n",
        "    ngrams = zip(*[words[i:] for i in range(n)])\n",
        "    return list(ngrams)\n",
        "\n",
        "sentence = \"안녕 오랜만이야! 만나서 반가워~\"\n",
        "\n",
        "# 각 n에 따라 n-그램을 생성해봄\n",
        "unigram = ngrams(sentence, 1)  # 유니그램\n",
        "bigram = ngrams(sentence, 2)   # 바이그램\n",
        "trigram = ngrams(sentence, 3)  # 트라이그램\n",
        "\n",
        "# 생성된 n-그램을 출력함\n",
        "print(unigram)\n",
        "print(bigram)\n",
        "print(trigram)\n",
        "\n",
        "# NLTK 라이브러리를 활용하여 n-그램을 생성해봄\n",
        "unigram = nltk.ngrams(sentence.split(), 1)\n",
        "bigram = nltk.ngrams(sentence.split(), 2)\n",
        "trigram = nltk.ngrams(sentence.split(), 3)\n",
        "\n",
        "# NLTK로 생성한 n-그램을 출력함\n",
        "print(list(unigram))\n",
        "print(list(bigram))\n",
        "print(list(trigram))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.02 TF-IDF 계산"
      ],
      "metadata": {
        "id": "L0ulBUn6IwqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "corpus = [\n",
        "    \"I like to play football\",\n",
        "    \"My favorite sport is football\",\n",
        "    \"I don’t like to play basketball\",\n",
        "    \"My favorite sport is basketball\"\n",
        "]\n",
        "\n",
        "# TF-IDF 벡터라이저를 초기화\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# 코퍼스를 바탕으로 단어 사전을 학습함\n",
        "tfidf_vectorizer.fit(corpus)\n",
        "\n",
        "# 학습된 벡터라이저로 코퍼스를 변환하여 TF-IDF 행렬을 만듦\n",
        "tfidf_matrix = tfidf_vectorizer.transform(corpus)\n",
        "\n",
        "# TF-IDF 행렬을 배열로 변환하여 출력함\n",
        "print(tfidf_matrix.toarray())\n",
        "\n",
        "# TF-IDF 벡터라이저에 저장된 단어 사전 출력함\n",
        "print(tfidf_vectorizer.vocabulary_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omx0kY0FIPWG",
        "outputId": "7c65398a-8088-4f04-cba0-4c7709fd6728"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.         0.         0.5        0.         0.5\n",
            "  0.         0.5        0.         0.5       ]\n",
            " [0.         0.         0.4472136  0.4472136  0.4472136  0.\n",
            "  0.4472136  0.         0.4472136  0.        ]\n",
            " [0.4222466  0.53556627 0.         0.         0.         0.4222466\n",
            "  0.         0.4222466  0.         0.4222466 ]\n",
            " [0.4472136  0.         0.4472136  0.         0.4472136  0.\n",
            "  0.4472136  0.         0.4472136  0.        ]]\n",
            "{'like': 5, 'to': 9, 'play': 7, 'football': 3, 'my': 6, 'favorite': 2, 'sport': 8, 'is': 4, 'don': 1, 'basketball': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.03~6.12 Skip-gram 모델 실습"
      ],
      "metadata": {
        "id": "3q5qQKOsJP3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 필수 라이브러리 설치\n",
        "!pip install torch pandas Korpora konlpy numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rbt8JNpoJ_Vg",
        "outputId": "f072aacc-52a8-40ee-fe65-bfb6f2c6a710"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: Korpora in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: dataclasses>=0.6 in /usr/local/lib/python3.10/dist-packages (from Korpora) (0.6)\n",
            "Requirement already satisfied: tqdm>=4.46.0 in /usr/local/lib/python3.10/dist-packages (from Korpora) (4.66.5)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from Korpora) (2.32.3)\n",
            "Requirement already satisfied: xlrd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from Korpora) (2.0.1)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.5.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (24.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->Korpora) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->Korpora) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->Korpora) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->Korpora) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch import optim\n",
        "import pandas as pd\n",
        "from Korpora import Korpora\n",
        "from konlpy.tag import Okt\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from numpy.linalg import norm"
      ],
      "metadata": {
        "id": "zJDV0pjLKIdq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word2Vec 모델 클래스 정의"
      ],
      "metadata": {
        "id": "5_HrBzuWKUhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VanillaSkipgram 클래스는 단순한 스킵그램(word2vec) 모델 구조를 정의함\n",
        "class VanillaSkipgram(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        # 단어 임베딩을 위한 층을 정의함\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=embedding_dim\n",
        "        )\n",
        "        # 임베딩 벡터를 통해 단어 분류를 위한 출력층을 정의함\n",
        "        self.linear = nn.Linear(\n",
        "            in_features=embedding_dim,\n",
        "            out_features=vocab_size\n",
        "        )\n",
        "\n",
        "    # 순전파 계산을 정의함\n",
        "    def forward(self, input_ids):\n",
        "        embeddings = self.embedding(input_ids)\n",
        "        output = self.linear(embeddings)\n",
        "        return output"
      ],
      "metadata": {
        "id": "TXkUBdHsJrqD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터 처리 및 토큰화\n"
      ],
      "metadata": {
        "id": "USv36L3DKWun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 로드 및 토큰화\n",
        "corpus = Korpora.load(\"nsmc\")\n",
        "corpus = pd.DataFrame(corpus.test)\n",
        "tokenizer = Okt()\n",
        "tokens = [tokenizer.morphs(review) for review in corpus.text]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCK06yU9KM2y",
        "outputId": "1eec7625-e0c9-4a20-d207-fd04136fc68d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
            "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
            "\n",
            "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
            "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
            "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
            "\n",
            "    # Description\n",
            "    Author : e9t@github\n",
            "    Repository : https://github.com/e9t/nsmc\n",
            "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
            "\n",
            "    Naver sentiment movie corpus v1.0\n",
            "    This is a movie review dataset in the Korean language.\n",
            "    Reviews were scraped from Naver Movies.\n",
            "\n",
            "    The dataset construction is based on the method noted in\n",
            "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
            "\n",
            "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
            "\n",
            "    # License\n",
            "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
            "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
            "\n",
            "[Korpora] Corpus `nsmc` is already installed at /root/Korpora/nsmc/ratings_train.txt\n",
            "[Korpora] Corpus `nsmc` is already installed at /root/Korpora/nsmc/ratings_test.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 어휘 사전 구축\n"
      ],
      "metadata": {
        "id": "l-4iundgKZ1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 사전을 구축하는 함수임\n",
        "def build_vocab(corpus, n_vocab, special_tokens):\n",
        "    counter = Counter()\n",
        "    # 각 문서의 토큰을 세어서 카운트를 누적함\n",
        "    for tokens in corpus:\n",
        "        counter.update(tokens)\n",
        "    # 특수 토큰을 추가하고 상위 n_vocab 개의 단어만 사전에 추가함\n",
        "    vocab = special_tokens\n",
        "    for token, count in counter.most_common(n_vocab):\n",
        "        vocab.append(token)\n",
        "    return vocab\n",
        "\n",
        "# 사전을 구축하고 각 단어와 인덱스를 매핑함\n",
        "vocab = build_vocab(corpus=tokens, n_vocab=5000, special_tokens=[\"<unk>\"])\n",
        "token_to_id = {token: idx for idx, token in enumerate(vocab)}\n",
        "id_to_token = {idx: token for idx, token in enumerate(vocab)}"
      ],
      "metadata": {
        "id": "keSE3KcoKPoM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 단어 쌍 및 인덱스 생성\n"
      ],
      "metadata": {
        "id": "hFpaO1OTKmZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 중심 단어와 주변 단어 쌍을 만드는 함수\n",
        "def get_word_pairs(tokens, window_size):\n",
        "    pairs = []\n",
        "    for sentence in tokens:\n",
        "        sentence_length = len(sentence)\n",
        "        for idx, center_word in enumerate(sentence):\n",
        "            window_start = max(0, idx - window_size)\n",
        "            window_end = min(sentence_length, idx + window_size + 1)\n",
        "            context_words = sentence[window_start:idx] + sentence[idx+1:window_end]\n",
        "            for context_word in context_words:\n",
        "                pairs.append([center_word, context_word])\n",
        "    return pairs\n",
        "\n",
        "word_pairs = get_word_pairs(tokens, window_size=2)\n",
        "\n",
        "# 단어 쌍을 인덱스 쌍으로 변환하는 함수\n",
        "def get_index_pairs(word_pairs, token_to_id):\n",
        "    pairs = []\n",
        "    unk_index = token_to_id[\"<unk>\"]\n",
        "    for word_pair in word_pairs:\n",
        "        center_word, context_word = word_pair\n",
        "        center_index = token_to_id.get(center_word, unk_index)\n",
        "        context_index = token_to_id.get(context_word, unk_index)\n",
        "        pairs.append([center_index, context_index])\n",
        "    return pairs\n",
        "\n",
        "index_pairs = get_index_pairs(word_pairs, token_to_id)"
      ],
      "metadata": {
        "id": "tnq5g3GOKnp7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터셋 및 모델 학습\n"
      ],
      "metadata": {
        "id": "FHMiAASWKppG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 텐서 데이터셋 생성 및 DataLoader 설정\n",
        "index_pairs = torch.tensor(index_pairs)\n",
        "center_indexs = index_pairs[:, 0]\n",
        "context_indexs = index_pairs[:, 1]\n",
        "\n",
        "# 배치 학습을 위해 텐서 데이터셋과 DataLoader로 구성함\n",
        "dataset = TensorDataset(center_indexs, context_indexs)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# 모델 학습 설정\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "word2vec = VanillaSkipgram(vocab_size=len(token_to_id), embedding_dim=128).to(device)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.SGD(word2vec.parameters(), lr=0.1)\n",
        "\n",
        "# 학습 루프\n",
        "for epoch in range(10):\n",
        "    cost = 0.0\n",
        "    for input_ids, target_ids in dataloader:\n",
        "        input_ids = input_ids.to(device)\n",
        "        target_ids = target_ids.to(device)\n",
        "\n",
        "        # 모델에 입력을 전달하여 출력 로그잇을 얻음\n",
        "        logits = word2vec(input_ids)\n",
        "        # 손실을 계산함\n",
        "        loss = criterion(logits, target_ids)\n",
        "\n",
        "        # 역전파 및 가중치 갱신을 수행함\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        cost += loss\n",
        "\n",
        "    # 에포크별 평균 손실을 출력함\n",
        "    cost = cost / len(dataloader)\n",
        "    print(f\"Epoch : {epoch+1:4d}, Cost : {cost:.3f}\")"
      ],
      "metadata": {
        "id": "e_Ce1UpNKpXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 임베딩 추출 및 유사도 계산\n"
      ],
      "metadata": {
        "id": "vbXrSyyHKs3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 임베딩 매트릭스 및 유사도 계산\n",
        "token_to_embedding = dict()\n",
        "embedding_matrix = word2vec.embedding.weight.detach().cpu().numpy()\n",
        "\n",
        "# 각 단어의 임베딩을 사전에 저장함\n",
        "for word, embedding in zip(vocab, embedding_matrix):\n",
        "    token_to_embedding[word] = embedding\n",
        "\n",
        "index = 30\n",
        "token = vocab[30]\n",
        "token_embedding = token_to_embedding[token]\n",
        "\n",
        "# 코사인 유사도를 계산하는 함수임\n",
        "def cosine_similarity(a, b):\n",
        "    cosine = np.dot(b, a) / (norm(b, axis=1) * norm(a))\n",
        "    return cosine\n",
        "\n",
        "# 유사도가 가장 높은 단어 인덱스를 찾는 함수임\n",
        "def top_n_index(cosine_matrix, n):\n",
        "    closest_indexes = cosine_matrix.argsort()[::-1]\n",
        "    top_n = closest_indexes[1 : n + 1]\n",
        "    return top_n\n",
        "\n",
        "# 코사인 유사도를 계산하고 상위 5개 단어를 찾음\n",
        "cosine_matrix = cosine_similarity(token_embedding, embedding_matrix)\n",
        "top_n = top_n_index(cosine_matrix, n=5)\n",
        "\n",
        "# 가장 유사한 단어와 유사도를 출력함\n",
        "print(f\"{token}와 가장 유사한 5 개 단어\")\n",
        "for index in top_n:\n",
        "    print(f\"{id_to_token[index]} - 유사도 : {cosine_matrix[index]:.4f}\")"
      ],
      "metadata": {
        "id": "489xuWctKuMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.13~6.14 Word2Vec 모델 실습"
      ],
      "metadata": {
        "id": "fCjLa-GMMxFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from Korpora import Korpora\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "\n",
        "corpus = Korpora.load(\"nsmc\")\n",
        "corpus = pd.DataFrame(corpus.test)"
      ],
      "metadata": {
        "id": "qMqyf0mhMzCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Okt()\n",
        "tokens = [tokenizer.morphs(review) for review in corpus.text]"
      ],
      "metadata": {
        "id": "VhL8esZfM1PX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "\n",
        "word2vec = Word2Vec(\n",
        "    sentences=tokens,\n",
        "    vector_size=128,\n",
        "    window=5,\n",
        "    min_count=1,\n",
        "    sg=1,\n",
        "    epochs=3,\n",
        "    max_final_vocab=10000\n",
        ")"
      ],
      "metadata": {
        "id": "Y8I2oUD8M28p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec.save(\"../models/word2vec.model\")\n",
        "word2vec = Word2Vec.load(\"../models/word2vec.model\")"
      ],
      "metadata": {
        "id": "BeHn-rxbM5jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"연기\"\n",
        "print(word2vec.wv[word])\n",
        "print(word2vec.wv.most_similar(word, topn=5))\n",
        "print(word2vec.wv.similarity(w1=word, w2=\"연기력\"))"
      ],
      "metadata": {
        "id": "WSYzzdW8M66K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.15~6.17 fastText OOV 모델 실습"
      ],
      "metadata": {
        "id": "Xtm9xLf6M-TI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from Korpora import Korpora\n",
        "\n",
        "\n",
        "corpus = Korpora.load(\"kornli\")\n",
        "corpus_texts = corpus.get_all_texts() + corpus.get_all_pairs()\n",
        "tokens = [sentence.split() for sentence in corpus_texts]\n",
        "\n",
        "print(tokens[:3])"
      ],
      "metadata": {
        "id": "AKLTTkbmM_1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import FastText\n",
        "\n",
        "\n",
        "fastText = FastText(\n",
        "    sentences=tokens,\n",
        "    vector_size=128,\n",
        "    window=5,\n",
        "    min_count=5,\n",
        "    sg=1,\n",
        "    epochs=3,\n",
        "    min_n=2,\n",
        "    max_n=6\n",
        ")\n",
        "\n",
        "# fastText.save(\"../models/fastText.model\")\n",
        "# fastText = FastText.load(\"../models/fastText.model\")"
      ],
      "metadata": {
        "id": "VKMbZrV9NCVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oov_token = \"사랑해요\"\n",
        "oov_vector = fastText.wv[oov_token]\n",
        "\n",
        "print(oov_token in fastText.wv.index_to_key)\n",
        "print(fastText.wv.most_similar(oov_vector, topn=5))"
      ],
      "metadata": {
        "id": "P-Mejwp5NEqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.18 양방향 다층 신경망"
      ],
      "metadata": {
        "id": "uoHtR9WXNJjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# 모델의 입력 크기와 출력 크기, RNN 층의 개수 및 양방향 설정을 정의함\n",
        "input_size = 128  # 입력 벡터의 차원 크기\n",
        "ouput_size = 256  # RNN의 은닉 상태(hidden state) 크기\n",
        "num_layers = 3    # RNN 계층 수\n",
        "bidirectional = True  # 양방향 RNN 설정 여부\n",
        "\n",
        "# RNN 모델을 초기화함\n",
        "model = nn.RNN(\n",
        "    input_size=input_size,  # 입력 벡터의 차원 크기를 설정\n",
        "    hidden_size=ouput_size,  # 은닉 상태의 차원 크기를 설정\n",
        "    num_layers=num_layers,   # RNN 계층 수를 설정\n",
        "    nonlinearity=\"tanh\",     # 활성화 함수로 tanh 사용\n",
        "    batch_first=True,        # 입력의 첫 번째 차원이 배치(batch) 크기임을 설정\n",
        "    bidirectional=bidirectional,  # 양방향 RNN 설정\n",
        ")\n",
        "\n",
        "# 입력 데이터 설정\n",
        "batch_size = 4        # 배치의 크기\n",
        "sequence_len = 6      # 시퀀스 길이\n",
        "\n",
        "# 입력 데이터 텐서를 랜덤하게 생성 (배치 크기 x 시퀀스 길이 x 입력 크기)\n",
        "inputs = torch.randn(batch_size, sequence_len, input_size)\n",
        "\n",
        "# 초기 은닉 상태(h_0)를 랜덤하게 생성\n",
        "# 양방향인 경우, num_layers * 2 만큼의 은닉 상태를 설정함\n",
        "h_0 = torch.rand(num_layers * (int(bidirectional) + 1), batch_size, ouput_size)\n",
        "\n",
        "# 모델에 입력과 초기 은닉 상태를 전달하여 출력과 마지막 은닉 상태를 얻음\n",
        "outputs, hidden = model(inputs, h_0)\n",
        "\n",
        "# outputs는 모든 시퀀스 위치에서의 출력 값을 담고 있음\n",
        "# 출력 크기는 (배치 크기, 시퀀스 길이, 은닉 상태 크기 * 양방향)임\n",
        "print(outputs.shape)\n",
        "\n",
        "# hidden은 각 계층의 마지막 은닉 상태를 담고 있음\n",
        "# hidden 크기는 (계층 수 * 양방향, 배치 크기, 은닉 상태 크기)임\n",
        "print(hidden.shape)"
      ],
      "metadata": {
        "id": "hpSJP4XlNLy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.19 양방향 다층 장단기 메모리"
      ],
      "metadata": {
        "id": "29xkNqwBNWI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# 모델의 입력 크기, 출력 크기, LSTM 층 개수, 양방향 및 프로젝션 크기를 정의함\n",
        "input_size = 128    # 입력 벡터의 차원 크기\n",
        "ouput_size = 256    # LSTM의 은닉 상태(hidden state) 크기\n",
        "num_layers = 3      # LSTM 계층 수\n",
        "bidirectional = True  # 양방향 LSTM 설정 여부\n",
        "proj_size = 64      # 프로젝션(projection) 벡터 크기 (프로젝션을 통해 은닉 상태 크기를 줄임)\n",
        "\n",
        "# LSTM 모델을 초기화함\n",
        "model = nn.LSTM(\n",
        "    input_size=input_size,       # 입력 벡터의 차원 크기 설정\n",
        "    hidden_size=ouput_size,      # LSTM의 은닉 상태 크기 설정\n",
        "    num_layers=num_layers,       # LSTM 계층 수 설정\n",
        "    batch_first=True,            # 입력의 첫 번째 차원이 배치(batch) 크기임을 설정\n",
        "    bidirectional=bidirectional, # 양방향 LSTM 설정\n",
        "    proj_size=proj_size          # 프로젝션 크기 설정 (은닉 상태를 proj_size로 축소)\n",
        ")\n",
        "\n",
        "# 입력 데이터 설정\n",
        "batch_size = 4       # 배치 크기\n",
        "sequence_len = 6     # 시퀀스 길이\n",
        "\n",
        "# 입력 데이터 텐서를 랜덤하게 생성 (배치 크기 x 시퀀스 길이 x 입력 크기)\n",
        "inputs = torch.randn(batch_size, sequence_len, input_size)\n",
        "\n",
        "# 초기 은닉 상태(h_0) 및 셀 상태(c_0)를 랜덤하게 생성\n",
        "# 양방향 LSTM인 경우 num_layers * 2의 크기로 설정\n",
        "# h_0는 프로젝션 크기만큼 축소된 크기 사용, 그렇지 않으면 은닉 상태 크기 사용\n",
        "h_0 = torch.rand(\n",
        "    num_layers * (int(bidirectional) + 1),  # 계층 수와 양방향 여부에 따른 크기\n",
        "    batch_size,\n",
        "    proj_size if proj_size > 0 else ouput_size  # 프로젝션 적용 시 크기, 그렇지 않으면 은닉 상태 크기\n",
        ")\n",
        "\n",
        "# 초기 셀 상태(c_0)를 은닉 상태 크기로 생성\n",
        "c_0 = torch.rand(num_layers * (int(bidirectional) + 1), batch_size, ouput_size)\n",
        "\n",
        "# 모델에 입력과 초기 은닉 상태 및 셀 상태를 전달하여 출력과 마지막 은닉 및 셀 상태를 얻음\n",
        "outputs, (h_n, c_n) = model(inputs, (h_0, c_0))\n",
        "\n",
        "# outputs는 모든 시퀀스 위치에서의 출력 값을 담고 있음\n",
        "# 출력 크기는 (배치 크기, 시퀀스 길이, 프로젝션 크기 * 양방향)임\n",
        "print(outputs.shape)\n",
        "\n",
        "# h_n는 각 계층의 마지막 프로젝션된 은닉 상태를 담고 있음\n",
        "# h_n 크기는 (계층 수 * 양방향, 배치 크기, 프로젝션 크기)임\n",
        "print(h_n.shape)\n",
        "\n",
        "# c_n는 각 계층의 마지막 셀 상태를 담고 있음\n",
        "# c_n 크기는 (계층 수 * 양방향, 배치 크기, 은닉 상태 크기)임\n",
        "print(c_n.shape)"
      ],
      "metadata": {
        "id": "oJWkcNZtNrK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.20~6.27 문장 분류 모델 실습"
      ],
      "metadata": {
        "id": "z7Sgy59eNwAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "# 문장 분류 모델 클래스 정의\n",
        "class SentenceClassifier(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_vocab,       # 어휘 사전 크기\n",
        "        hidden_dim,    # 은닉 상태 크기\n",
        "        embedding_dim, # 임베딩 크기\n",
        "        n_layers,      # RNN 또는 LSTM 계층 수\n",
        "        dropout=0.5,   # 드롭아웃 확률\n",
        "        bidirectional=True, # 양방향 설정\n",
        "        model_type=\"lstm\"   # 모델 타입 (\"rnn\" 또는 \"lstm\")\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # 단어 임베딩 계층을 정의함\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=n_vocab,\n",
        "            embedding_dim=embedding_dim,\n",
        "            padding_idx=0  # 패딩 토큰에 대해 0번 인덱스를 사용함\n",
        "        )\n",
        "\n",
        "        # RNN 또는 LSTM 계층을 정의함\n",
        "        if model_type == \"rnn\":\n",
        "            self.model = nn.RNN(\n",
        "                input_size=embedding_dim,\n",
        "                hidden_size=hidden_dim,\n",
        "                num_layers=n_layers,\n",
        "                bidirectional=bidirectional,\n",
        "                dropout=dropout,\n",
        "                batch_first=True,\n",
        "            )\n",
        "        elif model_type == \"lstm\":\n",
        "            self.model = nn.LSTM(\n",
        "                input_size=embedding_dim,\n",
        "                hidden_size=hidden_dim,\n",
        "                num_layers=n_layers,\n",
        "                bidirectional=bidirectional,\n",
        "                dropout=dropout,\n",
        "                batch_first=True,\n",
        "            )\n",
        "\n",
        "        # 양방향 여부에 따라 분류기 계층의 입력 차원 설정\n",
        "        if bidirectional:\n",
        "            self.classifier = nn.Linear(hidden_dim * 2, 1)\n",
        "        else:\n",
        "            self.classifier = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "        # 드롭아웃 계층을 정의함\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # 순전파 계산을 정의함\n",
        "    def forward(self, inputs):\n",
        "        embeddings = self.embedding(inputs)  # 임베딩을 통해 입력을 임베딩 벡터로 변환\n",
        "        output, _ = self.model(embeddings)   # RNN 또는 LSTM 계층 통과\n",
        "        last_output = output[:, -1, :]       # 마지막 타임스텝의 출력만 선택\n",
        "        last_output = self.dropout(last_output)  # 드롭아웃 적용\n",
        "        logits = self.classifier(last_output)  # 분류기 통과하여 로짓 계산\n",
        "        return logits"
      ],
      "metadata": {
        "id": "i18QL3sLNxiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from Korpora import Korpora\n",
        "\n",
        "# NSMC 코퍼스를 불러와서 훈련 및 테스트 데이터로 나눔\n",
        "corpus = Korpora.load(\"nsmc\")\n",
        "corpus_df = pd.DataFrame(corpus.test)\n",
        "train = corpus_df.sample(frac=0.9, random_state=42)\n",
        "test = corpus_df.drop(train.index)\n",
        "\n",
        "print(train.head(5).to_markdown())\n",
        "print(\"Training Data Size :\", len(train))\n",
        "print(\"Testing Data Size :\", len(test))"
      ],
      "metadata": {
        "id": "XL5P19ZXOMFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "from collections import Counter\n",
        "\n",
        "# 어휘 사전 생성 함수 정의\n",
        "def build_vocab(corpus, n_vocab, special_tokens):\n",
        "    counter = Counter()\n",
        "    for tokens in corpus:\n",
        "        counter.update(tokens)  # 각 문서의 토큰 카운트를 누적함\n",
        "    vocab = special_tokens     # 특수 토큰 추가\n",
        "    for token, count in counter.most_common(n_vocab):\n",
        "        vocab.append(token)\n",
        "    return vocab\n",
        "\n",
        "# 토큰화 수행\n",
        "tokenizer = Okt()\n",
        "train_tokens = [tokenizer.morphs(review) for review in train.text]\n",
        "test_tokens = [tokenizer.morphs(review) for review in test.text]\n",
        "\n",
        "# 어휘 사전 구축 및 단어-인덱스 매핑 생성\n",
        "vocab = build_vocab(corpus=train_tokens, n_vocab=5000, special_tokens=[\"<pad>\", \"<unk>\"])\n",
        "token_to_id = {token: idx for idx, token in enumerate(vocab)}\n",
        "id_to_token = {idx: token for idx, token in enumerate(vocab)}\n",
        "\n",
        "print(vocab[:10])\n",
        "print(len(vocab))"
      ],
      "metadata": {
        "id": "hRlnIkaoOOBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# 패딩 처리를 위한 함수 정의\n",
        "def pad_sequences(sequences, max_length, pad_value):\n",
        "    result = []\n",
        "    for sequence in sequences:\n",
        "        sequence = sequence[:max_length]\n",
        "        pad_length = max_length - len(sequence)\n",
        "        padded_sequence = sequence + [pad_value] * pad_length\n",
        "        result.append(padded_sequence)\n",
        "    return np.asarray(result)\n",
        "\n",
        "# 토큰을 인덱스로 변환하고 패딩 처리\n",
        "unk_id = token_to_id[\"<unk>\"]\n",
        "train_ids = [\n",
        "    [token_to_id.get(token, unk_id) for token in review] for review in train_tokens\n",
        "]\n",
        "test_ids = [\n",
        "    [token_to_id.get(token, unk_id) for token in review] for review in test_tokens\n",
        "]\n",
        "\n",
        "max_length = 32\n",
        "pad_id = token_to_id[\"<pad>\"]\n",
        "train_ids = pad_sequences(train_ids, max_length, pad_id)\n",
        "test_ids = pad_sequences(test_ids, max_length, pad_id)\n",
        "\n",
        "print(train_ids[0])\n",
        "print(test_ids[0])"
      ],
      "metadata": {
        "id": "Z07jMr7GOQKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# 텐서 데이터셋 및 데이터로더 설정\n",
        "train_ids = torch.tensor(train_ids)\n",
        "test_ids = torch.tensor(test_ids)\n",
        "train_labels = torch.tensor(train.label.values, dtype=torch.float32)\n",
        "test_labels = torch.tensor(test.label.values, dtype=torch.float32)\n",
        "\n",
        "train_dataset = TensorDataset(train_ids, train_labels)\n",
        "test_dataset = TensorDataset(test_ids, test_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "id": "-kpG5PXtOQuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "\n",
        "# 모델 학습 설정\n",
        "n_vocab = len(token_to_id)\n",
        "hidden_dim = 64\n",
        "embedding_dim = 128\n",
        "n_layers = 2\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "classifier = SentenceClassifier(\n",
        "    n_vocab=n_vocab, hidden_dim=hidden_dim, embedding_dim=embedding_dim, n_layers=n_layers\n",
        ").to(device)\n",
        "criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "optimizer = optim.RMSprop(classifier.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "Gf2PXbLgOWhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습 함수 정의\n",
        "def train(model, datasets, criterion, optimizer, device, interval):\n",
        "    model.train()\n",
        "    losses = []\n",
        "\n",
        "    for step, (input_ids, labels) in enumerate(datasets):\n",
        "        input_ids = input_ids.to(device)\n",
        "        labels = labels.to(device).unsqueeze(1)\n",
        "\n",
        "        logits = model(input_ids)\n",
        "        loss = criterion(logits, labels)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % interval == 0:\n",
        "            print(f\"Train Loss {step} : {np.mean(losses)}\")\n",
        "\n",
        "\n",
        "# 모델 테스트 함수 정의\n",
        "def test(model, datasets, criterion, device):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    corrects = []\n",
        "\n",
        "    for step, (input_ids, labels) in enumerate(datasets):\n",
        "        input_ids = input_ids.to(device)\n",
        "        labels = labels.to(device).unsqueeze(1)\n",
        "\n",
        "        logits = model(input_ids)\n",
        "        loss = criterion(logits, labels)\n",
        "        losses.append(loss.item())\n",
        "        yhat = torch.sigmoid(logits) > 0.5\n",
        "        corrects.extend(\n",
        "            torch.eq(yhat, labels).cpu().tolist()\n",
        "        )\n",
        "\n",
        "    print(f\"Val Loss : {np.mean(losses)}, Val Accuracy : {np.mean(corrects)}\")"
      ],
      "metadata": {
        "id": "9Qpk6DN7OZ-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습 및 테스트 실행\n",
        "epochs = 5\n",
        "interval = 500\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train(classifier, train_loader, criterion, optimizer, device, interval)\n",
        "    test(classifier, test_loader, criterion, device)"
      ],
      "metadata": {
        "id": "a81pgoyPObaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 임베딩 저장 및 특정 단어 확인\n",
        "token_to_embedding = dict()\n",
        "embedding_matrix = classifier.embedding.weight.detach().cpu().numpy()\n",
        "\n",
        "for word, emb in zip(vocab, embedding_matrix):\n",
        "    token_to_embedding[word] = emb\n",
        "\n",
        "token = vocab[1000]\n",
        "print(token, token_to_embedding[token])"
      ],
      "metadata": {
        "id": "GFALr8HkOcVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.28~6.30 Word2Vec 모델을 활용한 문장 분류 모델 실습"
      ],
      "metadata": {
        "id": "9lkKbW4_OeVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "# 문장 분류 모델 클래스 정의\n",
        "class SentenceClassifier(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_vocab,              # 어휘 사전 크기\n",
        "        hidden_dim,           # 은닉 상태 크기\n",
        "        embedding_dim,        # 임베딩 크기\n",
        "        n_layers,             # RNN 또는 LSTM 계층 수\n",
        "        dropout=0.5,          # 드롭아웃 확률\n",
        "        bidirectional=True,   # 양방향 설정\n",
        "        model_type=\"lstm\",    # 모델 타입 (\"rnn\" 또는 \"lstm\")\n",
        "        pretrained_embedding=None # 사전 학습된 임베딩\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # 사전 학습된 임베딩을 사용할 경우, 해당 임베딩을 적용\n",
        "        if pretrained_embedding is not None:\n",
        "            self.embedding = nn.Embedding.from_pretrained(\n",
        "                torch.tensor(pretrained_embedding, dtype=torch.float32)\n",
        "            )\n",
        "        # 사전 학습된 임베딩이 없을 경우, 새로운 임베딩 층 생성\n",
        "        else:\n",
        "            self.embedding = nn.Embedding(\n",
        "                num_embeddings=n_vocab,\n",
        "                embedding_dim=embedding_dim,\n",
        "                padding_idx=0  # 패딩 토큰의 인덱스\n",
        "            )\n",
        "\n",
        "        # RNN 또는 LSTM 계층을 설정\n",
        "        if model_type == \"rnn\":\n",
        "            self.model = nn.RNN(\n",
        "                input_size=embedding_dim,\n",
        "                hidden_size=hidden_dim,\n",
        "                num_layers=n_layers,\n",
        "                bidirectional=bidirectional,\n",
        "                dropout=dropout,\n",
        "                batch_first=True,\n",
        "            )\n",
        "        elif model_type == \"lstm\":\n",
        "            self.model = nn.LSTM(\n",
        "                input_size=embedding_dim,\n",
        "                hidden_size=hidden_dim,\n",
        "                num_layers=n_layers,\n",
        "                bidirectional=bidirectional,\n",
        "                dropout=dropout,\n",
        "                batch_first=True,\n",
        "            )\n",
        "\n",
        "        # 양방향 여부에 따라 분류기 계층의 입력 크기 설정\n",
        "        if bidirectional:\n",
        "            self.classifier = nn.Linear(hidden_dim * 2, 1)\n",
        "        else:\n",
        "            self.classifier = nn.Linear(hidden_dim, 1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # 순전파 계산 정의\n",
        "    def forward(self, inputs):\n",
        "        embeddings = self.embedding(inputs)   # 임베딩을 통해 입력 변환\n",
        "        output, _ = self.model(embeddings)    # RNN 또는 LSTM 계층 통과\n",
        "        last_output = output[:, -1, :]        # 마지막 타임스텝의 출력만 선택\n",
        "        last_output = self.dropout(last_output)  # 드롭아웃 적용\n",
        "        logits = self.classifier(last_output)    # 로짓 계산\n",
        "        return logits"
      ],
      "metadata": {
        "id": "X74W9zB2Oi7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from Korpora import Korpora\n",
        "\n",
        "# NSMC 코퍼스를 불러와서 훈련 및 테스트 데이터로 나눔\n",
        "corpus = Korpora.load(\"nsmc\")\n",
        "corpus_df = pd.DataFrame(corpus.test)\n",
        "train = corpus_df.sample(frac=0.9, random_state=42)\n",
        "test = corpus_df.drop(train.index)\n",
        "\n",
        "print(train.head(5).to_markdown())\n",
        "print(\"Training Data Size :\", len(train))\n",
        "print(\"Testing Data Size :\", len(test))"
      ],
      "metadata": {
        "id": "JGfD1tH-OuGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "from collections import Counter\n",
        "\n",
        "# 어휘 사전 생성 함수 정의\n",
        "def build_vocab(corpus, n_vocab, special_tokens):\n",
        "    counter = Counter()\n",
        "    for tokens in corpus:\n",
        "        counter.update(tokens)  # 각 문서의 토큰 카운트를 누적함\n",
        "    vocab = special_tokens     # 특수 토큰 추가\n",
        "    for token, count in counter.most_common(n_vocab):\n",
        "        vocab.append(token)\n",
        "    return vocab\n",
        "\n",
        "# 토큰화 수행\n",
        "tokenizer = Okt()\n",
        "train_tokens = [tokenizer.morphs(review) for review in train.text]\n",
        "test_tokens = [tokenizer.morphs(review) for review in test.text]\n",
        "\n",
        "# 어휘 사전 구축 및 단어-인덱스 매핑 생성\n",
        "vocab = build_vocab(corpus=train_tokens, n_vocab=5000, special_tokens=[\"<pad>\", \"<unk>\"])\n",
        "token_to_id = {token: idx for idx, token in enumerate(vocab)}\n",
        "id_to_token = {idx: token for idx, token in enumerate(vocab)}\n",
        "\n",
        "print(vocab[:10])\n",
        "print(len(vocab))"
      ],
      "metadata": {
        "id": "C936fFtAOwWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# 패딩 처리를 위한 함수 정의\n",
        "def pad_sequences(sequences, max_length, pad_value):\n",
        "    result = []\n",
        "    for sequence in sequences:\n",
        "        sequence = sequence[:max_length]\n",
        "        pad_length = max_length - len(sequence)\n",
        "        padded_sequence = sequence + [pad_value] * pad_length\n",
        "        result.append(padded_sequence)\n",
        "    return np.asarray(result)\n",
        "\n",
        "# 토큰을 인덱스로 변환하고 패딩 처리\n",
        "unk_id = token_to_id[\"<unk>\"]\n",
        "train_ids = [\n",
        "    [token_to_id.get(token, unk_id) for token in review] for review in train_tokens\n",
        "]\n",
        "test_ids = [\n",
        "    [token_to_id.get(token, unk_id) for token in review] for review in test_tokens\n",
        "]\n",
        "\n",
        "max_length = 32\n",
        "pad_id = token_to_id[\"<pad>\"]\n",
        "train_ids = pad_sequences(train_ids, max_length, pad_id)\n",
        "test_ids = pad_sequences(test_ids, max_length, pad_id)\n",
        "\n",
        "print(train_ids[0])\n",
        "print(test_ids[0])"
      ],
      "metadata": {
        "id": "oX08pI9lOxdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# 텐서 데이터셋 및 데이터로더 설정\n",
        "train_ids = torch.tensor(train_ids)\n",
        "test_ids = torch.tensor(test_ids)\n",
        "train_labels = torch.tensor(train.label.values, dtype=torch.float32)\n",
        "test_labels = torch.tensor(test.label.values, dtype=torch.float32)\n",
        "\n",
        "train_dataset = TensorDataset(train_ids, train_labels)\n",
        "test_dataset = TensorDataset(test_ids, test_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "id": "Tq97hmSfOzLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# 사전 학습된 Word2Vec 모델을 불러와서 임베딩 행렬 초기화\n",
        "word2vec = Word2Vec.load(\"../models/word2vec.model\")\n",
        "init_embeddings = np.zeros((n_vocab, embedding_dim))\n",
        "\n",
        "# 각 단어에 해당하는 임베딩을 초기화 행렬에 채움\n",
        "for index, token in id_to_token.items():\n",
        "    if token not in [\"<pad>\", \"<unk>\"]:\n",
        "        init_embeddings[index] = word2vec.wv[token]\n",
        "\n",
        "# 사전 학습된 임베딩으로부터 임베딩 층을 생성\n",
        "embedding_layer = nn.Embedding.from_pretrained(\n",
        "    torch.tensor(init_embeddings, dtype=torch.float32)\n",
        ")"
      ],
      "metadata": {
        "id": "hFynrAEcO3up"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습 함수 정의\n",
        "def train(model, datasets, criterion, optimizer, device, interval):\n",
        "    model.train()\n",
        "    losses = []\n",
        "\n",
        "    for step, (input_ids, labels) in enumerate(datasets):\n",
        "        input_ids = input_ids.to(device)\n",
        "        labels = labels.to(device).unsqueeze(1)\n",
        "\n",
        "        logits = model(input_ids)\n",
        "        loss = criterion(logits, labels)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % interval == 0:\n",
        "            print(f\"Train Loss {step} : {np.mean(losses)}\")\n",
        "\n",
        "\n",
        "# 모델 테스트 함수 정의\n",
        "def test(model, datasets, criterion, device):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    corrects = []\n",
        "\n",
        "    for step, (input_ids, labels) in enumerate(datasets):\n",
        "        input_ids = input_ids.to(device)\n",
        "        labels = labels.to(device).unsqueeze(1)\n",
        "\n",
        "        logits = model(input_ids)\n",
        "        loss = criterion(logits, labels)\n",
        "        losses.append(loss.item())\n",
        "        yhat = torch.sigmoid(logits) > 0.5\n",
        "        corrects.extend(\n",
        "            torch.eq(yhat, labels).cpu().tolist()\n",
        "        )\n",
        "\n",
        "    print(f\"Val Loss : {np.mean(losses)}, Val Accuracy : {np.mean(corrects)}\")"
      ],
      "metadata": {
        "id": "fnknPKE5O5HG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "\n",
        "# 모델 및 학습 설정\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "classifier = SentenceClassifier(\n",
        "    n_vocab=n_vocab, hidden_dim=hidden_dim, embedding_dim=embedding_dim,\n",
        "    n_layers=n_layers, pretrained_embedding=init_embeddings\n",
        ").to(device)\n",
        "criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "optimizer = optim.RMSprop(classifier.parameters(), lr=0.001)\n",
        "\n",
        "# 모델 학습 및 테스트 실행\n",
        "epochs = 5\n",
        "interval = 500\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train(classifier, train_loader, criterion, optimizer, device, interval)\n",
        "    test(classifier, test_loader, criterion, device)"
      ],
      "metadata": {
        "id": "_Vo2V26hO7K5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.31 합성곱 모델"
      ],
      "metadata": {
        "id": "SG5uNlGyO8dX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# CNN 클래스 정의\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # 첫 번째 합성곱 계층 정의\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=3,     # 입력 채널 수 (RGB 이미지라서 3)\n",
        "                out_channels=16,   # 출력 채널 수 (필터 수)\n",
        "                kernel_size=3,     # 필터 크기\n",
        "                stride=2,          # 필터 이동 간격\n",
        "                padding=1          # 패딩 크기\n",
        "            ),\n",
        "            nn.ReLU(),            # 활성화 함수\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # 2x2 최대 풀링\n",
        "        )\n",
        "\n",
        "        # 두 번째 합성곱 계층 정의\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=16,    # 이전 계층의 출력 채널 수가 입력 채널로 사용됨\n",
        "                out_channels=32,   # 출력 채널 수\n",
        "                kernel_size=3,     # 필터 크기\n",
        "                stride=1,          # 필터 이동 간격\n",
        "                padding=1          # 패딩 크기\n",
        "            ),\n",
        "            nn.ReLU(),            # 활성화 함수\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # 2x2 최대 풀링\n",
        "        )\n",
        "\n",
        "        # 전결합 계층 (FC) 정의\n",
        "        self.fc = nn.Linear(32 * 32 * 32, 10)  # 출력 뉴런 수는 10 (예: 클래스 10개)\n",
        "\n",
        "    # 순전파 함수 정의\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)         # 첫 번째 합성곱 계층 통과\n",
        "        x = self.conv2(x)         # 두 번째 합성곱 계층 통과\n",
        "        x = torch.flatten(x)      # 출력을 평탄화하여 FC 계층에 전달\n",
        "        x = self.fc(x)            # 전결합 계층 통과\n",
        "        return x"
      ],
      "metadata": {
        "id": "b2QwnochPAHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.32~6.33 합성곱 신경망 분류 모델 실습"
      ],
      "metadata": {
        "id": "ECZxaMVDPEGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# 문장 분류 모델 클래스 정의\n",
        "class SentenceClassifier(nn.Module):\n",
        "    def __init__(self, pretrained_embedding, filter_sizes, max_length, dropout=0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        # 사전 학습된 임베딩을 사용하여 임베딩 층 초기화\n",
        "        self.embedding = nn.Embedding.from_pretrained(\n",
        "            torch.tensor(pretrained_embedding, dtype=torch.float32)\n",
        "        )\n",
        "        embedding_dim = self.embedding.weight.shape[1]  # 임베딩 차원 수\n",
        "\n",
        "        # CNN 필터 생성\n",
        "        conv = []\n",
        "        for size in filter_sizes:\n",
        "            conv.append(\n",
        "                nn.Sequential(\n",
        "                    nn.Conv1d(\n",
        "                        in_channels=embedding_dim,  # 입력 채널 수 (임베딩 차원)\n",
        "                        out_channels=1,            # 출력 채널 수\n",
        "                        kernel_size=size           # 커널 크기\n",
        "                    ),\n",
        "                    nn.ReLU(),  # 활성화 함수\n",
        "                    nn.MaxPool1d(kernel_size=max_length - size - 1),  # 맥스 풀링 적용\n",
        "                )\n",
        "            )\n",
        "        self.conv_filters = nn.ModuleList(conv)  # CNN 필터 리스트로 저장\n",
        "\n",
        "        # CNN 출력 크기에 따라 분류기 계층을 정의\n",
        "        output_size = len(filter_sizes)\n",
        "        self.pre_classifier = nn.Linear(output_size, output_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.classifier = nn.Linear(output_size, 1)  # 최종 이진 분류기\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # 입력을 임베딩으로 변환하고 차원 변경 (배치, 채널, 길이 순서로 맞춤)\n",
        "        embeddings = self.embedding(inputs)\n",
        "        embeddings = embeddings.permute(0, 2, 1)\n",
        "\n",
        "        # 각 CNN 필터를 적용하여 출력 리스트 생성\n",
        "        conv_outputs = [conv(embeddings) for conv in self.conv_filters]\n",
        "        # CNN 출력 텐서들을 결합\n",
        "        concat_outputs = torch.cat([conv.squeeze(-1) for conv in conv_outputs], dim=1)\n",
        "\n",
        "        # 분류기 계층 통과 후 드롭아웃 및 최종 로짓 계산\n",
        "        logits = self.pre_classifier(concat_outputs)\n",
        "        logits = self.dropout(logits)\n",
        "        logits = self.classifier(logits)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "poj97XzZPHrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from Korpora import Korpora\n",
        "\n",
        "# NSMC 코퍼스를 불러와서 훈련 및 테스트 데이터로 나눔\n",
        "corpus = Korpora.load(\"nsmc\")\n",
        "corpus_df = pd.DataFrame(corpus.test)\n",
        "train = corpus_df.sample(frac=0.9, random_state=42)\n",
        "test = corpus_df.drop(train.index)\n",
        "\n",
        "print(train.head(5).to_markdown())\n",
        "print(\"Training Data Size :\", len(train))\n",
        "print(\"Testing Data Size :\", len(test))"
      ],
      "metadata": {
        "id": "3doF2B4fPPNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "from collections import Counter\n",
        "\n",
        "# 어휘 사전 생성 함수 정의\n",
        "def build_vocab(corpus, n_vocab, special_tokens):\n",
        "    counter = Counter()\n",
        "    for tokens in corpus:\n",
        "        counter.update(tokens)  # 각 문서의 토큰 카운트를 누적함\n",
        "    vocab = special_tokens     # 특수 토큰 추가\n",
        "    for token, count in counter.most_common(n_vocab):\n",
        "        vocab.append(token)\n",
        "    return vocab\n",
        "\n",
        "# 토큰화 수행\n",
        "tokenizer = Okt()\n",
        "train_tokens = [tokenizer.morphs(review) for review in train.text]\n",
        "test_tokens = [tokenizer.morphs(review) for review in test.text]\n",
        "\n",
        "# 어휘 사전 구축 및 단어-인덱스 매핑 생성\n",
        "vocab = build_vocab(corpus=train_tokens, n_vocab=5000, special_tokens=[\"<pad>\", \"<unk>\"])\n",
        "token_to_id = {token: idx for idx, token in enumerate(vocab)}\n",
        "id_to_token = {idx: token for idx, token in enumerate(vocab)}\n",
        "\n",
        "print(vocab[:10])\n",
        "print(len(vocab))"
      ],
      "metadata": {
        "id": "PC-hUNmFPPmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# 패딩 처리를 위한 함수 정의\n",
        "def pad_sequences(sequences, max_length, pad_value):\n",
        "    result = []\n",
        "    for sequence in sequences:\n",
        "        sequence = sequence[:max_length]  # 최대 길이에 맞게 자름\n",
        "        pad_length = max_length - len(sequence)\n",
        "        padded_sequence = sequence + [pad_value] * pad_length  # 패딩 값으로 채움\n",
        "        result.append(padded_sequence)\n",
        "    return np.asarray(result)\n",
        "\n",
        "# 토큰을 인덱스로 변환하고 패딩 처리\n",
        "unk_id = token_to_id[\"<unk>\"]\n",
        "train_ids = [\n",
        "    [token_to_id.get(token, unk_id) for token in review] for review in train_tokens\n",
        "]\n",
        "test_ids = [\n",
        "    [token_to_id.get(token, unk_id) for token in review] for review in test_tokens\n",
        "]\n",
        "\n",
        "max_length = 32\n",
        "pad_id = token_to_id[\"<pad>\"]\n",
        "train_ids = pad_sequences(train_ids, max_length, pad_id)\n",
        "test_ids = pad_sequences(test_ids, max_length, pad_id)\n",
        "\n",
        "print(train_ids[0])\n",
        "print(test_ids[0])"
      ],
      "metadata": {
        "id": "duwpzylbPPxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# 텐서 데이터셋 및 데이터로더 설정\n",
        "train_ids = torch.tensor(train_ids)\n",
        "test_ids = torch.tensor(test_ids)\n",
        "train_labels = torch.tensor(train.label.values, dtype=torch.float32)\n",
        "test_labels = torch.tensor(test.label.values, dtype=torch.float32)\n",
        "\n",
        "train_dataset = TensorDataset(train_ids, train_labels)\n",
        "test_dataset = TensorDataset(test_ids, test_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "id": "Q3mfhtFwPULw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# 사전 학습된 Word2Vec 모델을 불러와서 임베딩 행렬 초기화\n",
        "n_vocab = len(token_to_id)\n",
        "embedding_dim = 128\n",
        "word2vec = Word2Vec.load(\"../models/word2vec.model\")\n",
        "init_embeddings = np.zeros((n_vocab, embedding_dim))\n",
        "\n",
        "# 각 단어에 해당하는 임베딩을 초기화 행렬에 채움\n",
        "for index, token in id_to_token.items():\n",
        "    if token not in [\"<pad>\", \"<unk>\"]:\n",
        "        init_embeddings[index] = word2vec.wv[token]"
      ],
      "metadata": {
        "id": "5UXpUsIpPUGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "\n",
        "# 모델 및 학습 설정\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "filter_sizes = [3, 3, 4, 4, 5, 5]  # CNN 필터 크기 설정\n",
        "classifier = SentenceClassifier(\n",
        "    pretrained_embedding=init_embeddings,\n",
        "    filter_sizes=filter_sizes,\n",
        "    max_length=max_length\n",
        ").to(device)\n",
        "criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "optimizer = optim.RMSprop(classifier.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "TpglYQRaPWSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습 함수 정의\n",
        "def train(model, datasets, criterion, optimizer, device, interval):\n",
        "    model.train()\n",
        "    losses = []\n",
        "\n",
        "    for step, (input_ids, labels) in enumerate(datasets):\n",
        "        input_ids = input_ids.to(device)\n",
        "        labels = labels.to(device).unsqueeze(1)\n",
        "\n",
        "        logits = model(input_ids)\n",
        "        loss = criterion(logits, labels)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % interval == 0:\n",
        "            print(f\"Train Loss {step} : {np.mean(losses)}\")\n",
        "\n",
        "\n",
        "# 모델 테스트 함수 정의\n",
        "def test(model, datasets, criterion, device):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    corrects = []\n",
        "\n",
        "    for step, (input_ids, labels) in enumerate(datasets):\n",
        "        input_ids = input_ids.to(device)\n",
        "        labels = labels.to(device).unsqueeze(1)\n",
        "\n",
        "        logits = model(input_ids)\n",
        "        loss = criterion(logits, labels)\n",
        "        losses.append(loss.item())\n",
        "        yhat = torch.sigmoid(logits) > 0.5  # 예측값이 0.5보다 큰 경우 1로 분류\n",
        "        corrects.extend(torch.eq(yhat, labels).cpu().tolist())\n",
        "\n",
        "    print(f\"Val Loss : {np.mean(losses)}, Val Accuracy : {np.mean(corrects)}\")"
      ],
      "metadata": {
        "id": "ElnuEF3qPXcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습 및 테스트 실행\n",
        "epochs = 5\n",
        "interval = 500\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train(classifier, train_loader, criterion, optimizer, device, interval)\n",
        "    test(classifier, test_loader, criterion, device)"
      ],
      "metadata": {
        "id": "Kda86RsPPZr2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}